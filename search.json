[
  {
    "objectID": "wandb.html",
    "href": "wandb.html",
    "title": "wandb",
    "section": "",
    "text": "Once setup, WandB tracks datasets, models, training runs, evaluation runs across several experiments. The original documentation is here.\nKey concepts we use in this package:",
    "crumbs": [
      "General Code",
      "wandb"
    ]
  },
  {
    "objectID": "wandb.html#steps",
    "href": "wandb.html#steps",
    "title": "wandb",
    "section": "Steps:",
    "text": "Steps:\n\nLogin to wandb:\n\nMay require an API key, which is available at https://wandb.ai/authorize. To access the API key, must be logged in onto WandB.\n\nInitialize a Run with desired parameters and metadata\nPerform operations to be tracked (e.g. train model, load dataset as artifact, …)\nFinish the Run\n\n\nsource\n\nlogin_nb\n\n login_nb (nb_file:str|pathlib.Path)\n\nFirst step to setup WandB from notebook. Logs in and logs passed notebook as source of code\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nnb_file\nstr | pathlib.Path\nname of the notebook (str) or path to the notebook (Path)\n\n\n\nTo allow WandB to store the code used for the session, the name or path of the notebook must be passed as argument nb_file.\nExample:\n\nlogin_nb('01_wandb')\n\nLogging in from notebook: /home/vtec/projects/bio/metagentools/nbs-dev/01_wandb.ipynb\n\n\nwandb: Currently logged in as: vtecftyw. Use `wandb login --relogin` to force relogin\n\n\nlogin_nb raises error in the following cases:\n\nIf nb_file is not passed, the function raises a TypeError\nIf nb_file is not a string or a Path, the function raises a TypeError\nThere must exist a file nb_file or a ValueError is raised\n\n\nsource\n\n\nWandbRun\n\n WandbRun (entity:str='', project:str='', run_name:str='',\n           job_type:str='', notes:str='',\n           logs_dir:str|pathlib.Path|None=None, testing:bool=False)\n\nManages a WandB run and all logged actions performed while run is active. Close run with .finish()\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nentity\nstr\n\nuser or organization under which the run will be logged. Default: metagenomics_sh\n\n\nproject\nstr\n\nname of the WandB project under which the run will be logged\n\n\nrun_name\nstr\n\nunique name for the run,\n\n\njob_type\nstr\n\ne.g.: load_datasets, train_exp, …\n\n\nnotes\nstr\n\nany text description or additional information to store with the run\n\n\nlogs_dir\nstr | pathlib.Path | None\nNone\ndefault is project_root/wandb-logs if None, or uses the passed Path\n\n\ntesting\nbool\nFalse\n(optional) If True, will not create a run on WandB. Use for local testing\n\n\n\n\nCreate a Run instance\nWandbRun allows to define a set of metadata associated with the run, such as entity, project, name, job_type and additional notes.\nExample:\n\nset the parameters\n\n\nentity = 'metagenomics_sh'\nproject = 'coding-with-nbdev'\nrun_name = 'nbdev-test'\njob_type = \"code_testing\"\nnotes = 'any other information of interest for the future'\n\n\ncreate a WandbRun instance called wandb_run\n\n\nwandb_run = WandbRun(\n    entity=entity, \n    project=project, \n    run_name=run_name, \n    job_type=job_type, \n    notes=notes\n    )\n\nwandb: Currently logged in as: vtecftyw (metagenomics_sh). Use `wandb login --relogin` to force relogin\n\n\nwandb version 0.19.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.16.6\n\n\nRun data is saved locally in /home/vtec/projects/bio/metagentools/wandb-logs/wandb/run-20250201_174624-e4wz0f2e\n\n\nSyncing run nbdev-test to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/metagenomics_sh/coding-with-nbdev\n\n\n View run at https://wandb.ai/metagenomics_sh/coding-with-nbdev/runs/e4wz0f2e\n\n\nWandbRun instantiation raises an error in the following cases:\n\nIf one of entity, project, run_name or job_type is not passed, the function raises a ValueError\nIf one of entity, project, run_name, job_type or notes is not a string, the function raises a TypeError\n\n\nsource\n\n\n\nWandbRun.upload_dataset\n\n WandbRun.upload_dataset (ds_path:str, ds_name:str, ds_type:str,\n                          ds_descr:str, ds_metadata:dict,\n                          load_type:str='file',\n                          wait_completion:bool=False)\n\nLoad a dataset from a file as WandB artifact, with associated information and metadata\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nds_path\nstr\n\npath to the file or directory to load as dataset artifact\n\n\nds_name\nstr\n\nname for the dataset\n\n\nds_type\nstr\n\ntype of dataset: e.g. raw_data, processed_data, …\n\n\nds_descr\nstr\n\nshort description of the dataset\n\n\nds_metadata\ndict\n\nkeys/values for metadata on the dataset, eg. nb_samples, …\n\n\nload_type\nstr\nfile\nfile to load a single file, dir to load all files in a directory\n\n\nwait_completion\nbool\nFalse\nwhen True, wait completion of the logging before returning artifact\n\n\n\n\nLoad a dataset from a single file\n\np2ds = Path('data_dev/ncbi/refsequences/cov/cov_virus_sequence_one.fa')\nassert p2ds.is_file()\n\nds_fname = str(p2ds.absolute())\nds_name = 'cov_one_sequence'\nds_type = 'cov_sequences'\nds_descr = 'one covid sequence fasta file'\n\nds_metadata = {\n    'nb_sequences': 1,\n    'file type': 'fasta',\n}\n\n\natx_one_file = wandb_run.upload_dataset(\n    ds_path=ds_fname,\n    ds_name=ds_name,\n    ds_type=ds_type,\n    ds_descr=ds_descr,\n    ds_metadata=ds_metadata,\n    load_type='file',\n)\n\nDataset cov_one_sequence is being logged as artifact ...\n\n\n\n\nLoad a dataset with several files from a directory.\n\np2ds_dir = Path('data_dev/ncbi/refsequences/cov/single_1seq_150bp')\nassert p2ds_dir.is_dir()\n\nds_dirname = str(p2ds_dir.absolute())\nds_name = 'cov_reads_single_1_sequence_150bp'\nds_type = 'sim_reads'\nds_descr = 'Simulated single reads of one cov sequence fq and aln files'\n\nds_metadata = {\n    'nb_sequences': 1,\n    'sim_type': 'single',\n    'read_length': 150,\n    'fold': 100,\n}\n\n\natx_multi_files = wandb_run.upload_dataset(\n    ds_path=ds_dirname,\n    ds_name=ds_name,\n    ds_type=ds_type,\n    ds_descr=ds_descr,\n    ds_metadata=ds_metadata,\n    load_type='dir',\n)\n\nwandb: Adding directory to artifact (/home/vtec/projects/bio/metagentools/nbs-dev/data_dev/ncbi/refsequences/cov/single_1seq_150bp)... Done. 0.2s\n\n\nDataset cov_reads_single_1_sequence_150bp is being logged as artifact ...\n\n\nWandbRun.upload_dataset raises an error in the following cases:\n\nds_path is a file and load_type is dir\nds_path is a directory and load_type is ’file`\nload_type has another value then file or dir\n\n\n\n\nClose a WandB run\n\nwandb_run.finish()\n\n\n\n\n View run nbdev-test at: https://wandb.ai/metagenomics_sh/coding-with-nbdev/runs/e4wz0f2e View project at: https://wandb.ai/metagenomics_sh/coding-with-nbdevSynced 7 W&B file(s), 0 media file(s), 6 artifact file(s) and 0 other file(s)\n\n\nFind logs at: /home/vtec/projects/bio/metagentools/wandb-logs/wandb/run-20250201_174624-e4wz0f2e/logs\n\n\n\nsource\n\n\nentity_projects\n\n entity_projects (entity:str)\n\nReturns all projects under ‘entity’, as an iterable collection\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nentity\nstr\nname of the entity from which the projects will be retrieved\n\n\nReturns\nProjects\nProjects iterator\n\n\n\nentity_projects inquires WandB to retrieve all the projects, and returns them as an iterable object.\nEach element in the iterator is a wandb.Project object. Each Project object has the following attributes:\n\n_attrs: dict of attributes associated with the project (id, name, entityName, createdAt). These attributes can be called directly as object.id, …\nentity\nname: project name\npath: as a list [entity, name]\nurl: the url to the project workspace (‘https://wandb.ai/entity/project/workspace’)\n\n\nprojs = entity_projects(entity='vtecftyw')\n\nfor p in projs:\n    print(f\"{p.name}:\")\n    print('  name:   ', p.name)\n    print('  entity  ', p.entity)\n    print('  path:   ', p.path)\n    print()\n    print('  url:    ', p.url)\n    print('  id:     ', p.id)\n    print('  created:', p.createdAt)\n    print('  _attrs: ', p._attrs)\n    print()\n\npytorch-intro:\n  name:    pytorch-intro\n  entity   vtecftyw\n  path:    ['vtecftyw', 'pytorch-intro']\n\n  url:     https://wandb.ai/vtecftyw/pytorch-intro/workspace\n  id:      UHJvamVjdDp2MTpweXRvcmNoLWludHJvOnZ0ZWNmdHl3\n  created: 2024-12-12T09:04:33Z\n  _attrs:  {'id': 'UHJvamVjdDp2MTpweXRvcmNoLWludHJvOnZ0ZWNmdHl3', 'name': 'pytorch-intro', 'entityName': 'vtecftyw', 'createdAt': '2024-12-12T09:04:33Z', 'isBenchmark': False}\n\nbasic-intro:\n  name:    basic-intro\n  entity   vtecftyw\n  path:    ['vtecftyw', 'basic-intro']\n\n  url:     https://wandb.ai/vtecftyw/basic-intro/workspace\n  id:      UHJvamVjdDp2MTpiYXNpYy1pbnRybzp2dGVjZnR5dw==\n  created: 2024-12-12T08:54:34Z\n  _attrs:  {'id': 'UHJvamVjdDp2MTpiYXNpYy1pbnRybzp2dGVjZnR5dw==', 'name': 'basic-intro', 'entityName': 'vtecftyw', 'createdAt': '2024-12-12T08:54:34Z', 'isBenchmark': False}\n\ntut_artifacts:\n  name:    tut_artifacts\n  entity   vtecftyw\n  path:    ['vtecftyw', 'tut_artifacts']\n\n  url:     https://wandb.ai/vtecftyw/tut_artifacts/workspace\n  id:      UHJvamVjdDp2MTp0dXRfYXJ0aWZhY3RzOnZ0ZWNmdHl3\n  created: 2022-09-30T04:39:35Z\n  _attrs:  {'id': 'UHJvamVjdDp2MTp0dXRfYXJ0aWZhY3RzOnZ0ZWNmdHl3', 'name': 'tut_artifacts', 'entityName': 'vtecftyw', 'createdAt': '2022-09-30T04:39:35Z', 'isBenchmark': False}\n\nmetagenomics:\n  name:    metagenomics\n  entity   vtecftyw\n  path:    ['vtecftyw', 'metagenomics']\n\n  url:     https://wandb.ai/vtecftyw/metagenomics/workspace\n  id:      UHJvamVjdDp2MTptZXRhZ2Vub21pY3M6dnRlY2Z0eXc=\n  created: 2022-09-09T10:39:00Z\n  _attrs:  {'id': 'UHJvamVjdDp2MTptZXRhZ2Vub21pY3M6dnRlY2Z0eXc=', 'name': 'metagenomics', 'entityName': 'vtecftyw', 'createdAt': '2022-09-09T10:39:00Z', 'isBenchmark': False}\n\nwand-hello-world-fastai:\n  name:    wand-hello-world-fastai\n  entity   vtecftyw\n  path:    ['vtecftyw', 'wand-hello-world-fastai']\n\n  url:     https://wandb.ai/vtecftyw/wand-hello-world-fastai/workspace\n  id:      UHJvamVjdDp2MTp3YW5kLWhlbGxvLXdvcmxkLWZhc3RhaTp2dGVjZnR5dw==\n  created: 2022-06-14T15:45:17Z\n  _attrs:  {'id': 'UHJvamVjdDp2MTp3YW5kLWhlbGxvLXdvcmxkLWZhc3RhaTp2dGVjZnR5dw==', 'name': 'wand-hello-world-fastai', 'entityName': 'vtecftyw', 'createdAt': '2022-06-14T15:45:17Z', 'isBenchmark': False}\n\n\n\n\nsource\n\n\nget_project\n\n get_project (entity:str, project_name:str)\n\nReturns project object defined by entity and project name\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nentity\nstr\nname of the entity from which the project will be retrieved\n\n\nproject_name\nstr\nname of the project to retrieve\n\n\nReturns\nProject\nProject object\n\n\n\n\np = get_project('vtecftyw', 'tut_artifacts')\n\nprint(type(p))\n\nprint(p.entity,'\\n', p.name,'\\n', p.path,'\\n', p.url)\n\n&lt;class 'wandb.apis.public.projects.Project'&gt;\nvtecftyw \n tut_artifacts \n ['vtecftyw', 'tut_artifacts'] \n https://wandb.ai/vtecftyw/tut_artifacts/workspace\n\n\n\nsource\n\n\nprint_entity_project_list\n\n print_entity_project_list (entity)\n\nPrint the name and url of all projects in entity\n\nprint_entity_project_list('vtecftyw')\n\nList of projects under entity &lt;vtecftyw&gt;\n  0. pytorch-intro                  (url: https://wandb.ai/vtecftyw/pytorch-intro/workspace)\n  1. basic-intro                    (url: https://wandb.ai/vtecftyw/basic-intro/workspace)\n  2. tut_artifacts                  (url: https://wandb.ai/vtecftyw/tut_artifacts/workspace)\n  3. metagenomics                   (url: https://wandb.ai/vtecftyw/metagenomics/workspace)\n  4. wand-hello-world-fastai        (url: https://wandb.ai/vtecftyw/wand-hello-world-fastai/workspace)\n\n\n\nsource\n\n\nproject_artifacts\n\n project_artifacts (entity:str, project_name:str, by_alias:str='latest',\n                    by_type:str=None, by_version:str=None)\n\nReturns all artifacts in project, w/ key info, filtered by alias, types and version + list of artifact types\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nentity\nstr\n\nname of the entity from which to retrieve the artifacts\n\n\nproject_name\nstr\n\nname of the project from which to retrieve the artifacts\n\n\nby_alias\nstr\nlatest\nname of the alias to filter by\n\n\nby_type\nstr\nNone\nname of the artifact type to filter by (optional)\n\n\nby_version\nstr\nNone\nversion to filter by (optional)\n\n\nReturns\nTuple\n\ndf w/ all artifacts and related info; list of artifact types in the project\n\n\n\nproject_artifacts returns:\n\na DataFrame including all the artifacts available under the project (entity/project_name)\na list of all artifact types in the projects\n\n\natx_df, atx_type_list = project_artifacts(\n    entity='metagenomics_sh', \n    project_name='coding-with-nbdev'\n    )\n\natx_type_list\n\n['code', 'cov_sequences', 'sim_reads', 'job']\n\n\n\natx_df\n\n\n\n\n\n\n\n\natx_name\natx_version\natx_type\natx_aliases\nfile_count\ncreated\nupdated\natx_id\n\n\n\n\n0\nsource-coding-with-nbdev-_home_vtec_projects_b...\nv0\ncode\nlatest\n1\n2025-02-01T09:46:40Z\n2025-02-01T09:46:43Z\nQXJ0aWZhY3Q6MTQ4MjA0MzE4Ng==\n\n\n1\ncov_one_sequence:v0\nv0\ncov_sequences\nlatest\n1\n2025-02-01T09:50:13Z\n2025-02-01T09:50:15Z\nQXJ0aWZhY3Q6MTQ4MjA0NjkyNQ==\n\n\n2\ncov_reads_single_1_sequence_150bp:v0\nv0\nsim_reads\nlatest\n2\n2025-02-01T09:52:45Z\n2025-02-01T10:02:12Z\nQXJ0aWZhY3Q6MTQ4MjA0OTY3MA==\n\n\n\n\n\n\n\nThe list of artifacts can be filtered, for instance, by artifact type\n\natx_df, atx_type_list = project_artifacts(\n    entity='metagenomics_sh', \n    project_name='coding-with-nbdev',\n    by_type='cov_sequences'\n    )\n\natx_df\n\n\n\n\n\n\n\n\natx_name\natx_version\natx_type\natx_aliases\nfile_count\ncreated\nupdated\natx_id\n\n\n\n\n0\ncov_one_sequence:v0\nv0\ncov_sequences\nlatest\n1\n2025-02-01T09:50:13Z\n2025-02-01T09:50:15Z\nQXJ0aWZhY3Q6MTQ4MjA0NjkyNQ==\n\n\n\n\n\n\n\n\nsource\n\n\nrun_name_exists\n\n run_name_exists (run_name:str, entity:str, project_name:str)\n\nCheck whether a run with name run_name already exists in entity/project_name\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nrun_name\nstr\nname of the run to check\n\n\nentity\nstr\nname of the entity from which to retrieve the artifacts\n\n\nproject_name\nstr\nname of the project from which to retrieve the artifacts\n\n\nReturns\nbool\nTrue if a run exists with the name run_name, False otherwise\n\n\n\n\nrun_name_exists(\n    run_name='nbdev-test', \n    entity='metagenomics_sh', \n    project_name='coding-with-nbdev'\n    )\n\nTrue\n\n\n\nrun_name_exists(\n    run_name='train_1M', \n    entity='metagenomics_sh', \n    project_name='coding-with-nbdev'\n    )\n\nFalse\n\n\n\nsource\n\n\nunique_run_name\n\n unique_run_name (name_seed:str)\n\nCreate a unique run name by adding a timestamp to the passed seed\n\n\n\n\nType\nDetails\n\n\n\n\nname_seed\nstr\nRun name to which a timestamp will be added\n\n\n\n\nunique_run_name('this_is_a_run_name')\n\n'this_is_a_run_name-250201-1816'",
    "crumbs": [
      "General Code",
      "wandb"
    ]
  },
  {
    "objectID": "wandb.html#technical-notes-for-development-with-nbdev",
    "href": "wandb.html#technical-notes-for-development-with-nbdev",
    "title": "wandb",
    "section": "Technical Notes for development with nbdev",
    "text": "Technical Notes for development with nbdev\nResolve problem with nbdev_export() for this notebook\nWhen using nbdev.nbdev_export() in this notebook, the code exported seems to be old code. In particular, the dependency import section in cell is exported as:\n# %% ../nbs-dev/wandb/run-20221123_121523-2z5ycjrb/tmp/code/01_wandb.ipynb 2\n# Imports all dependencies\n\nimport configparser\nimport numpy as np\nimport psutil\nimport os\nThe hint is in the first line:\n# %% ../nbs-dev/wandb/run-20221123_121523-2z5ycjrb/tmp/code/01_wandb.ipynb 2\nIt shows that the notebook used for exporting is not /nbs-dev/01_wandb.ipynb as it should be. This is because the WandB package creates a local directory /nbs-dev/wandb/ where it keeps local logs and artifacts.\nThe solution is to move the directory where WandB stores local logs outside nbs-dev, which can be done with the dir argument in wandb.Run()\nIllustrating by reproducing the functions from nbdev and a few dependencies\n\nfrom nbdev.config import get_config\nfrom fastcore.xtras import globtastic\nfrom fastcore.meta import delegates\n\n\n# from nbdev.doclinks.py\n\n# line 105\n@delegates(globtastic)\ndef nbglob(path=None, skip_folder_re = '^[_.]', file_glob='*.ipynb', skip_file_re='^[_.]', key='nbs_path', as_path=False, **kwargs):\n    \"Find all files in a directory matching an extension given a config key.\"\n    path = Path(path or get_config()[key])\n    recursive=get_config().recursive\n    res = globtastic(path, file_glob=file_glob, skip_folder_re=skip_folder_re,\n                     skip_file_re=skip_file_re, recursive=recursive, **kwargs)\n    return res.map(Path) if as_path else res\n\n\n# line 131 MODIFIED\ndef modified_nbdev_export(\n    path:str=None, # Path or filename\n    **kwargs):\n    \"Export notebooks in `path` to Python modules\"\n    if os.environ.get('IN_TEST',0): return\n    files = nbglob(path=path, as_path=True, **kwargs).sorted('name')\n#     for f in files: nb_export(f)\n    for f in files: print(f)\n#     add_init(get_config().lib_path)\n#     _build_modidx()\n\nBefore the change:\nmodified_nbdev_export()\n/home/vtec/projects/bio/metagentools/nbs-dev/00_core.ipynb\n/home/vtec/projects/bio/metagentools/nbs-dev/01_wandb.ipynb\n/home/vtec/projects/bio/metagentools/nbs-dev/wandb/run-20221122_182641-1eafsab9/tmp/code/01_wandb.ipynb\n/home/vtec/projects/bio/metagentools/nbs-dev/wandb/run-20221122_180513-1vgzoryt/tmp/code/01_wandb.ipynb\n/home/vtec/projects/bio/metagentools/nbs-dev/wandb/run-20221123_121523-2z5ycjrb/tmp/code/01_wandb.ipynb\n/home/vtec/projects/bio/metagentools/nbs-dev/index.ipynb\nAfter the change\n\nmodified_nbdev_export()\n\n/home/vtec/projects/bio/metagentools/nbs-dev/00_core.ipynb\n/home/vtec/projects/bio/metagentools/nbs-dev/01_wandb.ipynb\n/home/vtec/projects/bio/metagentools/nbs-dev/02_art.ipynb\n/home/vtec/projects/bio/metagentools/nbs-dev/03_bio.ipynb\n/home/vtec/projects/bio/metagentools/nbs-dev/03_cnn_virus_architecture.ipynb\n/home/vtec/projects/bio/metagentools/nbs-dev/03_cnn_virus_data.ipynb\n/home/vtec/projects/bio/metagentools/nbs-dev/03_cnn_virus_utils.ipynb\n/home/vtec/projects/bio/metagentools/nbs-dev/index.ipynb",
    "crumbs": [
      "General Code",
      "wandb"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "metagentorch",
    "section": "",
    "text": "This repo includes the code developed as part of a research project aiming at using deep learning techniques to classify a wide range of viruses based on NG reads.\nThe code is structured into general code and submodules:\nGeneral code is used across the project and can also be reused for other projects:\n\nart: wrapper code to use ART-ILLUMINA read simulator package from a jupyter notebook. MUST BE RUN LOCALLY.\ncore: base classes used accross the package\nwandb: utility classes to open, retrieve, use WandB runs and artifacts accross the project\n\nSubmodules cover code written for a specific part of the project. At the moment, the only submodule is cnn_virus.\n\ncnn_virus: all specific code related to the evaluation and improvement of CNN Virus model\n\narchitecture: all code related to evaluated and trained architectures\ndata: data processing, iterators and parsing functions to handle data\nutils: other utility functions\n\n\nNote: This package started as metagentoools, using keras 2 and tensorflow. Now with keras 3, it is possible to use pytorch as a backend and the package is ported to metagentorch. This is the active package, with metagentools currently no longer maintained.\nDisclaimer: This repository is currently under development and is offered as-is. Please note that certain functions or modules might raise errors or become deprecated without prior notice across different versions. Efforts will be made to clearly indicate in warnings or error messages how to substitute deprecated code. Support may or may not be available, depending on when the request for assistance is made.\nInstallation:\n\npip install metagentorch for the stable version\npip install git+https://github.com/vtecftwy/metagentorch.git@main for the latest committed version\n\nRequirements:\n\npython 3.10 or higher is required for this package\nkeras 3.9 and pytorch 2.5\npackage tested with python 3.10",
    "crumbs": [
      "metagentorch"
    ]
  },
  {
    "objectID": "bio.html",
    "href": "bio.html",
    "title": "bio",
    "section": "",
    "text": "FASTA is a standard format to store genetic sequences.\n\n\nFASTA files count two lines for each sequence (see reference on NCBI site):\n\nline 1: the fasta definition line starting with &gt; and a unique identifier, followed by optional additional information.\nline 2: the sequence line: TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCA ...\n\nThe format of the definition line varies from data source to data source.\nExample:\n    &gt;MCHU - Calmodulin - Human, rabbit, bovine, rat, and chicken\n    MADQLTEEQIAEFKEAFSLFDKDGDGTITTKELGTVMRSLGQNPTEAELQDMINEVDADGNGTIDFP ....\n\n\n\n\nIn this project, we use read sequences from NextGen sequencing tools as well as read sequences simulated using the ART Illumina simulator tool.\nNextGen sequencing tools most often store read sequences in a FASTQ format.\nART Illumina simulator tool outputs both a FASTQ file and an alignment file (ALN) providing information on the original reference sequence it was simulated from. ALN files are also referred to as Clustal Alignnent format.\n\n\nA FASTQ file contains one 4-line block for each read. It includes a readID, followed by both the read sequence and a quality scores, in the following format:\n    @readid \n    sequence_read \n    + \n    base_quality_scores \nExample:\n    @2591237:ncbi:1-60400\n    ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n    +\n    CCCBCGFGBGGGGGGGBGGGGGGGGG&gt;GGG1G=/GGGGGGGGGGGGGGGG\nFor each base in the read sequence, a base quality score is coded by an ASCII character. They are Q Scores or Phred+33 encoded, where the quality score is equal to ASCII code of the character minus 33.\n\nReference information:\n\nART Illumina README file\nIllumina site\n\n\nInformation extraction\nThe following metadata information can be parsed from FASTQ generated by ART Illumina:\nFASTQ file element:\n    @2591237:ncbi:1-60400\n    ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n    +\n    CCCBCGFGBGGGGGGGBGGGGGGGGG&gt;GGG1G=/GGGGGGGGGGGGGGGG\n\nreadid, e.g. 2591237:ncbi:1-60400.\nread_sequence itself, e.g. ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\nbase_quality_score, the Q Score in ASCII, e.g. CCCBCGFGBGGGGGGGBGGGGGGGGG&gt;GGG1G=/GGGGGGGGGGGGGGGG\n\nFurthermore, the following metadata can be parsed from the first line:\n- `refseqid` of the original sequence used for the read, e.g. `2591237:nbci-1`\n- `read_nbr` of the read, e.g. `1-60400\n\n\n\nAn alignment file in ALN format has a Header and main Body parts.\nThe header part includes the command used to generate this file and reference sequence id and length. The header @CM tag for command line, and @SQ for reference sequence.\nA Header always starts with ##ART and ends with ##Header End.\nHeader example, for ALN file generated by ART Illumina:\n    ##ART_Illumina  read_length 50\n    @CM /bin/art_illumina -i /home/vtec/projects/bio/metagentools/data/cov_data/cov_virus_sequences_ten.fa -ss HS25 -l 50 -f 100 -o /home/vtec/projects/bio/metagentools/data/cov_simreads/single_10seq_50bp/single_10seq_50bp -rs 1674660835\n    @SQ 2591237:ncbi:1 [MK211378]   2591237 ncbi    1 [MK211378] 2591237    Coronavirus BtRs-BetaCoV/YN2018D    30213\n    @SQ 11128:ncbi:2 [LC494191] 11128   ncbi    2 [LC494191] 11128  Bovine coronavirus      30942\n    @SQ 31631:ncbi:3 [KY967361] 31631   ncbi    3 [KY967361] 31631  Human coronavirus OC43      30661\n    ##Header End\nFor each reference sequence used to build the reads, we can parse the following metadata info:\n    refseqid:       2591237:ncbi:1\n    ref_seq_length: 30213\nThe Body part contains each read’s alignments in the following format:\n    &gt;refseqid   read_id   aln_start_pos    ref_seq_strand\n    ref_seq_aligned\n    read_seq_aligned \n\naln_start_pos is the alignment start position of the read in the reference sequence.\nref_seq_aligned is the aligned region of reference sequence, which can be from plus strand or minus strand of the reference sequence.\n\nread_seq_aligned is the aligned sequence read, which always in the same orientation of the same read in the corresponding fastq file.\n\naln_start_pos is always relative to the strand of reference sequence. That is, aln_start_pos 10 in the plus (+) strand is different from aln_start_pos 10 in the minus (‐) stand.\nBody example:\n    &gt;2591237:ncbi:1 2591237:ncbi:1-60400    14770   +\n    ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n    ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n    refseqid:          2591237:ncbi:1\n    readid:            2591237:ncbi:1-20100\n    aln_start_pos:     23878\n    ref_seq_strand:    -\n    ref_seq_aligned:   ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n    read_seq_aligned:  ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n\nReference information:\n\nART Illumina README file\nIllumina site\n\n\nThe quality score of a base, also known as a Phred or Q score, is an integer value representing the estimated probability of an error, i.e. the probability that the base is incorrectly sequenced.\nIf \\(P\\) is the error probability and \\(Q\\) is the quality score for a base, then:\n\n\\(P = 10^{-Q/10}\\)\n\\(Q = -10 \\log_{10}(P)\\)\n\nQ scores are often represented as ASCII characters. The rule for converting an ASCII character to an integer varies.\nThe two tables below convert between integer Q scores, ASCII characters and error probabilities.\nASCII_BASE = 33 (table on top) is now almost universally used. In older systems, ASCII_BASE = 64 is also used.\n\n\n\nimage.png\n\n\n(See reference here)\n\n\n\nASCII character is K\nASCII code is 75\nQ score: $Q = 75 - $ \\(= 75 - 33 = 42\\)\nP_error = \\(P = 10^{-Q/10}\\), i.e. \\(10^{-42/10}\\) = 0.00006\n\n\nPython note:\nDecode an ASCII character with ord(c) and encode it with chr(i)\n\n\nord('!'), chr(33), ord('K'), chr(75)\n\n(33, '!', 75, 'K')\n\n\n\nsource\n\n\n\n\n\n q_score2prob_error (char:str, ASCII_base:int=33)\n\nReturn the probability of error for a given Q score encoded as ASCII character\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nchar\nstr\n\nASCII character retrieved from Q Score or Phred value in FASTQ\n\n\nASCII_base\nint\n33\nASCII base. Mostly 33, can be 64 in old FASTQ files\n\n\n\n\n\nSelection of high Q score ASICC code, Q score values and corresponding P error:\nASCII Q Score | Q Score | P error\n--------------|---------|--------\n      K       |   42    | 0.00006 \n      J       |   41    | 0.00008 \n      I       |   40    | 0.00010 \n      H       |   39    | 0.00013 \n      G       |   38    | 0.00016 \n      F       |   37    | 0.00020 \n      E       |   36    | 0.00025 \n      D       |   35    | 0.00032 \n      C       |   34    | 0.00040 \n      B       |   33    | 0.00050 \n      A       |   32    | 0.00063 \n      @       |   31    | 0.00079 \n      ?       |   30    | 0.00100 \n      &gt;       |   29    | 0.00126 \n      =       |   28    | 0.00158 \n\n\nIn the case of the following FASTQ file:\n    @2591237:ncbi:1-60400\n    ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n    +\n    CCCBCGFGBGGGGGGGBGGGGGGGGG&gt;GGG1G=/GGGGGGGGGGGGGGGG\nThe following metadata information can be parsed:\n\nread_id, e.g. 2591237:ncbi:1-60400. The following information can be parsed:\n\nref_seq_id of the original sequence used for the read, e.g. 2591237:nbci-1\nread_nbr of the read, e.g. `60400\n\nsequence_read itself, e.g. ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\nbase_quality_score, the Q Score in ASCII, e.g. CCCBCGFGBGGGGGGGBGGGGGGGGG&gt;GGG1G=/GGGGGGGGGGGGGGGG\n\nThis ASCII encoded Q Score can be converted into probability of error for each bp in the read:\n\nq_scores = 'CCCBCGFGBGGGGGGGBGGGGGGGGG&gt;GGG1G=/GGGGGGGGGGGGGGGG'\nnum_scores = [f\"{q_score2prob_error(char):6.5f}\" for char in q_scores]\nprint('; '.join(num_scores))\nprint(f\"Maximum error probability: {max(num_scores)}\")\n\n0.00040; 0.00040; 0.00040; 0.00050; 0.00040; 0.00016; 0.00020; 0.00016; 0.00050; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00050; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00126; 0.00016; 0.00016; 0.00016; 0.02512; 0.00016; 0.00158; 0.03981; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016\nMaximum error probability: 0.03981\n\n\nIt shows that most bp have a very low probability of error, except a few bp with probability or 2.5% and 3.9%,",
    "crumbs": [
      "General Code",
      "bio"
    ]
  },
  {
    "objectID": "bio.html#sequence-file-formats",
    "href": "bio.html#sequence-file-formats",
    "title": "bio",
    "section": "",
    "text": "FASTA is a standard format to store genetic sequences.\n\n\nFASTA files count two lines for each sequence (see reference on NCBI site):\n\nline 1: the fasta definition line starting with &gt; and a unique identifier, followed by optional additional information.\nline 2: the sequence line: TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCA ...\n\nThe format of the definition line varies from data source to data source.\nExample:\n    &gt;MCHU - Calmodulin - Human, rabbit, bovine, rat, and chicken\n    MADQLTEEQIAEFKEAFSLFDKDGDGTITTKELGTVMRSLGQNPTEAELQDMINEVDADGNGTIDFP ....",
    "crumbs": [
      "General Code",
      "bio"
    ]
  },
  {
    "objectID": "bio.html#read-files",
    "href": "bio.html#read-files",
    "title": "bio",
    "section": "",
    "text": "In this project, we use read sequences from NextGen sequencing tools as well as read sequences simulated using the ART Illumina simulator tool.\nNextGen sequencing tools most often store read sequences in a FASTQ format.\nART Illumina simulator tool outputs both a FASTQ file and an alignment file (ALN) providing information on the original reference sequence it was simulated from. ALN files are also referred to as Clustal Alignnent format.\n\n\nA FASTQ file contains one 4-line block for each read. It includes a readID, followed by both the read sequence and a quality scores, in the following format:\n    @readid \n    sequence_read \n    + \n    base_quality_scores \nExample:\n    @2591237:ncbi:1-60400\n    ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n    +\n    CCCBCGFGBGGGGGGGBGGGGGGGGG&gt;GGG1G=/GGGGGGGGGGGGGGGG\nFor each base in the read sequence, a base quality score is coded by an ASCII character. They are Q Scores or Phred+33 encoded, where the quality score is equal to ASCII code of the character minus 33.\n\nReference information:\n\nART Illumina README file\nIllumina site\n\n\nInformation extraction\nThe following metadata information can be parsed from FASTQ generated by ART Illumina:\nFASTQ file element:\n    @2591237:ncbi:1-60400\n    ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n    +\n    CCCBCGFGBGGGGGGGBGGGGGGGGG&gt;GGG1G=/GGGGGGGGGGGGGGGG\n\nreadid, e.g. 2591237:ncbi:1-60400.\nread_sequence itself, e.g. ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\nbase_quality_score, the Q Score in ASCII, e.g. CCCBCGFGBGGGGGGGBGGGGGGGGG&gt;GGG1G=/GGGGGGGGGGGGGGGG\n\nFurthermore, the following metadata can be parsed from the first line:\n- `refseqid` of the original sequence used for the read, e.g. `2591237:nbci-1`\n- `read_nbr` of the read, e.g. `1-60400\n\n\n\nAn alignment file in ALN format has a Header and main Body parts.\nThe header part includes the command used to generate this file and reference sequence id and length. The header @CM tag for command line, and @SQ for reference sequence.\nA Header always starts with ##ART and ends with ##Header End.\nHeader example, for ALN file generated by ART Illumina:\n    ##ART_Illumina  read_length 50\n    @CM /bin/art_illumina -i /home/vtec/projects/bio/metagentools/data/cov_data/cov_virus_sequences_ten.fa -ss HS25 -l 50 -f 100 -o /home/vtec/projects/bio/metagentools/data/cov_simreads/single_10seq_50bp/single_10seq_50bp -rs 1674660835\n    @SQ 2591237:ncbi:1 [MK211378]   2591237 ncbi    1 [MK211378] 2591237    Coronavirus BtRs-BetaCoV/YN2018D    30213\n    @SQ 11128:ncbi:2 [LC494191] 11128   ncbi    2 [LC494191] 11128  Bovine coronavirus      30942\n    @SQ 31631:ncbi:3 [KY967361] 31631   ncbi    3 [KY967361] 31631  Human coronavirus OC43      30661\n    ##Header End\nFor each reference sequence used to build the reads, we can parse the following metadata info:\n    refseqid:       2591237:ncbi:1\n    ref_seq_length: 30213\nThe Body part contains each read’s alignments in the following format:\n    &gt;refseqid   read_id   aln_start_pos    ref_seq_strand\n    ref_seq_aligned\n    read_seq_aligned \n\naln_start_pos is the alignment start position of the read in the reference sequence.\nref_seq_aligned is the aligned region of reference sequence, which can be from plus strand or minus strand of the reference sequence.\n\nread_seq_aligned is the aligned sequence read, which always in the same orientation of the same read in the corresponding fastq file.\n\naln_start_pos is always relative to the strand of reference sequence. That is, aln_start_pos 10 in the plus (+) strand is different from aln_start_pos 10 in the minus (‐) stand.\nBody example:\n    &gt;2591237:ncbi:1 2591237:ncbi:1-60400    14770   +\n    ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n    ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n    refseqid:          2591237:ncbi:1\n    readid:            2591237:ncbi:1-20100\n    aln_start_pos:     23878\n    ref_seq_strand:    -\n    ref_seq_aligned:   ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n    read_seq_aligned:  ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n\nReference information:\n\nART Illumina README file\nIllumina site\n\n\nThe quality score of a base, also known as a Phred or Q score, is an integer value representing the estimated probability of an error, i.e. the probability that the base is incorrectly sequenced.\nIf \\(P\\) is the error probability and \\(Q\\) is the quality score for a base, then:\n\n\\(P = 10^{-Q/10}\\)\n\\(Q = -10 \\log_{10}(P)\\)\n\nQ scores are often represented as ASCII characters. The rule for converting an ASCII character to an integer varies.\nThe two tables below convert between integer Q scores, ASCII characters and error probabilities.\nASCII_BASE = 33 (table on top) is now almost universally used. In older systems, ASCII_BASE = 64 is also used.\n\n\n\nimage.png\n\n\n(See reference here)\n\n\n\nASCII character is K\nASCII code is 75\nQ score: $Q = 75 - $ \\(= 75 - 33 = 42\\)\nP_error = \\(P = 10^{-Q/10}\\), i.e. \\(10^{-42/10}\\) = 0.00006\n\n\nPython note:\nDecode an ASCII character with ord(c) and encode it with chr(i)\n\n\nord('!'), chr(33), ord('K'), chr(75)\n\n(33, '!', 75, 'K')\n\n\n\nsource\n\n\n\n\n\n q_score2prob_error (char:str, ASCII_base:int=33)\n\nReturn the probability of error for a given Q score encoded as ASCII character\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nchar\nstr\n\nASCII character retrieved from Q Score or Phred value in FASTQ\n\n\nASCII_base\nint\n33\nASCII base. Mostly 33, can be 64 in old FASTQ files\n\n\n\n\n\nSelection of high Q score ASICC code, Q score values and corresponding P error:\nASCII Q Score | Q Score | P error\n--------------|---------|--------\n      K       |   42    | 0.00006 \n      J       |   41    | 0.00008 \n      I       |   40    | 0.00010 \n      H       |   39    | 0.00013 \n      G       |   38    | 0.00016 \n      F       |   37    | 0.00020 \n      E       |   36    | 0.00025 \n      D       |   35    | 0.00032 \n      C       |   34    | 0.00040 \n      B       |   33    | 0.00050 \n      A       |   32    | 0.00063 \n      @       |   31    | 0.00079 \n      ?       |   30    | 0.00100 \n      &gt;       |   29    | 0.00126 \n      =       |   28    | 0.00158 \n\n\nIn the case of the following FASTQ file:\n    @2591237:ncbi:1-60400\n    ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n    +\n    CCCBCGFGBGGGGGGGBGGGGGGGGG&gt;GGG1G=/GGGGGGGGGGGGGGGG\nThe following metadata information can be parsed:\n\nread_id, e.g. 2591237:ncbi:1-60400. The following information can be parsed:\n\nref_seq_id of the original sequence used for the read, e.g. 2591237:nbci-1\nread_nbr of the read, e.g. `60400\n\nsequence_read itself, e.g. ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\nbase_quality_score, the Q Score in ASCII, e.g. CCCBCGFGBGGGGGGGBGGGGGGGGG&gt;GGG1G=/GGGGGGGGGGGGGGGG\n\nThis ASCII encoded Q Score can be converted into probability of error for each bp in the read:\n\nq_scores = 'CCCBCGFGBGGGGGGGBGGGGGGGGG&gt;GGG1G=/GGGGGGGGGGGGGGGG'\nnum_scores = [f\"{q_score2prob_error(char):6.5f}\" for char in q_scores]\nprint('; '.join(num_scores))\nprint(f\"Maximum error probability: {max(num_scores)}\")\n\n0.00040; 0.00040; 0.00040; 0.00050; 0.00040; 0.00016; 0.00020; 0.00016; 0.00050; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00050; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00126; 0.00016; 0.00016; 0.00016; 0.02512; 0.00016; 0.00158; 0.03981; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016; 0.00016\nMaximum error probability: 0.03981\n\n\nIt shows that most bp have a very low probability of error, except a few bp with probability or 2.5% and 3.9%,",
    "crumbs": [
      "General Code",
      "bio"
    ]
  },
  {
    "objectID": "bio.html#standard-dna-codon-table",
    "href": "bio.html#standard-dna-codon-table",
    "title": "bio",
    "section": "Standard DNA codon table",
    "text": "Standard DNA codon table\nThese are the basic codon tables for DNA sequences. They are used to translate DNA sequences into amino acid sequences.\nDirect DNA codon table\n\nInverse DNA codon table\n\n(Source)\n\nsource\n\nStandardDNACodon\n\n StandardDNACodon ()\n\n*Hold standard DNA codon reference information\n\nDNA codon direct table: codon -&gt; amino acid information\nDNA codin inverse table amino acid code (e.g. Cys) -&gt; list of codons*\n\n\ncodons = StandardDNACodon()\n\n\nkey = 'Phe'\ncodons.inverse_table[key]\n\n{'amino acid name': 'Phenylalanine',\n 'amino acid letter': 'F',\n 'codons': ['TTT', 'TTC']}\n\n\n\nkey = 'TTT'\ncodons.direct_table[key]\n\n{'amino acid symbol': 'Phe',\n 'amino acid name': 'Phenylalanine',\n 'amino acid letter': 'F'}\n\n\n\ncodons.amino_acid_symbols[:5]\n\n['Ala', 'Arg', 'Asn', 'Asp', 'Cys']\n\n\n\ncodons.amino_acid_names[:5]\n\n['Alanine', 'Arginine', 'Asparagine', 'Aspartic acid', 'Cysteine']\n\n\n\ncodons.amino_acid_letters[:5]\n\n['A', 'R', 'N', 'D', 'C']",
    "crumbs": [
      "General Code",
      "bio"
    ]
  },
  {
    "objectID": "data_dev/CNN_Virus_data/readme.html",
    "href": "data_dev/CNN_Virus_data/readme.html",
    "title": "metagentorch",
    "section": "",
    "text": "CNN Virus data (development directory version)\nThis directory includes a set of short data files used to test train and inference with the CNN Virus.\n\nFile list and description:\n\n50-mer\n50-mer reads and their labels, in text format with one line per sample. Each line consists of three components, separated by tabs: the 50-mer read or sequence, the virus species label and the position label:\n'TTACNAGCTCCAGTCTAAGATTGTAACTGGCCTTTTTAAAGATTGCTCTA    94    5\\n'\nFiles:\n\n50mer_ds_100_seq: small dataset with 100 reads\n5train_short: small 1000-read subset from the original training dataset for experiments\nval_short: small 500-read subset from the original validation dataset for experiments\n\n\n\n150-mer\n150-mer reads and their labels in text format in a similar format as above:\n'TTCTTTCACCACCACAACCAGTCGGCCGTGGAGAGGCGTCGCCGCGTCTCGTTCGTCGAGGCCGATCGACTGCCGCATGAGAGCGGGTGGTATTCTTCCGAAGACGACGGAGACCGGGACGGTGATGAGGAAACTGGAGAGAGCCACAAC    6    0\\n'\nFiles:\n\n150mer_ds_100_reads: small subset of 100 reads from original ICTV_150mer_benchmarking file\n\n\n\nOther files:\n\nvirus_name_mapping: mapping between virus species and their numerical label\nweight_of_classes: weights for each virus species class in the training dataset"
  },
  {
    "objectID": "data_dev/ncbi/simreads/readme.html",
    "href": "data_dev/ncbi/simreads/readme.html",
    "title": "metagentorch",
    "section": "",
    "text": "NCBI simulated reads\nThis directory includes all sets of simulated read sequence files generated from NCBI viral sequences using ARC Illumina.\nthis-directory\n    |--cov\n    |    |\n    |    |--single_10seq_50bp\n    |    |    |--single_10seq_50bp.fq\n    |    |    |--single_10seq_50bp.alnEnd\n    |    |-- ...\n    |    |--single_100seq_150bp\n    |    |    |--single_100seq_150bp.fq\n    |    |    |--single_100seq_150bp.aln\n    |    |--paired_100seq_50bp\n    |    |    |--paired_100seq_50bp2.aln\n    |    |    |--paired_100seq_50bp1.aln\n    |    |    |--paired_100seq_50bp2.fq\n    |    |    |--paired_100seq_50bp1.fq\n    |    |-- ...\n    |    |\n    |---yf\n    |    |\n    |    |--yf_AY968064-single-150bp\n    |    |    |--yf_AY968064-single-1seq-150bp.fq\n    |    |    |--yf_AY968064-single-1seq-150bp.aln\n    |    |\n    |--mRhiFer1\n    |    |--mRhiFer1_v1.p.dna_rm.primary_assembly.1\n    |    |    |--mRhiFer1_v1.p.dna_rm.primary_assembly.1.fq\n    |    |    |--mRhiFer1_v1.p.dna_rm.primary_assembly.1.aln\n    |    |\n\nThis directory includes several subdirectories, each for one virus, e.g. cov for corona, yf for yellow fever.\nIn each virus subdirectory, several simreads directory includes simulated reads with various parameters, named as &lt;method&gt;_&lt;nb-seq&gt;_&lt;nb-bp&gt; where” - &lt;method&gt; is either single or paired depending on the simulation method - &lt;nb-seq&gt; is the number of reference sequences used for simulation, and refers to the fa file used - &lt;nb-bp&gt; is the number of base pairs used to simulate reads\nEach sub-directory includes simreads files made using a simulation method and a specific number of reference sequences. - xxx.fq and xxx.aln files when method is single - xxx1.fq, xxx2.fq, xxx1.aln and xxx2.aln files when method is paired.\nExample: - paired_10seq_50bp means that the simreads were generated by using the paired method to simulate 50-bp reads, and using the fa file cov_virus_sequences_010-seqs.fa. - single_100seq_50bp means that the simreads were generated by using the single method to simulate 50-bp reads, and using the fa file cov_virus_sequences_100-seqs.fa. Note that this generated 20,660,104 reads !\n\nSimread file formats\nSimulated reads information is split between two files: - FASTQ (.fq) files providing the read sequences and their ASCII quality scores - ALN (.aln) files with alignment information\n\nFASTQ (.fq)\nFASTQ files generated by ART Illumina have the following structure (showing 5 reads), with 4 lines for each read:\n@2591237:ncbi:1-60400\nACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n+\nCCCBCGFGBGGGGGGGBGGGGGGGGG&gt;GGG1G=/GGGGGGGGGGGGGGGG\n@2591237:ncbi:1-60399\nGATCAATGTGGCATCTACAATACAGACAGCATGAAGCACCACCAAAGGAC\n+\nBCBCCFGGGGGGGG1CGGGG&lt;GGBGGGGGFGCGGGGGGDGGG/GG1GGGG\n@2591237:ncbi:1-60398\nATCTACCAGTGGTAGATGGGTTCTTAATAATGAACATTATAGAGCTCTAC\n+\nCCCCCGGGEGG1GGF1G/GGEGGGGGGGGGGGGFFGGGGGGGGGGDGGDG\n@2591237:ncbi:1-60397\nCGTAAAGTAGAGGCTGTATGGTAGCTAGCACAAATGCCAGCACCAATAGG\n+\nBCCCCGGGFGGGGGGFGGGGFGG1GGGGGGG&gt;GG1GGGGGGGGGGE&lt;GGG\n@2591237:ncbi:1-60396\nGGTATCGGGTATCTCCTGCATCAATGCAAGGTCTTACAAAGATAAATACT\n+\nCBCCCGGG@CGGGGGGGGGGGG=GFGGGGDGGGFG1GGGGGGGG@GGGGG\nThe following information can be parsed from the each read sequence in the FASTQ file:\n\nLine 1: readid, a unique ID for the read, under for format @readid\nLine 2: readseq, the sequence of the read\nLine 3: a separator +\nLine 4: read_qscores, the base quality scores encoded in ASCII\n\nExample:\n@2591237:ncbi:1-60400\nACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n+\nCCCBCGFGBGGGGGGGBGGGGGGGGG&gt;GGG1G=/GGGGGGGGGGGGGGGG\n\nreadid = 2591237:ncbi:1-60400\nreadseq = ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG, a 50 bp read\nread_qscores = CCCBCGFGBGGGGGGGBGGGGGGGGG&gt;GGG1G=/GGGGGGGGGGGGGGGG\n\n\n\n\nALN (.aln)\nALN files generated by ART Illumina consist of : - a header with the ART-Ilumina command used for the simulation (@CM) and info on each of the reference sequences used for the simulations (@SQ). Header always starts with ##ART_Illumina and ends with ##Header End : - the body with 3 lines for each read: 1. definition line with readid, - reference sequence identification number refseqid, - the position in the read in the reference sequence aln_start_pos - the strand the read was taken from ref_seq_strand. + for coding strand and - for template strand 2. aligned reference sequence, that is the sequence segment in the original reference corresponding to the read 3. aligned read sequence, that is the simmulated read sequence, where each bp corresponds to the reference sequence bp in the same position.\nExample of a ALN file generated by ART Illumina (showing 5 reads):\n##ART_Illumina    read_length    50\n@CM    /bin/art_illumina -i /home/vtec/projects/bio/metagentools/data/cov_data/cov_virus_sequences_ten.fa -ss HS25 -l 50 -f 100 -o /home/vtec/projects/bio/metagentools/data/cov_simreads/single_10seq_50bp/single_10seq_50bp -rs 1674660835\n@SQ    2591237:ncbi:1 1   MK211378    2591237    ncbi    1     Coronavirus BtRs-BetaCoV/YN2018D    30213\n@SQ    11128:ncbi:2   2   LC494191    11128    ncbi    2     Bovine coronavirus    30942\n@SQ    31631:ncbi:3   3   KY967361    31631    ncbi    3     Human coronavirus OC43        30661\n@SQ    277944:ncbi:4  4   LC654455    277944    ncbi    4     Human coronavirus NL63    27516\n@SQ    11120:ncbi:5   5   MN987231    11120    ncbi    5     Infectious bronchitis virus    27617\n@SQ    28295:ncbi:6   6   KU893866    28295    ncbi    6     Porcine epidemic diarrhea virus    28043\n@SQ    28295:ncbi:7   7   KJ645638    28295    ncbi    7     Porcine epidemic diarrhea virus    27998\n@SQ    28295:ncbi:8   8   KJ645678    28295    ncbi    8     Porcine epidemic diarrhea virus    27998\n@SQ    28295:ncbi:9   9   KR873434    28295    ncbi    9     Porcine epidemic diarrhea virus    28038\n@SQ    1699095:ncbi:10 10  KT368904    1699095    ncbi    10     Camel alphacoronavirus    27395\n##Header End\n&gt;2591237:ncbi:1    2591237:ncbi:1-60400    14770    +\nACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\nACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n&gt;2591237:ncbi:1    2591237:ncbi:1-60399    17012    -\nGATCAATGTGGCATCTACAATACAGACAGCATGAAGCACCACCAAAGGAC\nGATCAATGTGGCATCTACAATACAGACAGCATGAAGCACCACCAAAGGAC\n&gt;2591237:ncbi:1    2591237:ncbi:1-60398    9188    +\nATCTACCAGTGGTAGATGGGTTCTTAATAATGAACATTATAGAGCTCTAC\nATCTACCAGTGGTAGATGGGTTCTTAATAATGAACATTATAGAGCTCTAC\n....."
  },
  {
    "objectID": "data_dev/saved/readme.html",
    "href": "data_dev/saved/readme.html",
    "title": "metagentorch",
    "section": "",
    "text": "Saved data related to models\nThis directory includes all data related to models and saved: - saved model parameters - saved datasets\nFor example: - cnn_virus_original/pretrained_model.h5 is the saved model parameters for the CNN Virus model"
  },
  {
    "objectID": "cnn_virus_data.html",
    "href": "cnn_virus_data.html",
    "title": "data",
    "section": "",
    "text": "There are many different types of files and datasets for this project. All data are located in directory data, under the project root. The following is an overview of the main types of data and where they sit in the directory tree.\n\n\nLet’s create an instance of ProjectFileSystem to access the data directory and its content.\n\npfs = ProjectFileSystem()\npfs.info()\n\nRunning linux on local computer\nDevice's home directory: /home/vtec\nProject file structure:\n - Root ........ /home/vtec/projects/bio/metagentorch \n - Data Dir .... /home/vtec/projects/bio/metagentorch/nbs-dev/data_dev \n - Notebooks ... /home/vtec/projects/bio/metagentorch/nbs\n\n\nNote that we have setup this notebook to use the development data directory metagentorch/nbs-dev/data_dev and not the standard metagentorch/data.\nIn each directory, a readme.md file or another *.md file can be added to provide a description of the directory content.\nThese readme.md files can be conveniently accessed using the .readme(path) method available with the class ProjectFileSystem (from core module).\n\npfs.readme()\n\n\n\n\nReadMe file for directory nbs-dev/data_dev:\n\n\n\n\n\n\nThis directory includes all data required to validate and test this package code.\ndata_dev\n |--- CNN_Virus_data\n |     |--- 50mer_ds_100_seq\n |     |--- 150mer_ds_100_seq\n |     |--- train_short\n |     |--- val_short\n |     |--- weight_of_classes\n |--- ncbi\n |     |--- infer_results\n |     |     |--- cnn_virus\n |     |     |--- csv\n |     |     |--- xlsx\n |     |     |--- testdb.db\n |     |--- refsequences\n |     |     |--- cov\n |     |     |     |--cov_virus_sequence_one_metadata.json\n |     |     |     |--sequences_two_no_matching_rule.fa\n |     |     |     |--another_sequence.fa\n |     |     |     |--cov_virus_sequences_two.fa\n |     |     |     |--cov_virus_sequences_two_metadata.json\n |     |     |     |--cov_virus_sequence_one.fa\n |     |     |     |--single_1seq_150bp\n |     |     |     |    |--single_1seq_150bp.fq\n |     |     |     |    |--single_1seq_150bp.aln\n |     |     |     |--paired_1seq_150bp\n |     |     |     |    |--paired_1seq_150bp2.aln\n |     |     |     |    |--paired_1seq_150bp2.fq\n |     |     |     |    |--paired_1seq_150bp1.fq \n |     |     |     |    |--paired_1seq_150bp1.aln \n |     |--- simreads\n |     |     |--- cov\n |     |     |     |--- paired_1seq_50bp\n |     |     |     |      |--- paired_1seq_50bp_1.aln\n |     |     |     |      |--- paired_1seq_50bp_1.fq\n |     |     |     |--- single_1seq_50bp\n |     |     |     |      |--- single_1seq_50bp_1.aln\n |     |     |     |      |--- single_1seq_50bp_1.fq\n |     |     |--- cov\n |     |     |     |--single_1seq_50bp\n |     |     |     |    |--single_1seq_50bp.aln\n |     |     |     |    |--single_1seq_50bp.fq\n |     |     |     |--single_1seq_150bp\n |     |     |     |    |--single_1seq_150bp.fq\n |     |     |     |    |--single_1seq_150bp.aln\n |     |     |     |--paired_1seq_150bp\n |     |     |     |    |--paired_1seq_150bp2.aln\n |     |     |     |    |--paired_1seq_150bp2.fq\n |     |     |     |    |--paired_1seq_150bp1.fq\n |     |     |     |    |--paired_1seq_150bp1.aln\n |--- saved           \n |--- readme.md               \n\n\n\n\n\n\n\n\nThe CNN Virus team provides training and validation/test datasets for their model. These datasets and the pretrained model parameters are are available on their Google drive shared directory here. A set of shortened datasets are also available in the data-dev development data directory for testing the code.\n\npfs.readme(pfs.data/'CNN_Virus_data')\n\n\n\n\nReadMe file for directory nbs-dev/data_dev/CNN_Virus_data:\n\n\n\n\n\n\nThis directory includes a set of short data files used to test train and inference with the CNN Virus.\n\n\n\n\n50-mer reads and their labels, in text format with one line per sample. Each line consists of three components, separated by tabs: the 50-mer read or sequence, the virus species label and the position label:\n'TTACNAGCTCCAGTCTAAGATTGTAACTGGCCTTTTTAAAGATTGCTCTA    94    5\\n'\nFiles:\n\n50mer_ds_100_seq: small dataset with 100 reads\n5train_short: small 1000-read subset from the original training dataset for experiments\nval_short: small 500-read subset from the original validation dataset for experiments\n\n\n\n\n150-mer reads and their labels in text format in a similar format as above:\n'TTCTTTCACCACCACAACCAGTCGGCCGTGGAGAGGCGTCGCCGCGTCTCGTTCGTCGAGGCCGATCGACTGCCGCATGAGAGCGGGTGGTATTCTTCCGAAGACGACGGAGACCGGGACGGTGATGAGGAAACTGGAGAGAGCCACAAC    6    0\\n'\nFiles:\n\n150mer_ds_100_reads: small subset of 100 reads from original ICTV_150mer_benchmarking file\n\n\n\n\n\nvirus_name_mapping: mapping between virus species and their numerical label\nweight_of_classes: weights for each virus species class in the training dataset\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n OriginalLabels (p2mapping:pathlib.Path|None=None)\n\nConverts between labels and species name as per original training dataset\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np2mapping\npathlib.Path | None\nNone\nPath to the mapping file. Uses virus_name_mapping by default\n\n\n\nOriginal data include 187 viruses, with label from 0 to 186.\nWith the class method .label2species(n) and .species2label(species) we can convert between the label and the species name.\n\nspecies = OriginalLabels()\nfor n in [0, 94, 117, 118]:\n    print(f\"{n:3d} -&gt; {species.label2species(n)}\")\n\n  0 -&gt; Variola_virus\n 94 -&gt; Middle_East_respiratory_syndrome-related_coronavirus\n117 -&gt; Severe_acute_respiratory_syndrome-related_coronavirus\n118 -&gt; Yellow_fever_virus\n\n\n\nfor s in ['Variola_virus', 'Yellow_fever_virus']:\n    print(f\"{s:20s} -&gt; {species.species2label(s)}\")\n\nVariola_virus        -&gt; 0\nYellow_fever_virus   -&gt; 118\n\n\nWhen looking for a numerical specie label it is often more convenient to use a partial name, and the method .search(species) because we do not need to know the full specie name.\n\nsource\n\n\n\n\n OriginalLabels.search (s:str)\n\nPrints all species whose name contains the passed string, with their numerical label\n\n\n\n\nType\nDetails\n\n\n\n\ns\nstr\nstring to search through all original virus species\n\n\n\n\nspecies.search('fever')\n\nSandfly_fever_Naples_phlebovirus. Label: 35\nCrimean-Congo_hemorrhagic_fever_orthonairovirus. Label: 76\nYellow_fever_virus. Label: 118\nRift_Valley_fever_phlebovirus. Label: 156\n\n\n\n\n\n\nWe simulate reads using many reference sequences from the NCBI GenBank database. We group all reference sequences as well as all reads simulated from these reference sequences under the ncbi directory.\n\npfs = ProjectFileSystem()\n\n\npfs.readme(pfs.data / 'ncbi')\n\n\n\n\nReadMe file for directory nbs-dev/data_dev/ncbi:\n\n\n\n\n\n\nThis directory includes all data related to the work done with reference sequences from NCBI.\nThe data is organized in the following subfolders:\n\nrefsequences: reference CoV sequences downloaded from NCBI, and related metadata\nsimreads: all data from simulated reads, using ART Illumina simulator and the reference sequences\ninfer_results: results from the inference using models with the simulated reads\nds: datasets in proper format for training or inference/prediction using the CNN Virus model\n\n\n\n\n\n\n\npfs.readme(pfs.data / 'ncbi/refsequences')\n\n\n\n\nReadMe file for directory nbs-dev/data_dev/ncbi/refsequences:\n\n\nNo markdown file in this folder\n\n\n\npfs.readme(pfs.data / 'ncbi/simreads')\n\n\n\n\nReadMe file for directory nbs-dev/data_dev/ncbi/simreads:\n\n\n\n\n\n\nThis directory includes all sets of simulated read sequence files generated from NCBI viral sequences using ARC Illumina.\nthis-directory\n    |--cov\n    |    |\n    |    |--single_10seq_50bp\n    |    |    |--single_10seq_50bp.fq\n    |    |    |--single_10seq_50bp.alnEnd\n    |    |-- ...\n    |    |--single_100seq_150bp\n    |    |    |--single_100seq_150bp.fq\n    |    |    |--single_100seq_150bp.aln\n    |    |--paired_100seq_50bp\n    |    |    |--paired_100seq_50bp2.aln\n    |    |    |--paired_100seq_50bp1.aln\n    |    |    |--paired_100seq_50bp2.fq\n    |    |    |--paired_100seq_50bp1.fq\n    |    |-- ...\n    |    |\n    |---yf\n    |    |\n    |    |--yf_AY968064-single-150bp\n    |    |    |--yf_AY968064-single-1seq-150bp.fq\n    |    |    |--yf_AY968064-single-1seq-150bp.aln\n    |    |\n    |--mRhiFer1\n    |    |--mRhiFer1_v1.p.dna_rm.primary_assembly.1\n    |    |    |--mRhiFer1_v1.p.dna_rm.primary_assembly.1.fq\n    |    |    |--mRhiFer1_v1.p.dna_rm.primary_assembly.1.aln\n    |    |\n\nThis directory includes several subdirectories, each for one virus, e.g. cov for corona, yf for yellow fever.\nIn each virus subdirectory, several simreads directory includes simulated reads with various parameters, named as &lt;method&gt;_&lt;nb-seq&gt;_&lt;nb-bp&gt; where” - &lt;method&gt; is either single or paired depending on the simulation method - &lt;nb-seq&gt; is the number of reference sequences used for simulation, and refers to the fa file used - &lt;nb-bp&gt; is the number of base pairs used to simulate reads\nEach sub-directory includes simreads files made using a simulation method and a specific number of reference sequences. - xxx.fq and xxx.aln files when method is single - xxx1.fq, xxx2.fq, xxx1.aln and xxx2.aln files when method is paired.\nExample: - paired_10seq_50bp means that the simreads were generated by using the paired method to simulate 50-bp reads, and using the fa file cov_virus_sequences_010-seqs.fa. - single_100seq_50bp means that the simreads were generated by using the single method to simulate 50-bp reads, and using the fa file cov_virus_sequences_100-seqs.fa. Note that this generated 20,660,104 reads !\n\n\nSimulated reads information is split between two files: - FASTQ (.fq) files providing the read sequences and their ASCII quality scores - ALN (.aln) files with alignment information\n\n\nFASTQ files generated by ART Illumina have the following structure (showing 5 reads), with 4 lines for each read:\n@2591237:ncbi:1-60400\nACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n+\nCCCBCGFGBGGGGGGGBGGGGGGGGG&gt;GGG1G=/GGGGGGGGGGGGGGGG\n@2591237:ncbi:1-60399\nGATCAATGTGGCATCTACAATACAGACAGCATGAAGCACCACCAAAGGAC\n+\nBCBCCFGGGGGGGG1CGGGG&lt;GGBGGGGGFGCGGGGGGDGGG/GG1GGGG\n@2591237:ncbi:1-60398\nATCTACCAGTGGTAGATGGGTTCTTAATAATGAACATTATAGAGCTCTAC\n+\nCCCCCGGGEGG1GGF1G/GGEGGGGGGGGGGGGFFGGGGGGGGGGDGGDG\n@2591237:ncbi:1-60397\nCGTAAAGTAGAGGCTGTATGGTAGCTAGCACAAATGCCAGCACCAATAGG\n+\nBCCCCGGGFGGGGGGFGGGGFGG1GGGGGGG&gt;GG1GGGGGGGGGGE&lt;GGG\n@2591237:ncbi:1-60396\nGGTATCGGGTATCTCCTGCATCAATGCAAGGTCTTACAAAGATAAATACT\n+\nCBCCCGGG@CGGGGGGGGGGGG=GFGGGGDGGGFG1GGGGGGGG@GGGGG\nThe following information can be parsed from the each read sequence in the FASTQ file:\n\nLine 1: readid, a unique ID for the read, under for format @readid\nLine 2: readseq, the sequence of the read\nLine 3: a separator +\nLine 4: read_qscores, the base quality scores encoded in ASCII\n\nExample:\n@2591237:ncbi:1-60400\nACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n+\nCCCBCGFGBGGGGGGGBGGGGGGGGG&gt;GGG1G=/GGGGGGGGGGGGGGGG\n\nreadid = 2591237:ncbi:1-60400\nreadseq = ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG, a 50 bp read\nread_qscores = CCCBCGFGBGGGGGGGBGGGGGGGGG&gt;GGG1G=/GGGGGGGGGGGGGGGG\n\n\n\n\n\nALN files generated by ART Illumina consist of : - a header with the ART-Ilumina command used for the simulation (@CM) and info on each of the reference sequences used for the simulations (@SQ). Header always starts with ##ART_Illumina and ends with ##Header End : - the body with 3 lines for each read: 1. definition line with readid, - reference sequence identification number refseqid, - the position in the read in the reference sequence aln_start_pos - the strand the read was taken from ref_seq_strand. + for coding strand and - for template strand 2. aligned reference sequence, that is the sequence segment in the original reference corresponding to the read 3. aligned read sequence, that is the simmulated read sequence, where each bp corresponds to the reference sequence bp in the same position.\nExample of a ALN file generated by ART Illumina (showing 5 reads):\n##ART_Illumina    read_length    50\n@CM    /bin/art_illumina -i /home/vtec/projects/bio/metagentools/data/cov_data/cov_virus_sequences_ten.fa -ss HS25 -l 50 -f 100 -o /home/vtec/projects/bio/metagentools/data/cov_simreads/single_10seq_50bp/single_10seq_50bp -rs 1674660835\n@SQ    2591237:ncbi:1 1   MK211378    2591237    ncbi    1     Coronavirus BtRs-BetaCoV/YN2018D    30213\n@SQ    11128:ncbi:2   2   LC494191    11128    ncbi    2     Bovine coronavirus    30942\n@SQ    31631:ncbi:3   3   KY967361    31631    ncbi    3     Human coronavirus OC43        30661\n@SQ    277944:ncbi:4  4   LC654455    277944    ncbi    4     Human coronavirus NL63    27516\n@SQ    11120:ncbi:5   5   MN987231    11120    ncbi    5     Infectious bronchitis virus    27617\n@SQ    28295:ncbi:6   6   KU893866    28295    ncbi    6     Porcine epidemic diarrhea virus    28043\n@SQ    28295:ncbi:7   7   KJ645638    28295    ncbi    7     Porcine epidemic diarrhea virus    27998\n@SQ    28295:ncbi:8   8   KJ645678    28295    ncbi    8     Porcine epidemic diarrhea virus    27998\n@SQ    28295:ncbi:9   9   KR873434    28295    ncbi    9     Porcine epidemic diarrhea virus    28038\n@SQ    1699095:ncbi:10 10  KT368904    1699095    ncbi    10     Camel alphacoronavirus    27395\n##Header End\n&gt;2591237:ncbi:1    2591237:ncbi:1-60400    14770    +\nACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\nACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n&gt;2591237:ncbi:1    2591237:ncbi:1-60399    17012    -\nGATCAATGTGGCATCTACAATACAGACAGCATGAAGCACCACCAAAGGAC\nGATCAATGTGGCATCTACAATACAGACAGCATGAAGCACCACCAAAGGAC\n&gt;2591237:ncbi:1    2591237:ncbi:1-60398    9188    +\nATCTACCAGTGGTAGATGGGTTCTTAATAATGAACATTATAGAGCTCTAC\nATCTACCAGTGGTAGATGGGTTCTTAATAATGAACATTATAGAGCTCTAC\n.....\n\n\n\n\n\n\n\n\n\n\npfs = ProjectFileSystem()\n\n\npfs.readme(pfs.data / 'saved')\n\n\n\n\nReadMe file for directory nbs-dev/data_dev/saved:\n\n\n\n\n\n\nThis directory includes all data related to models and saved: - saved model parameters - saved datasets\nFor example: - cnn_virus_original/pretrained_model.h5 is the saved model parameters for the CNN Virus model",
    "crumbs": [
      "CNN Virus",
      "data"
    ]
  },
  {
    "objectID": "cnn_virus_data.html#data-directory",
    "href": "cnn_virus_data.html#data-directory",
    "title": "data",
    "section": "",
    "text": "Let’s create an instance of ProjectFileSystem to access the data directory and its content.\n\npfs = ProjectFileSystem()\npfs.info()\n\nRunning linux on local computer\nDevice's home directory: /home/vtec\nProject file structure:\n - Root ........ /home/vtec/projects/bio/metagentorch \n - Data Dir .... /home/vtec/projects/bio/metagentorch/nbs-dev/data_dev \n - Notebooks ... /home/vtec/projects/bio/metagentorch/nbs\n\n\nNote that we have setup this notebook to use the development data directory metagentorch/nbs-dev/data_dev and not the standard metagentorch/data.\nIn each directory, a readme.md file or another *.md file can be added to provide a description of the directory content.\nThese readme.md files can be conveniently accessed using the .readme(path) method available with the class ProjectFileSystem (from core module).\n\npfs.readme()\n\n\n\n\nReadMe file for directory nbs-dev/data_dev:\n\n\n\n\n\n\nThis directory includes all data required to validate and test this package code.\ndata_dev\n |--- CNN_Virus_data\n |     |--- 50mer_ds_100_seq\n |     |--- 150mer_ds_100_seq\n |     |--- train_short\n |     |--- val_short\n |     |--- weight_of_classes\n |--- ncbi\n |     |--- infer_results\n |     |     |--- cnn_virus\n |     |     |--- csv\n |     |     |--- xlsx\n |     |     |--- testdb.db\n |     |--- refsequences\n |     |     |--- cov\n |     |     |     |--cov_virus_sequence_one_metadata.json\n |     |     |     |--sequences_two_no_matching_rule.fa\n |     |     |     |--another_sequence.fa\n |     |     |     |--cov_virus_sequences_two.fa\n |     |     |     |--cov_virus_sequences_two_metadata.json\n |     |     |     |--cov_virus_sequence_one.fa\n |     |     |     |--single_1seq_150bp\n |     |     |     |    |--single_1seq_150bp.fq\n |     |     |     |    |--single_1seq_150bp.aln\n |     |     |     |--paired_1seq_150bp\n |     |     |     |    |--paired_1seq_150bp2.aln\n |     |     |     |    |--paired_1seq_150bp2.fq\n |     |     |     |    |--paired_1seq_150bp1.fq \n |     |     |     |    |--paired_1seq_150bp1.aln \n |     |--- simreads\n |     |     |--- cov\n |     |     |     |--- paired_1seq_50bp\n |     |     |     |      |--- paired_1seq_50bp_1.aln\n |     |     |     |      |--- paired_1seq_50bp_1.fq\n |     |     |     |--- single_1seq_50bp\n |     |     |     |      |--- single_1seq_50bp_1.aln\n |     |     |     |      |--- single_1seq_50bp_1.fq\n |     |     |--- cov\n |     |     |     |--single_1seq_50bp\n |     |     |     |    |--single_1seq_50bp.aln\n |     |     |     |    |--single_1seq_50bp.fq\n |     |     |     |--single_1seq_150bp\n |     |     |     |    |--single_1seq_150bp.fq\n |     |     |     |    |--single_1seq_150bp.aln\n |     |     |     |--paired_1seq_150bp\n |     |     |     |    |--paired_1seq_150bp2.aln\n |     |     |     |    |--paired_1seq_150bp2.fq\n |     |     |     |    |--paired_1seq_150bp1.fq\n |     |     |     |    |--paired_1seq_150bp1.aln\n |--- saved           \n |--- readme.md",
    "crumbs": [
      "CNN Virus",
      "data"
    ]
  },
  {
    "objectID": "cnn_virus_data.html#original-datasets",
    "href": "cnn_virus_data.html#original-datasets",
    "title": "data",
    "section": "",
    "text": "The CNN Virus team provides training and validation/test datasets for their model. These datasets and the pretrained model parameters are are available on their Google drive shared directory here. A set of shortened datasets are also available in the data-dev development data directory for testing the code.\n\npfs.readme(pfs.data/'CNN_Virus_data')\n\n\n\n\nReadMe file for directory nbs-dev/data_dev/CNN_Virus_data:\n\n\n\n\n\n\nThis directory includes a set of short data files used to test train and inference with the CNN Virus.\n\n\n\n\n50-mer reads and their labels, in text format with one line per sample. Each line consists of three components, separated by tabs: the 50-mer read or sequence, the virus species label and the position label:\n'TTACNAGCTCCAGTCTAAGATTGTAACTGGCCTTTTTAAAGATTGCTCTA    94    5\\n'\nFiles:\n\n50mer_ds_100_seq: small dataset with 100 reads\n5train_short: small 1000-read subset from the original training dataset for experiments\nval_short: small 500-read subset from the original validation dataset for experiments\n\n\n\n\n150-mer reads and their labels in text format in a similar format as above:\n'TTCTTTCACCACCACAACCAGTCGGCCGTGGAGAGGCGTCGCCGCGTCTCGTTCGTCGAGGCCGATCGACTGCCGCATGAGAGCGGGTGGTATTCTTCCGAAGACGACGGAGACCGGGACGGTGATGAGGAAACTGGAGAGAGCCACAAC    6    0\\n'\nFiles:\n\n150mer_ds_100_reads: small subset of 100 reads from original ICTV_150mer_benchmarking file\n\n\n\n\n\nvirus_name_mapping: mapping between virus species and their numerical label\nweight_of_classes: weights for each virus species class in the training dataset\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n OriginalLabels (p2mapping:pathlib.Path|None=None)\n\nConverts between labels and species name as per original training dataset\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np2mapping\npathlib.Path | None\nNone\nPath to the mapping file. Uses virus_name_mapping by default\n\n\n\nOriginal data include 187 viruses, with label from 0 to 186.\nWith the class method .label2species(n) and .species2label(species) we can convert between the label and the species name.\n\nspecies = OriginalLabels()\nfor n in [0, 94, 117, 118]:\n    print(f\"{n:3d} -&gt; {species.label2species(n)}\")\n\n  0 -&gt; Variola_virus\n 94 -&gt; Middle_East_respiratory_syndrome-related_coronavirus\n117 -&gt; Severe_acute_respiratory_syndrome-related_coronavirus\n118 -&gt; Yellow_fever_virus\n\n\n\nfor s in ['Variola_virus', 'Yellow_fever_virus']:\n    print(f\"{s:20s} -&gt; {species.species2label(s)}\")\n\nVariola_virus        -&gt; 0\nYellow_fever_virus   -&gt; 118\n\n\nWhen looking for a numerical specie label it is often more convenient to use a partial name, and the method .search(species) because we do not need to know the full specie name.\n\nsource\n\n\n\n\n OriginalLabels.search (s:str)\n\nPrints all species whose name contains the passed string, with their numerical label\n\n\n\n\nType\nDetails\n\n\n\n\ns\nstr\nstring to search through all original virus species\n\n\n\n\nspecies.search('fever')\n\nSandfly_fever_Naples_phlebovirus. Label: 35\nCrimean-Congo_hemorrhagic_fever_orthonairovirus. Label: 76\nYellow_fever_virus. Label: 118\nRift_Valley_fever_phlebovirus. Label: 156",
    "crumbs": [
      "CNN Virus",
      "data"
    ]
  },
  {
    "objectID": "cnn_virus_data.html#ncbi-reference-sequences",
    "href": "cnn_virus_data.html#ncbi-reference-sequences",
    "title": "data",
    "section": "",
    "text": "We simulate reads using many reference sequences from the NCBI GenBank database. We group all reference sequences as well as all reads simulated from these reference sequences under the ncbi directory.\n\npfs = ProjectFileSystem()\n\n\npfs.readme(pfs.data / 'ncbi')\n\n\n\n\nReadMe file for directory nbs-dev/data_dev/ncbi:\n\n\n\n\n\n\nThis directory includes all data related to the work done with reference sequences from NCBI.\nThe data is organized in the following subfolders:\n\nrefsequences: reference CoV sequences downloaded from NCBI, and related metadata\nsimreads: all data from simulated reads, using ART Illumina simulator and the reference sequences\ninfer_results: results from the inference using models with the simulated reads\nds: datasets in proper format for training or inference/prediction using the CNN Virus model\n\n\n\n\n\n\n\npfs.readme(pfs.data / 'ncbi/refsequences')\n\n\n\n\nReadMe file for directory nbs-dev/data_dev/ncbi/refsequences:\n\n\nNo markdown file in this folder\n\n\n\npfs.readme(pfs.data / 'ncbi/simreads')\n\n\n\n\nReadMe file for directory nbs-dev/data_dev/ncbi/simreads:\n\n\n\n\n\n\nThis directory includes all sets of simulated read sequence files generated from NCBI viral sequences using ARC Illumina.\nthis-directory\n    |--cov\n    |    |\n    |    |--single_10seq_50bp\n    |    |    |--single_10seq_50bp.fq\n    |    |    |--single_10seq_50bp.alnEnd\n    |    |-- ...\n    |    |--single_100seq_150bp\n    |    |    |--single_100seq_150bp.fq\n    |    |    |--single_100seq_150bp.aln\n    |    |--paired_100seq_50bp\n    |    |    |--paired_100seq_50bp2.aln\n    |    |    |--paired_100seq_50bp1.aln\n    |    |    |--paired_100seq_50bp2.fq\n    |    |    |--paired_100seq_50bp1.fq\n    |    |-- ...\n    |    |\n    |---yf\n    |    |\n    |    |--yf_AY968064-single-150bp\n    |    |    |--yf_AY968064-single-1seq-150bp.fq\n    |    |    |--yf_AY968064-single-1seq-150bp.aln\n    |    |\n    |--mRhiFer1\n    |    |--mRhiFer1_v1.p.dna_rm.primary_assembly.1\n    |    |    |--mRhiFer1_v1.p.dna_rm.primary_assembly.1.fq\n    |    |    |--mRhiFer1_v1.p.dna_rm.primary_assembly.1.aln\n    |    |\n\nThis directory includes several subdirectories, each for one virus, e.g. cov for corona, yf for yellow fever.\nIn each virus subdirectory, several simreads directory includes simulated reads with various parameters, named as &lt;method&gt;_&lt;nb-seq&gt;_&lt;nb-bp&gt; where” - &lt;method&gt; is either single or paired depending on the simulation method - &lt;nb-seq&gt; is the number of reference sequences used for simulation, and refers to the fa file used - &lt;nb-bp&gt; is the number of base pairs used to simulate reads\nEach sub-directory includes simreads files made using a simulation method and a specific number of reference sequences. - xxx.fq and xxx.aln files when method is single - xxx1.fq, xxx2.fq, xxx1.aln and xxx2.aln files when method is paired.\nExample: - paired_10seq_50bp means that the simreads were generated by using the paired method to simulate 50-bp reads, and using the fa file cov_virus_sequences_010-seqs.fa. - single_100seq_50bp means that the simreads were generated by using the single method to simulate 50-bp reads, and using the fa file cov_virus_sequences_100-seqs.fa. Note that this generated 20,660,104 reads !\n\n\nSimulated reads information is split between two files: - FASTQ (.fq) files providing the read sequences and their ASCII quality scores - ALN (.aln) files with alignment information\n\n\nFASTQ files generated by ART Illumina have the following structure (showing 5 reads), with 4 lines for each read:\n@2591237:ncbi:1-60400\nACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n+\nCCCBCGFGBGGGGGGGBGGGGGGGGG&gt;GGG1G=/GGGGGGGGGGGGGGGG\n@2591237:ncbi:1-60399\nGATCAATGTGGCATCTACAATACAGACAGCATGAAGCACCACCAAAGGAC\n+\nBCBCCFGGGGGGGG1CGGGG&lt;GGBGGGGGFGCGGGGGGDGGG/GG1GGGG\n@2591237:ncbi:1-60398\nATCTACCAGTGGTAGATGGGTTCTTAATAATGAACATTATAGAGCTCTAC\n+\nCCCCCGGGEGG1GGF1G/GGEGGGGGGGGGGGGFFGGGGGGGGGGDGGDG\n@2591237:ncbi:1-60397\nCGTAAAGTAGAGGCTGTATGGTAGCTAGCACAAATGCCAGCACCAATAGG\n+\nBCCCCGGGFGGGGGGFGGGGFGG1GGGGGGG&gt;GG1GGGGGGGGGGE&lt;GGG\n@2591237:ncbi:1-60396\nGGTATCGGGTATCTCCTGCATCAATGCAAGGTCTTACAAAGATAAATACT\n+\nCBCCCGGG@CGGGGGGGGGGGG=GFGGGGDGGGFG1GGGGGGGG@GGGGG\nThe following information can be parsed from the each read sequence in the FASTQ file:\n\nLine 1: readid, a unique ID for the read, under for format @readid\nLine 2: readseq, the sequence of the read\nLine 3: a separator +\nLine 4: read_qscores, the base quality scores encoded in ASCII\n\nExample:\n@2591237:ncbi:1-60400\nACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n+\nCCCBCGFGBGGGGGGGBGGGGGGGGG&gt;GGG1G=/GGGGGGGGGGGGGGGG\n\nreadid = 2591237:ncbi:1-60400\nreadseq = ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG, a 50 bp read\nread_qscores = CCCBCGFGBGGGGGGGBGGGGGGGGG&gt;GGG1G=/GGGGGGGGGGGGGGGG\n\n\n\n\n\nALN files generated by ART Illumina consist of : - a header with the ART-Ilumina command used for the simulation (@CM) and info on each of the reference sequences used for the simulations (@SQ). Header always starts with ##ART_Illumina and ends with ##Header End : - the body with 3 lines for each read: 1. definition line with readid, - reference sequence identification number refseqid, - the position in the read in the reference sequence aln_start_pos - the strand the read was taken from ref_seq_strand. + for coding strand and - for template strand 2. aligned reference sequence, that is the sequence segment in the original reference corresponding to the read 3. aligned read sequence, that is the simmulated read sequence, where each bp corresponds to the reference sequence bp in the same position.\nExample of a ALN file generated by ART Illumina (showing 5 reads):\n##ART_Illumina    read_length    50\n@CM    /bin/art_illumina -i /home/vtec/projects/bio/metagentools/data/cov_data/cov_virus_sequences_ten.fa -ss HS25 -l 50 -f 100 -o /home/vtec/projects/bio/metagentools/data/cov_simreads/single_10seq_50bp/single_10seq_50bp -rs 1674660835\n@SQ    2591237:ncbi:1 1   MK211378    2591237    ncbi    1     Coronavirus BtRs-BetaCoV/YN2018D    30213\n@SQ    11128:ncbi:2   2   LC494191    11128    ncbi    2     Bovine coronavirus    30942\n@SQ    31631:ncbi:3   3   KY967361    31631    ncbi    3     Human coronavirus OC43        30661\n@SQ    277944:ncbi:4  4   LC654455    277944    ncbi    4     Human coronavirus NL63    27516\n@SQ    11120:ncbi:5   5   MN987231    11120    ncbi    5     Infectious bronchitis virus    27617\n@SQ    28295:ncbi:6   6   KU893866    28295    ncbi    6     Porcine epidemic diarrhea virus    28043\n@SQ    28295:ncbi:7   7   KJ645638    28295    ncbi    7     Porcine epidemic diarrhea virus    27998\n@SQ    28295:ncbi:8   8   KJ645678    28295    ncbi    8     Porcine epidemic diarrhea virus    27998\n@SQ    28295:ncbi:9   9   KR873434    28295    ncbi    9     Porcine epidemic diarrhea virus    28038\n@SQ    1699095:ncbi:10 10  KT368904    1699095    ncbi    10     Camel alphacoronavirus    27395\n##Header End\n&gt;2591237:ncbi:1    2591237:ncbi:1-60400    14770    +\nACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\nACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n&gt;2591237:ncbi:1    2591237:ncbi:1-60399    17012    -\nGATCAATGTGGCATCTACAATACAGACAGCATGAAGCACCACCAAAGGAC\nGATCAATGTGGCATCTACAATACAGACAGCATGAAGCACCACCAAAGGAC\n&gt;2591237:ncbi:1    2591237:ncbi:1-60398    9188    +\nATCTACCAGTGGTAGATGGGTTCTTAATAATGAACATTATAGAGCTCTAC\nATCTACCAGTGGTAGATGGGTTCTTAATAATGAACATTATAGAGCTCTAC\n.....",
    "crumbs": [
      "CNN Virus",
      "data"
    ]
  },
  {
    "objectID": "cnn_virus_data.html#model-related-data",
    "href": "cnn_virus_data.html#model-related-data",
    "title": "data",
    "section": "",
    "text": "pfs = ProjectFileSystem()\n\n\npfs.readme(pfs.data / 'saved')\n\n\n\n\nReadMe file for directory nbs-dev/data_dev/saved:\n\n\n\n\n\n\nThis directory includes all data related to models and saved: - saved model parameters - saved datasets\nFor example: - cnn_virus_original/pretrained_model.h5 is the saved model parameters for the CNN Virus model",
    "crumbs": [
      "CNN Virus",
      "data"
    ]
  },
  {
    "objectID": "cnn_virus_data.html#fasta-file",
    "href": "cnn_virus_data.html#fasta-file",
    "title": "data",
    "section": "FASTA file",
    "text": "FASTA file\nExtension of TextFileBaseReader class for fasta sequence files.\nStructure of a FASTA sequence file:\n&gt;definition line - format varies from dataset to dataset\nsequence line: sequence of bases\nExample for the NCBI datasets:\n&gt;seqid accession taxonomyid source seqnb organism\nTATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAAT ...\n&gt;2591237:ncbi:1 MK211378    2591237 ncbi    1 Coronavirus BtRs-BetaCoV/YN2018D\nTATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAAT ...\n\np2fasta = pfs.data / 'ncbi/refsequences/cov/cov_virus_sequences_two.fa'\n\nfasta = TextFileBaseReader(p2fasta, nlines=1)\nfor i, t in enumerate(fasta):\n    txt = t.replace('\\n', '')[:80] + ' ...'\n    print(f\"{txt}\")\n\n&gt;2591237:ncbi:1 1   MK211378    2591237 ncbi    Coronavirus BtRs-BetaCoV/YN2018D ...\nTATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAAT ...\n&gt;11128:ncbi:2   2   LC494191    11128   ncbi    Bovine coronavirus ...\nCATCCCGCTTCACTGATCTCTTGTTAGATCTTTTCATAATCTAAACTTTATAAAAACATCCACTCCCTGTAGTCTATGCC ...\n\n\n\nsource\n\nFastaFileReader\n\n FastaFileReader (path:str|pathlib.Path)\n\nWrap a FASTA file and retrieve its content in raw format and parsed format\n\n\n\n\nType\nDetails\n\n\n\n\npath\nstr | pathlib.Path\npath to the Fasta file\n\n\n\nAs an iterator, FastaFileReader returns a dict at each step, as follows:\n{\n    'definition line': 'string in file as the definition line for the sequence',\n    'sequence': 'the full sequence'\n}\nIllustration:\n\np2fasta = pfs.data / 'ncbi/refsequences/cov/cov_virus_sequences_two.fa'\nfasta = FastaFileReader(p2fasta)\niteration_output = next(fasta)\n\nprint(iteration_output['definition line'][:80], '...')\nprint(iteration_output['sequence'][:80], '...')\n\n&gt;2591237:ncbi:1 1   MK211378    2591237 ncbi    Coronavirus BtRs-BetaCoV/YN2018D ...\nTATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAAT ...\n\n\n\nprint(f\"output type :     {type(iteration_output)}\")\nprint(f\"keys :            {iteration_output.keys()}\")\nprint(f\"definition line : {iteration_output['definition line'][:80]} ...'\")\nprint(f\"sequence :       '{iteration_output['sequence'][:100]} ...'\")\n\noutput type :     &lt;class 'dict'&gt;\nkeys :            dict_keys(['definition line', 'sequence'])\ndefinition line : &gt;2591237:ncbi:1   1   MK211378    2591237 ncbi    Coronavirus BtRs-BetaCoV/YN2018D ...'\nsequence :       'TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAATCTGTGTAGCTGTCGCTCGGC ...'\n\n\nThe definition line is a string, with tab separated values.\n\ndisplay(iteration_output['definition line'])\n\n'&gt;2591237:ncbi:1\\t1\\tMK211378\\t2591237\\tncbi\\tCoronavirus BtRs-BetaCoV/YN2018D'\n\n\n\nsource\n\n\nFastaFileReader.review\n\n FastaFileReader.review ()\n\nPrints the first and last sequences and metadata in the fasta file and returns the nb or sequences\n\nnb_seqs = fasta.review()\nnb_seqs\n\nThere are 2 sequences in this file\n\nFirst Sequence:\n&gt;2591237:ncbi:1 1   MK211378    2591237 ncbi    Coronavirus BtRs-BetaCoV/YN2018D\nTATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAAT ...\n{'seqid': '2591237:ncbi:1', 'taxonomyid': '2591237', 'source': 'ncbi', 'seqnb': '1', 'accession': 'MK211378', 'organism': 'Coronavirus BtRs-BetaCoV/YN2018D'}\n\nLast Sequence:\n&gt;11128:ncbi:2   2   LC494191    11128   ncbi    Bovine coronavirus\nCATCCCGCTTCACTGATCTCTTGTTAGATCTTTTCATAATCTAAACTTTATAAAAACATCCACTCCCTGTAGTCTATGCC ...\n{'seqid': '11128:ncbi:2', 'taxonomyid': '11128', 'source': 'ncbi', 'seqnb': '2', 'accession': 'LC494191', 'organism': 'Bovine coronavirus'}\n\n\n2\n\n\n\nsource\n\n\nFastaFileReader.print_first_chunks\n\n FastaFileReader.print_first_chunks (nchunks:int=3)\n\nPrint the first nchunks chunks of text from the file\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnchunks\nint\n3\nnumber of chunks to print out\n\n\n\nThis is convenient to quickly discover and explore new fasta files in raw text format:\n\nfasta = FastaFileReader(p2fasta)\nfasta.print_first_chunks(nchunks=2)\n\n\nSequence 1:\n&gt;2591237:ncbi:1 1   MK211378    2591237 ncbi    Coronavirus BtRs-BetaCoV/YN2018D\nTATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAAT ...\n\nSequence 2:\n&gt;11128:ncbi:2   2   LC494191    11128   ncbi    Bovine coronavirus\nCATCCCGCTTCACTGATCTCTTGTTAGATCTTTTCATAATCTAAACTTTATAAAAACATCCACTCCCTGTAGTCTATGCC ...\n\n\n\n\nParsing metadata\nThe class also provides methods to parse metadata from the file content (definition line, headers, …).\nA regex pattern is used for parsing metadata fom the definition lines in the reference sequence fasta file.\nBelow, we parse the data from the definition line of our Corona virus NCBI dataset (rule fasta_ncbi_std):\nSequence 1:\n\nDefinition Line:\n\n&gt;2591237:ncbi:1 [MK211378]  2591237 ncbi    1 [MK211378] 2591237    Coronavirus YN2018D     scientific name\n\nMetadata:\n\nseqid = 2591237:ncbi:1\ntaxonomyid = 2591237\nsource = ncbi\nseqnb = 1\naccession = MK211378\nspecies = Coronavirus BtRs-BetaCoV/YN2018D\n\n\nSequence 2:\n\nDefinition Line\n\n    &gt;11128:ncbi:2 [LC494191]\n\nMetadata:\n\nseqid = 11128:ncbi:2\ntaxonomyid = 11128\nsource = ncbi\nseqnb = 2\naccession = LC494191\nspecies = ''\n\n\nFastaFileReader offers: - parse_text a method to parse the metadata - an option to set a default “parsing rule” for one instance with set_parsing_rules. - parse_file a method to parse the metadata from all sequences in the file and save it as a json file.\n\nsource\n\n\nTextFileBaseReader.parse_text\n\n TextFileBaseReader.parse_text (txt:str, pattern:str|None=None)\n\nParse text using regex pattern with groups. Return a metadata dictionary.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntxt\nstr\n\ntext to parse\n\n\npattern\nstr | None\nNone\nIf None, uses standard regex pattern to extract metadata, otherwise, uses passed regex\n\n\nReturns\ndict\n\nparsed metadata in key/value format\n\n\n\nRunning the parser function with specifically defined pattern and keys.\n\nfasta = FastaFileReader(p2fasta)\ndfn_line, sequence = next(fasta).values()\nprint(dfn_line.replace('\\n', ''))\n\n&gt;2591237:ncbi:1 1   MK211378    2591237 ncbi    Coronavirus BtRs-BetaCoV/YN2018D\n\n\n\npattern = r\"^&gt;(?P&lt;seqid&gt;(?P&lt;taxonomyid&gt;\\d+):(?P&lt;source&gt;ncbi):(?P&lt;seqnb&gt;\\d*))[\\s\\t]*(?P=seqnb)[\\s\\t](?P&lt;accession&gt;[\\w\\d]*)([\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P=source)[\\s\\t][\\s\\t]*(?P&lt;organism&gt;[\\w\\s\\-\\_\\/]*))?\"\n\n\nfasta.parse_text(dfn_line, pattern=pattern)\n\n{'seqid': '2591237:ncbi:1',\n 'taxonomyid': '2591237',\n 'source': 'ncbi',\n 'seqnb': '1',\n 'accession': 'MK211378',\n 'organism': 'Coronavirus BtRs-BetaCoV/YN2018D'}\n\n\nWhen a FastaFileReader instance is created, all existing rules in the file default_parsing_rules.json are tested on the first definition line of the fasta file and the one rule that parses the most matches will be selected automatically and saved in instance attributes re_rule_name, re_pattern and re_keys.\nparse_file extract metadata from each definition line in the fasta file and return a dictionary with all metadata.\n\nprint(fasta.re_rule_name)\nprint(fasta.re_pattern)\nprint(fasta.re_keys)\n\nfasta_ncbi_std\n^&gt;(?P&lt;seqid&gt;(?P&lt;taxonomyid&gt;\\d+):(?P&lt;source&gt;ncbi):(?P&lt;seqnb&gt;\\d*))[\\s\\t]*(?P=seqnb)[\\s\\t](?P&lt;accession&gt;[\\w\\d]*)([\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P=source)[\\s\\t][\\s\\t]*(?P&lt;organism&gt;[\\w\\s\\-\\_/]*))?\n['seqid', 'taxonomyid', 'source', 'seqnb', 'accession', 'organism']\n\n\n\nfasta.parse_text(dfn_line)\n\n{'seqid': '2591237:ncbi:1',\n 'taxonomyid': '2591237',\n 'source': 'ncbi',\n 'seqnb': '1',\n 'accession': 'MK211378',\n 'organism': 'Coronavirus BtRs-BetaCoV/YN2018D'}\n\n\nWhen another fasta file, which has another definition line structure, is used, another parsing rule is selected.\n\np2other = pfs.data / 'ncbi/refsequences/cov/another_sequence.fa'\nassert p2other.is_file()\n\nit2 = FastaFileReader(path=p2other)\n\ndfn_line, sequence = next(it2).values()\nprint(dfn_line.replace('\\n', ''))\n\n&gt;1 dna_rm:primary_assembly primary_assembly:mRhiFer1_v1.p:1:1:124933378:1 REF\n\n\n\nprint(it2.re_rule_name)\nprint(it2.re_pattern)\nprint(it2.re_keys)\n\nfasta_rhinolophus_ferrumequinum\n^&gt;\\d[\\s\\t](?P&lt;seq_type&gt;dna_rm):(?P&lt;id_type&gt;[\\w\\_]*)[\\s\\w](?P=id_type):(?P&lt;assy&gt;[\\w\\d\\_]*)\\.(?P&lt;seq_level&gt;[\\w]*):\\d*:\\d*:(?P&lt;taxonomy&gt;\\d*):(?P&lt;id&gt;\\d*)[\\s    ]REF$\n['seq_type', 'id_type', 'assy', 'seq_level', 'taxonomy', 'id']\n\n\n\npprint(it2.parse_text(dfn_line))\n\n{'assy': 'mRhiFer1_v1',\n 'id': '1',\n 'id_type': 'primary_assembly',\n 'seq_level': 'p',\n 'seq_type': 'dna_rm',\n 'taxonomy': '124933378'}\n\n\nThis rule selection is performed by the class method set_parsing_rule. The method can also be called with specific pattern and keys to force parsing rule not yet saved in the json file.\n\nsource\n\n\nTextFileBaseReader.set_parsing_rules\n\n TextFileBaseReader.set_parsing_rules (pattern:str|None=None,\n                                       verbose:bool=False)\n\n*Set the standard regex parsing rule for the file.\nRules can be set:\n\nmanually by passing specific custom values for pattern and keys\nautomatically, by testing all parsing rules saved in parsing_rule.json\n\nAutomatic selection of parsing rules works by testing each rule saved in parsing_rule.json on the first definition line of the fasta file, and selecting the one rule that generates the most metadata matches.\nRules consists of two parameters:\n\nThe regex pattern including one group for each metadata item, e.g (?P&lt;group_name&gt;regex_code)\nThe list of keys, i.e. the list with the name of each regex groups, used as key in the metadata dictionary\n\nThis method updates the three following class attributes: re_rule_name, re_pattern, re_keys*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npattern\nstr | None\nNone\nregex pattern to apply to parse the text, search in parsing rules json if None\n\n\nverbose\nbool\nFalse\nwhen True, provides information on each rule\n\n\nReturns\nNone\n\n\n\n\n\n\nfasta = FastaFileReader(p2fasta)\ndfn_line, sequence = next(fasta).values()\nprint(f\"definition line: '{dfn_line[:-1]}'\")\n\ndefinition line: '&gt;2591237:ncbi:1   1   MK211378    2591237 ncbi    Coronavirus BtRs-BetaCoV/YN2018'\n\n\nAutomatic parsing works by testing each saved rule for the value of definition line in the first sequence in the fasta file.\n\nprint(f\"key for text to parse: {fasta.text_to_parse_key}\\n\")\nfasta.reset_iterator()\nprint('Text to parse for testing (extracted from first iteration):')\nprint(next(fasta)[fasta.text_to_parse_key])\nprint()\nfasta.set_parsing_rules(verbose=True)\n\nkey for text to parse: definition line\n\nText to parse for testing (extracted from first iteration):\n&gt;2591237:ncbi:1 1   MK211378    2591237 ncbi    Coronavirus BtRs-BetaCoV/YN2018D\n\n--------------------------------------------------------------------------------\n6 matches generated with rule &lt;fasta_ncbi_std&gt; \n--------------------------------------------------------------------------------\n^&gt;(?P&lt;seqid&gt;(?P&lt;taxonomyid&gt;\\d+):(?P&lt;source&gt;ncbi):(?P&lt;seqnb&gt;\\d*))[\\s\\t]*(?P=seqnb)[\\s\\t](?P&lt;accession&gt;[\\w\\d]*)([\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P=source)[\\s\\t][\\s\\t]*(?P&lt;organism&gt;[\\w\\s\\-\\_/]*))?\n['seqid', 'taxonomyid', 'source', 'seqnb', 'accession', 'organism']\n{'seqid': '2591237:ncbi:1', 'taxonomyid': '2591237', 'source': 'ncbi', 'seqnb': '1', 'accession': 'MK211378', 'organism': 'Coronavirus BtRs-BetaCoV/YN2018D'}\n--------------------------------------------------------------------------------\n0 matches generated with rule &lt;fastq_art_illumina_ncbi_std&gt; \n--------------------------------------------------------------------------------\n^@(?P&lt;readid&gt;(?P&lt;reftaxonomyid&gt;\\d*):(?P&lt;refsource&gt;\\w*):(?P&lt;refseqnb&gt;\\d*)-(?P&lt;readnb&gt;\\d*(\\/\\d)?))$\n['readid', 'reftaxonomyid', 'refsource', 'refseqnb', 'readnb']\n{'readid': '', 'reftaxonomyid': '', 'refsource': '', 'refseqnb': '', 'readnb': ''}\n--------------------------------------------------------------------------------\n0 matches generated with rule &lt;aln_art_illumina_ncbi_std&gt; \n--------------------------------------------------------------------------------\n^&gt;(?P&lt;refseqid&gt;(?P&lt;reftaxonomyid&gt;\\d*):(?P&lt;refsource&gt;\\w*):(?P&lt;refseqnb&gt;\\d*))(\\s| )*(?P&lt;readid&gt;(?P=reftaxonomyid):(?P=refsource):(?P=refseqnb)-(?P&lt;readnb&gt;\\d*(\\/\\d(-\\d)?)?))(\\s|\\t)(?P&lt;aln_start_pos&gt;\\d*)(\\s|\\t)(?P&lt;refseq_strand&gt;(-|\\+))$\n['refseqid', 'reftaxonomyid', 'refsource', 'refseqnb', 'readid', 'readnb', 'aln_start_pos', 'refseq_strand']\n{'refseqid': '', 'reftaxonomyid': '', 'refsource': '', 'refseqnb': '', 'readid': '', 'readnb': '', 'aln_start_pos': '', 'refseq_strand': ''}\n--------------------------------------------------------------------------------\n0 matches generated with rule &lt;aln_art_illumina-refseq-ncbi-std&gt; \n--------------------------------------------------------------------------------\n^@SQ[\\t\\s]*(?P&lt;refseqid&gt;(?P&lt;reftaxonomyid&gt;\\d*):(?P&lt;refsource&gt;\\w*):(?P&lt;refseqnb&gt;\\d*))[\\t\\s]*(?P=refseqnb)[\\t\\s]*(?P&lt;refseq_accession&gt;[\\w\\d]*)[\\t\\s]*(?P=reftaxonomyid)[\\t\\s]*(?P=refsource)[\\t\\s](?P&lt;organism&gt;.*)[\\t\\s](?P&lt;refseq_length&gt;\\d*)$\n['refseqid', 'reftaxonomyid', 'refsource', 'refseqnb', 'refseq_accession', 'organism', 'refseq_length']\n{'refseqid': '', 'reftaxonomyid': '', 'refsource': '', 'refseqnb': '', 'refseq_accession': '', 'organism': '', 'refseq_length': ''}\n--------------------------------------------------------------------------------\n0 matches generated with rule &lt;fasta_ncbi_cov&gt; \n--------------------------------------------------------------------------------\n^&gt;(?P&lt;seqid&gt;(?P&lt;taxonomyid&gt;\\d+):(?P&lt;source&gt;ncbi):(?P&lt;seqnb&gt;\\d*))[\\s\\t]*\\[(?P&lt;accession&gt;[\\w\\d]*)\\]([\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P=source)[\\s\\t]*(?P=seqnb)[\\s\\t]*\\[(?P=accession)\\][\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P&lt;organism&gt;[\\w\\s\\-\\_\\/]*))?\n['seqid', 'taxonomyid', 'source', 'seqnb', 'accession', 'organism']\n{'seqid': '', 'taxonomyid': '', 'source': '', 'seqnb': '', 'accession': '', 'organism': ''}\n--------------------------------------------------------------------------------\n0 matches generated with rule &lt;fasta_rhinolophus_ferrumequinum&gt; \n--------------------------------------------------------------------------------\n^&gt;\\d[\\s\\t](?P&lt;seq_type&gt;dna_rm):(?P&lt;id_type&gt;[\\w\\_]*)[\\s\\w](?P=id_type):(?P&lt;assy&gt;[\\w\\d\\_]*)\\.(?P&lt;seq_level&gt;[\\w]*):\\d*:\\d*:(?P&lt;taxonomy&gt;\\d*):(?P&lt;id&gt;\\d*)[\\s    ]REF$\n['seq_type', 'id_type', 'assy', 'seq_level', 'taxonomy', 'id']\n{'seq_type': '', 'id_type': '', 'assy': '', 'seq_level': '', 'taxonomy': '', 'id': ''}\n--------------------------------------------------------------------------------\nSelected rule with most matches: fasta_ncbi_std\n\n\nIf no saved rule generates a match, re_rule_name, re_pattern and re_keys remain None and a warning message is issued to ask user to add a parsing rule manually.\n\np2nomatch = pfs.data / 'ncbi/refsequences/cov/sequences_two_no_matching_rule.fa'\nfasta2 = FastaFileReader(p2nomatch)\n\n/home/vtec/projects/bio/metagentorch/metagentorch/core.py:651: UserWarning: \n        None of the saved parsing rules were able to extract metadata from the first line in this file.\n        You must set a custom rule (pattern + keys) before parsing text, by using:\n            `self.set_parsing_rules(custom_pattern)`\n                \n  warnings.warn(msg, category=UserWarning)\n\n\n\nfasta2.re_rule_name is None\n\nTrue\n\n\nBut we still can set a standard rule manually, by passing a re pattern and the corresponding list of keys.\n\npat = r\"^&gt;(?P&lt;seqid&gt;(?P&lt;taxonomyid&gt;\\d+):(?P&lt;source&gt;ncbi):(?P&lt;seqnb&gt;\\d*))\\s*(?P&lt;text&gt;[\\w\\s]*)$\"\nkeys = \"seqid taxonomyid source seqnb text\".split()\nfasta2.set_parsing_rules(pattern=pat)\n\nprint(fasta2.re_rule_name)\nprint(fasta2.re_pattern)\nprint(fasta2.re_keys)\n\nCustom Rule\n^&gt;(?P&lt;seqid&gt;(?P&lt;taxonomyid&gt;\\d+):(?P&lt;source&gt;ncbi):(?P&lt;seqnb&gt;\\d*))\\s*(?P&lt;text&gt;[\\w\\s]*)$\n['seqid', 'taxonomyid', 'source', 'seqnb', 'text']\n\n\n\nfasta2.reset_iterator()\ndfn_line, sequence = next(fasta2).values()\nprint(f\"definition line: '{dfn_line[:-1]}'\")\nfasta2.parse_text(dfn_line)\n\ndefinition line: '&gt;2591237:ncbi:1 this sequence does not match any saved parsing rul'\n\n\n{'seqid': '2591237:ncbi:1',\n 'taxonomyid': '2591237',\n 'source': 'ncbi',\n 'seqnb': '1',\n 'text': 'this sequence does not match any saved parsing rule'}\n\n\n\nsource\n\n\nFastaFileReader.parse_file\n\n FastaFileReader.parse_file (add_seq:bool=False, save_json:bool=False)\n\nRead fasta file and return a dictionary with definition line metadata and optionally sequences\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nadd_seq\nbool\nFalse\nWhen True, add the full sequence to the parsed metadata dictionary\n\n\nsave_json\nbool\nFalse\nWhen True, save the file metadata as a json file of same stem name\n\n\nReturns\ndict\n\nMetadata as Key/Values pairs\n\n\n\n\nfasta = FastaFileReader(p2fasta)\npprint(fasta.parse_file())\n\n{'11128:ncbi:2': {'accession': 'LC494191',\n                  'organism': 'Bovine coronavirus',\n                  'seqid': '11128:ncbi:2',\n                  'seqnb': '2',\n                  'source': 'ncbi',\n                  'taxonomyid': '11128'},\n '2591237:ncbi:1': {'accession': 'MK211378',\n                    'organism': 'Coronavirus BtRs-BetaCoV/YN2018D',\n                    'seqid': '2591237:ncbi:1',\n                    'seqnb': '1',\n                    'source': 'ncbi',\n                    'taxonomyid': '2591237'}}\n\n\n\nfasta.parse_file(save_json=True);\n\nMetadata for 'cov_virus_sequences_two.fa'&gt; saved as &lt;cov_virus_sequences_two_metadata.json&gt; in  \n/home/vtec/projects/bio/metagentorch/nbs-dev/data_dev/ncbi/refsequences/cov\n\n\n\n\nwith open('../default_parsing_rules.json', 'r') as fp:\n    pprint(json.load(fp), width=20)\n\n{'aln_art_illumina-refseq-ncbi-std': {'keys': 'refseqid '\n                                              'reftaxonomyid '\n                                              'refsource '\n                                              'refseqnb '\n                                              'refseq_accession '\n                                              'organism '\n                                              'refseq_length',\n                                      'pattern': '^@SQ[\\\\t\\\\s]*(?P&lt;refseqid&gt;(?P&lt;reftaxonomyid&gt;\\\\d*):(?P&lt;refsource&gt;\\\\w*):(?P&lt;refseqnb&gt;\\\\d*))[\\\\t\\\\s]*(?P=refseqnb)[\\\\t\\\\s]*(?P&lt;refseq_accession&gt;[\\\\w\\\\d]*)[\\\\t\\\\s]*(?P=reftaxonomyid)[\\\\t\\\\s]*(?P=refsource)[\\\\t\\\\s](?P&lt;organism&gt;.*)[\\\\t\\\\s](?P&lt;refseq_length&gt;\\\\d*)$'},\n 'aln_art_illumina_ncbi_std': {'keys': 'refseqid '\n                                       'reftaxonomyid '\n                                       'refsource '\n                                       'refseqnb '\n                                       'readid '\n                                       'readnb '\n                                       'aln_start_pos '\n                                       'refseq_strand',\n                               'pattern': '^&gt;(?P&lt;refseqid&gt;(?P&lt;reftaxonomyid&gt;\\\\d*):(?P&lt;refsource&gt;\\\\w*):(?P&lt;refseqnb&gt;\\\\d*))(\\\\s|\\t'\n                                          ')*(?P&lt;readid&gt;(?P=reftaxonomyid):(?P=refsource):(?P=refseqnb)-(?P&lt;readnb&gt;\\\\d*(\\\\/\\\\d(-\\\\d)?)?))(\\\\s|\\\\t)(?P&lt;aln_start_pos&gt;\\\\d*)(\\\\s|\\\\t)(?P&lt;refseq_strand&gt;(-|\\\\+))$'},\n 'fasta_ncbi_cov': {'keys': 'seqid '\n                            'taxonomyid '\n                            'source '\n                            'accession '\n                            'seqnb '\n                            'organism',\n                    'pattern': '^&gt;(?P&lt;seqid&gt;(?P&lt;taxonomyid&gt;\\\\d+):(?P&lt;source&gt;ncbi):(?P&lt;seqnb&gt;\\\\d*))[\\\\s\\\\t]*\\\\[(?P&lt;accession&gt;[\\\\w\\\\d]*)\\\\]([\\\\s\\\\t]*(?P=taxonomyid)[\\\\s\\\\t]*(?P=source)[\\\\s\\\\t]*(?P=seqnb)[\\\\s\\\\t]*\\\\[(?P=accession)\\\\][\\\\s\\\\t]*(?P=taxonomyid)[\\\\s\\\\t]*(?P&lt;organism&gt;[\\\\w\\\\s\\\\-\\\\_\\\\/]*))?'},\n 'fasta_ncbi_std': {'keys': 'seqid '\n                            'taxonomyid '\n                            'source '\n                            'accession '\n                            'seqnb '\n                            'organism',\n                    'pattern': '^&gt;(?P&lt;seqid&gt;(?P&lt;taxonomyid&gt;\\\\d+):(?P&lt;source&gt;ncbi):(?P&lt;seqnb&gt;\\\\d*))[\\\\s\\\\t]*(?P=seqnb)[\\\\s\\\\t](?P&lt;accession&gt;[\\\\w\\\\d]*)([\\\\s\\\\t]*(?P=taxonomyid)[\\\\s\\\\t]*(?P=source)[\\\\s\\\\t][\\\\s\\\\t]*(?P&lt;organism&gt;[\\\\w\\\\s\\\\-\\\\_/]*))?'},\n 'fasta_rhinolophus_ferrumequinum': {'keys': 'seq_type '\n                                             'id_type '\n                                             'assy '\n                                             'seq_level '\n                                             'taxonomy '\n                                             'id',\n                                     'pattern': '^&gt;\\\\d[\\\\s\\\\t](?P&lt;seq_type&gt;dna_rm):(?P&lt;id_type&gt;[\\\\w\\\\_]*)[\\\\s\\\\w](?P=id_type):(?P&lt;assy&gt;[\\\\w\\\\d\\\\_]*)\\\\.(?P&lt;seq_level&gt;[\\\\w]*):\\\\d*:\\\\d*:(?P&lt;taxonomy&gt;\\\\d*):(?P&lt;id&gt;\\\\d*)[\\\\s\\t'\n                                                ']REF$'},\n 'fastq_art_illumina_ncbi_std': {'keys': 'readid '\n                                         'reftaxonomyid '\n                                         'refsource '\n                                         'refseqnb '\n                                         'readnb',\n                                 'pattern': '^@(?P&lt;readid&gt;(?P&lt;reftaxonomyid&gt;\\\\d*):(?P&lt;refsource&gt;\\\\w*):(?P&lt;refseqnb&gt;\\\\d*)-(?P&lt;readnb&gt;\\\\d*(\\\\/\\\\d)?))$'}}\n\n\n\np2fasta = pfs.data / 'ncbi/refsequences/cov/cov_virus_sequence_one.fa'\nfasta = FastaFileReader(p2fasta)\nfasta_meta = fasta.parse_file(save_json=True)\npprint(fasta_meta)\n\nMetadata for 'cov_virus_sequence_one.fa'&gt; saved as &lt;cov_virus_sequence_one_metadata.json&gt; in  \n/home/vtec/projects/bio/metagentorch/nbs-dev/data_dev/ncbi/refsequences/cov\n\n{'2591237:ncbi:1': {'accession': 'MK211378',\n                    'organism': 'Coronavirus BtRs-BetaCoV/YN2018D',\n                    'seqid': '2591237:ncbi:1',\n                    'seqnb': '1',\n                    'source': 'ncbi',\n                    'taxonomyid': '2591237'}}",
    "crumbs": [
      "CNN Virus",
      "data"
    ]
  },
  {
    "objectID": "cnn_virus_data.html#fastq-file",
    "href": "cnn_virus_data.html#fastq-file",
    "title": "data",
    "section": "FASTQ file",
    "text": "FASTQ file\nExtension of TextFileBaseReader class for fastq sequence files.\nStructure of a FASTQ sequence file:\n\np2fastq = pfs.data / 'ncbi/simreads/cov/single_1seq_150bp/single_1seq_150bp.fq'\n\nfastq = TextFileBaseReader(p2fastq, nlines=1)\nfor i, t in enumerate(fastq):\n    txt = t.replace('\\n', '')[:80]\n    print(f\"{txt}\")\n    if i &gt;= 11: break\n\n@2591237:ncbi:1-40200\nTTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAAC\n+\nCCCGGGCGGGGGCJGJJJGJJGJJJGJGGJGJJJGJGGGGGGGGCJGJGGGGGJJJJGCCGGGGGJCGCGJGJCG=GGGG\n@2591237:ncbi:1-40199\nTCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGAGTCATTTGA\n+\n=CCGGGGCGGGGGJJGJJGJGJG=GJJGJCGJJJCJ=JJJJGGJJCJGJGG=JGC1JJGG8GCJCGGGCGG(GCGGCGC=\n@2591237:ncbi:1-40198\nTAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTCTTCATCATCTAACTCTGAATATTTATTCTTAGTT\n+\nC=CGGGGGGGGGGCJJJJ=JJJJJJJJJJJGGJJJJ1GJJ8GJJGGGJGGJJC=JJGGGCCGG88GG=GGGGGGCJGGGG\n\n\n\nsource\n\nFastqFileReader\n\n FastqFileReader (path:str|pathlib.Path)\n\nIterator going through a fastq file’s sequences and return each section + prob error as a dict\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\npath\nstr | pathlib.Path\npath to the fastq file\n\n\nReturns\ndict\nkey/value with keys: definition line; sequence; q score; prob error\n\n\n\n\nfastq = FastqFileReader(p2fastq)\niteration_output = next(fastq)\n\nprint(type(iteration_output))\nprint(iteration_output.keys())\nprint(f\"Definition line:  {iteration_output['definition line']}\")\nprint(f\"Read sequence:    {iteration_output['sequence']}\")\nprint(f\"Q scores (ASCII): {iteration_output['read_qscores']}\")\nprint(f\"Prob error:       {','.join([f'{p:.4f}' for p in iteration_output['probs error']])}\")\n\n&lt;class 'dict'&gt;\ndict_keys(['definition line', 'sequence', 'read_qscores', 'probs error'])\nDefinition line:  @2591237:ncbi:1-40200\nRead sequence:    TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAACTTACATAGCTCGCGTCTCAGTTTCAAGGAACTTTTAGTGTATGCTGCTGATCCAGCCATGCATGCAGCTT\nQ scores (ASCII): CCCGGGCGGGGGCJGJJJGJJGJJJGJGGJGJJJGJGGGGGGGGCJGJGGGGGJJJJGCCGGGGGJCGCGJGJCG=GGGG=CGGGGGG1GCGCGGGGCCGJC8GGGGGGGGGGGCGGGGGGGGGGGC8GGGGGGCGGC1GGGCGGGGGCC\nProb error:       0.0004,0.0004,0.0004,0.0002,0.0002,0.0002,0.0004,0.0002,0.0002,0.0002,0.0002,0.0002,0.0004,0.0001,0.0002,0.0001,0.0001,0.0001,0.0002,0.0001,0.0001,0.0002,0.0001,0.0001,0.0001,0.0002,0.0001,0.0002,0.0002,0.0001,0.0002,0.0001,0.0001,0.0001,0.0002,0.0001,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0004,0.0001,0.0002,0.0001,0.0002,0.0002,0.0002,0.0002,0.0002,0.0001,0.0001,0.0001,0.0001,0.0002,0.0004,0.0004,0.0002,0.0002,0.0002,0.0002,0.0002,0.0001,0.0004,0.0002,0.0004,0.0002,0.0001,0.0002,0.0001,0.0004,0.0002,0.0016,0.0002,0.0002,0.0002,0.0002,0.0016,0.0004,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0251,0.0002,0.0004,0.0002,0.0004,0.0002,0.0002,0.0002,0.0002,0.0004,0.0004,0.0002,0.0001,0.0004,0.0050,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0004,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0004,0.0050,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0004,0.0002,0.0002,0.0004,0.0251,0.0002,0.0002,0.0002,0.0004,0.0002,0.0002,0.0002,0.0002,0.0002,0.0004,0.0004\n\n\nFive largest probabilities of error:\n\nnp.sort(iteration_output['probs error'])[-5:]\n\narray([0.00158489, 0.00501187, 0.00501187, 0.02511886, 0.02511886])\n\n\n\nnp.argsort(iteration_output['probs error'])[-5:]\n\narray([ 80, 127, 102, 138,  88])\n\n\n\ndfn_line = iteration_output['definition line']\nmeta = fastq.parse_text(dfn_line)\nmeta\n\n{'readid': '2591237:ncbi:1-40200',\n 'reftaxonomyid': '2591237',\n 'refsource': 'ncbi',\n 'refseqnb': '1',\n 'readnb': '40200'}\n\n\n\nfastq = FastqFileReader(p2fastq)\nnext(fastq).keys()\n\ndict_keys(['definition line', 'sequence', 'read_qscores', 'probs error'])\n\n\n\nsource\n\n\nFastqFileReader.parse_file\n\n FastqFileReader.parse_file (add_readseq:bool=False,\n                             add_qscores:bool=False,\n                             add_probs_error:bool=False,\n                             save_json:bool=False)\n\nRead fastq file, return a dict with definition line metadata and optionally read sequence and q scores, …\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nadd_readseq\nbool\nFalse\nWhen True, add the full sequence to the parsed metadata dictionary\n\n\nadd_qscores\nbool\nFalse\nAdd the read ASCII Q Scores to the parsed dictionary when True\n\n\nadd_probs_error\nbool\nFalse\nAdd the read probability of error to the parsed dictionary when True\n\n\nsave_json\nbool\nFalse\nWhen True, save the file metadata as a json file of same stem name\n\n\nReturns\ndict\n\nMetadata as Key/Values pairs\n\n\n\n\nparsed = fastq.parse_file(add_readseq=False, add_qscores=False, add_probs_error=False)\nfor i, (k, v) in enumerate(parsed.items()):\n    print(k)\n    pprint(v)\n    if i &gt;=3: break\n\n2591237:ncbi:1-40200\n{'readid': '2591237:ncbi:1-40200',\n 'readnb': '40200',\n 'refseqnb': '1',\n 'refsource': 'ncbi',\n 'reftaxonomyid': '2591237'}\n2591237:ncbi:1-40199\n{'readid': '2591237:ncbi:1-40199',\n 'readnb': '40199',\n 'refseqnb': '1',\n 'refsource': 'ncbi',\n 'reftaxonomyid': '2591237'}\n2591237:ncbi:1-40198\n{'readid': '2591237:ncbi:1-40198',\n 'readnb': '40198',\n 'refseqnb': '1',\n 'refsource': 'ncbi',\n 'reftaxonomyid': '2591237'}\n2591237:ncbi:1-40197\n{'readid': '2591237:ncbi:1-40197',\n 'readnb': '40197',\n 'refseqnb': '1',\n 'refsource': 'ncbi',\n 'reftaxonomyid': '2591237'}\n\n\n\nmetadata = fastq.parse_file(add_readseq=True)\ndf = pd.DataFrame(metadata).T\ndf.head(10)\n\n\n\n\n\n\n\n\nreadid\nreftaxonomyid\nrefsource\nrefseqnb\nreadnb\nreadseq\n\n\n\n\n2591237:ncbi:1-40200\n2591237:ncbi:1-40200\n2591237\nncbi\n1\n40200\nTTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCG...\n\n\n2591237:ncbi:1-40199\n2591237:ncbi:1-40199\n2591237\nncbi\n1\n40199\nTCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATT...\n\n\n2591237:ncbi:1-40198\n2591237:ncbi:1-40198\n2591237\nncbi\n1\n40198\nTAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTC...\n\n\n2591237:ncbi:1-40197\n2591237:ncbi:1-40197\n2591237\nncbi\n1\n40197\nTAATCACTGATAGCAGCATTGCCATCCTGAGCAAAGAAGAAGTGTT...\n\n\n2591237:ncbi:1-40196\n2591237:ncbi:1-40196\n2591237\nncbi\n1\n40196\nCTAATGTCAGTACGCCTACAATGCCTGCATCACGCATAGCATCGCA...\n\n\n2591237:ncbi:1-40195\n2591237:ncbi:1-40195\n2591237\nncbi\n1\n40195\nAAGCTGAAGCATACATAACACAGTCCTTAAGCCGATAACCAGACAA...\n\n\n2591237:ncbi:1-40194\n2591237:ncbi:1-40194\n2591237\nncbi\n1\n40194\nAGTGGAAGAACTTCACCGTCAAGATGAAACTCGACGGGGCTCTCCA...\n\n\n2591237:ncbi:1-40193\n2591237:ncbi:1-40193\n2591237\nncbi\n1\n40193\nGCGTCTCGAGTGCTTCGAGTTCACCGTTCTTGAGAACAACCTCCTC...\n\n\n2591237:ncbi:1-40192\n2591237:ncbi:1-40192\n2591237\nncbi\n1\n40192\nCTGGTAGTATCTAAGGCTCCACTGAAATACTTGTACTTGTTATATA...\n\n\n2591237:ncbi:1-40191\n2591237:ncbi:1-40191\n2591237\nncbi\n1\n40191\nGTCTCTATCTGTAGTACTATGACAAATAGACAGTTTCATCAGAAAT...\n\n\n\n\n\n\n\n\nfastq.set_parsing_rules(verbose=True)\n\n--------------------------------------------------------------------------------\n0 matches generated with rule &lt;fasta_ncbi_std&gt; \n--------------------------------------------------------------------------------\n^&gt;(?P&lt;seqid&gt;(?P&lt;taxonomyid&gt;\\d+):(?P&lt;source&gt;ncbi):(?P&lt;seqnb&gt;\\d*))[\\s\\t]*(?P=seqnb)[\\s\\t](?P&lt;accession&gt;[\\w\\d]*)([\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P=source)[\\s\\t][\\s\\t]*(?P&lt;organism&gt;[\\w\\s\\-\\_/]*))?\n['seqid', 'taxonomyid', 'source', 'seqnb', 'accession', 'organism']\n{'seqid': '', 'taxonomyid': '', 'source': '', 'seqnb': '', 'accession': '', 'organism': ''}\n--------------------------------------------------------------------------------\n5 matches generated with rule &lt;fastq_art_illumina_ncbi_std&gt; \n--------------------------------------------------------------------------------\n^@(?P&lt;readid&gt;(?P&lt;reftaxonomyid&gt;\\d*):(?P&lt;refsource&gt;\\w*):(?P&lt;refseqnb&gt;\\d*)-(?P&lt;readnb&gt;\\d*(\\/\\d)?))$\n['readid', 'reftaxonomyid', 'refsource', 'refseqnb', 'readnb']\n{'readid': '2591237:ncbi:1-40200', 'reftaxonomyid': '2591237', 'refsource': 'ncbi', 'refseqnb': '1', 'readnb': '40200'}\n--------------------------------------------------------------------------------\n0 matches generated with rule &lt;aln_art_illumina_ncbi_std&gt; \n--------------------------------------------------------------------------------\n^&gt;(?P&lt;refseqid&gt;(?P&lt;reftaxonomyid&gt;\\d*):(?P&lt;refsource&gt;\\w*):(?P&lt;refseqnb&gt;\\d*))(\\s| )*(?P&lt;readid&gt;(?P=reftaxonomyid):(?P=refsource):(?P=refseqnb)-(?P&lt;readnb&gt;\\d*(\\/\\d(-\\d)?)?))(\\s|\\t)(?P&lt;aln_start_pos&gt;\\d*)(\\s|\\t)(?P&lt;refseq_strand&gt;(-|\\+))$\n['refseqid', 'reftaxonomyid', 'refsource', 'refseqnb', 'readid', 'readnb', 'aln_start_pos', 'refseq_strand']\n{'refseqid': '', 'reftaxonomyid': '', 'refsource': '', 'refseqnb': '', 'readid': '', 'readnb': '', 'aln_start_pos': '', 'refseq_strand': ''}\n--------------------------------------------------------------------------------\n0 matches generated with rule &lt;aln_art_illumina-refseq-ncbi-std&gt; \n--------------------------------------------------------------------------------\n^@SQ[\\t\\s]*(?P&lt;refseqid&gt;(?P&lt;reftaxonomyid&gt;\\d*):(?P&lt;refsource&gt;\\w*):(?P&lt;refseqnb&gt;\\d*))[\\t\\s]*(?P=refseqnb)[\\t\\s]*(?P&lt;refseq_accession&gt;[\\w\\d]*)[\\t\\s]*(?P=reftaxonomyid)[\\t\\s]*(?P=refsource)[\\t\\s](?P&lt;organism&gt;.*)[\\t\\s](?P&lt;refseq_length&gt;\\d*)$\n['refseqid', 'reftaxonomyid', 'refsource', 'refseqnb', 'refseq_accession', 'organism', 'refseq_length']\n{'refseqid': '', 'reftaxonomyid': '', 'refsource': '', 'refseqnb': '', 'refseq_accession': '', 'organism': '', 'refseq_length': ''}\n--------------------------------------------------------------------------------\n0 matches generated with rule &lt;fasta_ncbi_cov&gt; \n--------------------------------------------------------------------------------\n^&gt;(?P&lt;seqid&gt;(?P&lt;taxonomyid&gt;\\d+):(?P&lt;source&gt;ncbi):(?P&lt;seqnb&gt;\\d*))[\\s\\t]*\\[(?P&lt;accession&gt;[\\w\\d]*)\\]([\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P=source)[\\s\\t]*(?P=seqnb)[\\s\\t]*\\[(?P=accession)\\][\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P&lt;organism&gt;[\\w\\s\\-\\_\\/]*))?\n['seqid', 'taxonomyid', 'source', 'seqnb', 'accession', 'organism']\n{'seqid': '', 'taxonomyid': '', 'source': '', 'seqnb': '', 'accession': '', 'organism': ''}\n--------------------------------------------------------------------------------\n0 matches generated with rule &lt;fasta_rhinolophus_ferrumequinum&gt; \n--------------------------------------------------------------------------------\n^&gt;\\d[\\s\\t](?P&lt;seq_type&gt;dna_rm):(?P&lt;id_type&gt;[\\w\\_]*)[\\s\\w](?P=id_type):(?P&lt;assy&gt;[\\w\\d\\_]*)\\.(?P&lt;seq_level&gt;[\\w]*):\\d*:\\d*:(?P&lt;taxonomy&gt;\\d*):(?P&lt;id&gt;\\d*)[\\s    ]REF$\n['seq_type', 'id_type', 'assy', 'seq_level', 'taxonomy', 'id']\n{'seq_type': '', 'id_type': '', 'assy': '', 'seq_level': '', 'taxonomy': '', 'id': ''}\n--------------------------------------------------------------------------------\nSelected rule with most matches: fastq_art_illumina_ncbi_std",
    "crumbs": [
      "CNN Virus",
      "data"
    ]
  },
  {
    "objectID": "cnn_virus_data.html#aln-alignment-files",
    "href": "cnn_virus_data.html#aln-alignment-files",
    "title": "data",
    "section": "ALN Alignment Files",
    "text": "ALN Alignment Files\nExtension of TextFileBaseReader class for ALN read/sequence alignment files.\nStructure of a ALN sequence file:\n\np2aln = pfs.data / 'ncbi/simreads/cov/single_1seq_150bp/single_1seq_150bp.aln'\nassert p2aln.is_file()\n\naln = TextFileBaseReader(p2aln, nlines=1)\nfor i, t in enumerate(aln):\n    txt = t.replace('\\n', '')[:80]\n    print(f\"{txt}\")\n    if i &gt;= 12: break\n\n##ART_Illumina  read_length 150\n@CM /usr/bin/art_illumina -i /home/vtec/projects/bio/metagentools/data/ncbi/refs\n@SQ 2591237:ncbi:1  1   MK211378    2591237 ncbi    Coronavirus BtRs-BetaCoV/YN2018D    3021\n##Header End\n&gt;2591237:ncbi:1 2591237:ncbi:1-40200    14370   +\nTTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAAC\nTTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAAC\n&gt;2591237:ncbi:1 2591237:ncbi:1-40199    15144   -\nTCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGATTCATTTGA\nTCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGAGTCATTTGA\n&gt;2591237:ncbi:1 2591237:ncbi:1-40198    2971    -\nTAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTCTTCATCATCTAACTCTGAATATTTATTCTTAGTT\nTAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTCTTCATCATCTAACTCTGAATATTTATTCTTAGTT\n\n\n\nsource\n\nAlnFileReader\n\n AlnFileReader (path:str|pathlib.Path)\n\nIterator going through an ALN file\n\n\n\n\nType\nDetails\n\n\n\n\npath\nstr | pathlib.Path\npath to the aln file\n\n\nReturns\ndict\nkey/value with keys:\n\n\n\n\naln = AlnFileReader(p2aln)\n\nAlnFileReader iterator returns elements one by one, as dictionaries with each data line related to the read, accessible through the following keys:\n\nkey 'definition line': read definition line, including read metadata\nkey 'ref_seq_aligned': aligned reference sequence, that is the sequence segment in the original reference corresponding to the read\nkey 'read_seq_aligned': aligned read, that is the simmulated read sequence, where each bp corresponds to the reference sequence bp in the same position.\n\n\none_iteration = next(aln)\none_iteration.keys()\n\ndict_keys(['definition line', 'ref_seq_aligned', 'read_seq_aligned'])\n\n\n\npprint(one_iteration)\n\n{'definition line': '&gt;2591237:ncbi:1\\t2591237:ncbi:1-40200\\t14370\\t+',\n 'read_seq_aligned': 'TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAACTTACATAGCTCGCGTCTCAGTTTCAAGGAACTTTTAGTGTATGCTGCTGATCCAGCCATGCATGCAGCTT',\n 'ref_seq_aligned': 'TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAACTTACATAGCTCGCGTCTCAGTTTCAAGGAACTTTTAGTGTATGCTGCTGATCCAGCCATGCATGCAGCTT'}\n\n\n\ndfn_line, ref_seq_aligned, read_seq_aligned = one_iteration.values()\n\n\ndfn_line\n\n'&gt;2591237:ncbi:1\\t2591237:ncbi:1-40200\\t14370\\t+'\n\n\n\nref_seq_aligned[:100]\n\n'TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAACTTACATAGCTCGCGTCTCAG'\n\n\n\nread_seq_aligned[:100]\n\n'TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAACTTACATAGCTCGCGTCTCAG'\n\n\n\nanother_iteration = next(aln)\npprint(another_iteration)\n\n{'definition line': '&gt;2591237:ncbi:1\\t2591237:ncbi:1-40199\\t15144\\t-',\n 'read_seq_aligned': 'TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGAGTCATTTGAGTTATAGTAGGGATGACATTACGCTTAGTATACGCGAAAAGTGCATCTTGATCCTCATAACTCATTGAGT',\n 'ref_seq_aligned': 'TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGATTCATTTGAGTTATAGTAGGGATGACATTACGCTTAGTATACGCGAAAAGTGCATCTTGATCCTCATAACTCATTGAGT'}\n\n\n\naln.reset_iterator()\nfor i, d in enumerate(aln):\n    print(d['definition line'])\n    print(d['ref_seq_aligned'][:80], '...')\n    print(d['read_seq_aligned'][:80], '...\\n')\n    if i &gt;= 3: break\n\n&gt;2591237:ncbi:1 2591237:ncbi:1-40200    14370   +\nTTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAAC ...\nTTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAAC ...\n\n&gt;2591237:ncbi:1 2591237:ncbi:1-40199    15144   -\nTCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGATTCATTTGA ...\nTCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGAGTCATTTGA ...\n\n&gt;2591237:ncbi:1 2591237:ncbi:1-40198    2971    -\nTAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTCTTCATCATCTAACTCTGAATATTTATTCTTAGTT ...\nTAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTCTTCATCATCTAACTCTGAATATTTATTCTTAGTT ...\n\n&gt;2591237:ncbi:1 2591237:ncbi:1-40197    15485   -\nTAATCACTGATAGCAGCATTGCCATCCTGAGCAAAGAAGAAGTGTTTTAGTTCAACAGAACTTCCTTCCTTAAAGAAACC ...\nTAATCACTGATAGCAGCATTGCCATCCTGAGCAAAGAAGAAGTGTTTTAGTTCAACAGAACTTCCTTCCTTAAAGAAACC ...\n\n\n\nOnce instantiated, the AlnFileReader iterator gives access to the file’s header information through header instance attribute. It is a dictionary with two keys: 'command' and 'reference sequences':\n    {'command':             'art-illumina command used to create the reads',\n     'reference sequences': ['@SQ metadata on reference sequence 1 used for the reads',\n                             '@SQ metadata on reference sequence 2 used for the reads', \n                             ...\n                            ]\n    }\n\nprint(aln.header['command'])\n\n/usr/bin/art_illumina -i /home/vtec/projects/bio/metagentools/data/ncbi/refsequences/cov/cov_refseq_001-seq1.fa -ss HS25 -l 150 -f 200 -o /home/vtec/projects/bio/metagentools/data/ncbi/simreads/cov/single_1seq_150bp/single_1seq_150bp -rs 1723893089\n\n\n\nfor seq_info in aln.header['reference sequences']:\n    print(seq_info)\n\n@SQ 2591237:ncbi:1  1   MK211378    2591237 ncbi    Coronavirus BtRs-BetaCoV/YN2018D    30213\n\n\nThe read definition line includes key metadata, which need to be parsed using the appropriate parsing rule.\n\nsource\n\n\nTextFileBaseReader.parse_text\n\n TextFileBaseReader.parse_text (txt:str, pattern:str|None=None)\n\nParse text using regex pattern with groups. Return a metadata dictionary.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntxt\nstr\n\ntext to parse\n\n\npattern\nstr | None\nNone\nIf None, uses standard regex pattern to extract metadata, otherwise, uses passed regex\n\n\nReturns\ndict\n\nparsed metadata in key/value format\n\n\n\n\naln.parse_text(dfn_line, pattern)\n\n{'refseqid': '2591237:ncbi:1',\n 'reftaxonomyid': '2591237',\n 'refsource': 'ncbi',\n 'refseqnb': '1',\n 'readid': '2591237:ncbi:1-40200',\n 'readnb': '40200',\n 'aln_start_pos': '14370',\n 'refseq_strand': '+'}\n\n\n\nsource\n\n\nAlnFileReader.parse_definition_line_with_position\n\n AlnFileReader.parse_definition_line_with_position (dfn_line:str)\n\nParse definition line and adds relative position\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndfn_line\nstr\nfefinition line string to be parsed\n\n\nReturns\ndict\nparsed metadata in key/value format + relative position of the read\n\n\n\nUpon instance creation, AlnFileReader automatically checks the default_parsing_rules.json file for a workable rule among saved rules. Saved rules include the rule for ART Illumina ALN files.\n\naln.re_rule_name\n\n'aln_art_illumina_ncbi_std'\n\n\nIt is therefore not required to pass a specific pattern and keys parameter.\n\naln.parse_text(dfn_line)\n\n{'refseqid': '2591237:ncbi:1',\n 'reftaxonomyid': '2591237',\n 'refsource': 'ncbi',\n 'refseqnb': '1',\n 'readid': '2591237:ncbi:1-40200',\n 'readnb': '40200',\n 'aln_start_pos': '14370',\n 'refseq_strand': '+'}\n\n\nART Ilumina ALN files definition lines consist of:\n\nThe read ID: readid, e.g. 2591237:ncbi:1-20100\nthe read number (order in the file): readnb, e.g. 20100\nThe read start position in the reference sequence: aln_start_pos, e.g. 23878\nThe reference sequence ID: readid, e.g. 2591237:ncbi:1-20100\nThe reference sequence number: refseqnb, e.g. 1\nThe reference sequence source: refsource, e.g. ncbi\nThe reference sequence taxonomy: reftaxonomyid, e.g. 2591237\nThe reference sequence strand: refseq_strand wich is either + or -,\n\n\nsource\n\n\nAlnFileReader.parse_file\n\n AlnFileReader.parse_file (add_ref_seq_aligned:bool=False,\n                           add_read_seq_aligned:bool=False)\n\nRead ALN file, return a dict w/ alignment info for each read and optionaly aligned reference sequence & read\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nadd_ref_seq_aligned\nbool\nFalse\nAdd the reference sequence aligned to the parsed dictionary when True\n\n\nadd_read_seq_aligned\nbool\nFalse\nAdd the read sequence aligned to the parsed dictionary when True\n\n\nReturns\ndict\n\n\n\n\n\n\nparsed = aln.parse_file()\n\nfor i, (k, v) in enumerate(parsed.items()):\n    print(k)\n    pprint(v)\n    if i &gt; 3: break\n\n2591237:ncbi:1-40200\n{'aln_start_pos': '14370',\n 'readid': '2591237:ncbi:1-40200',\n 'readnb': '40200',\n 'refseq_strand': '+',\n 'refseqid': '2591237:ncbi:1',\n 'refseqnb': '1',\n 'refsource': 'ncbi',\n 'reftaxonomyid': '2591237'}\n2591237:ncbi:1-40199\n{'aln_start_pos': '15144',\n 'readid': '2591237:ncbi:1-40199',\n 'readnb': '40199',\n 'refseq_strand': '-',\n 'refseqid': '2591237:ncbi:1',\n 'refseqnb': '1',\n 'refsource': 'ncbi',\n 'reftaxonomyid': '2591237'}\n2591237:ncbi:1-40198\n{'aln_start_pos': '2971',\n 'readid': '2591237:ncbi:1-40198',\n 'readnb': '40198',\n 'refseq_strand': '-',\n 'refseqid': '2591237:ncbi:1',\n 'refseqnb': '1',\n 'refsource': 'ncbi',\n 'reftaxonomyid': '2591237'}\n2591237:ncbi:1-40197\n{'aln_start_pos': '15485',\n 'readid': '2591237:ncbi:1-40197',\n 'readnb': '40197',\n 'refseq_strand': '-',\n 'refseqid': '2591237:ncbi:1',\n 'refseqnb': '1',\n 'refsource': 'ncbi',\n 'reftaxonomyid': '2591237'}\n2591237:ncbi:1-40196\n{'aln_start_pos': '16221',\n 'readid': '2591237:ncbi:1-40196',\n 'readnb': '40196',\n 'refseq_strand': '-',\n 'refseqid': '2591237:ncbi:1',\n 'refseqnb': '1',\n 'refsource': 'ncbi',\n 'reftaxonomyid': '2591237'}\n\n\n\nsource\n\n\nAlnFileReader.parse_header_reference_sequences\n\n AlnFileReader.parse_header_reference_sequences (pattern:str|None=None)\n\nExtract metadata from all header reference sequences\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npattern\nstr | None\nNone\nregex pattern to apply to parse the reference sequence info\n\n\nReturns\ndict\n\nparsed metadata in key/value format\n\n\n\n\npprint(aln.parse_header_reference_sequences())\n\n{'2591237:ncbi:1': {'organism': 'Coronavirus BtRs-BetaCoV/YN2018D',\n                    'refseq_accession': 'MK211378',\n                    'refseq_length': '30213',\n                    'refseqid': '2591237:ncbi:1',\n                    'refseqnb': '1',\n                    'refsource': 'ncbi',\n                    'reftaxonomyid': '2591237'}}\n\n\n\nsource\n\n\nAlnFileReader.set_header_parsing_rules\n\n AlnFileReader.set_header_parsing_rules (pattern:str|None=None,\n                                         verbose:bool=False)\n\n*Set the regex parsing rule for reference sequence in ALN header.\nUpdates 3 class attributes: re_header_rule_name, re_header_pattern, re_header_keys\nTODO: refactor this and the method in Core: to use a single function for the common part and a parameter for the text_to_parse*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npattern\nstr | None\nNone\nregex pattern to apply to parse the text, search in parsing rules json if None\n\n\nverbose\nbool\nFalse\nwhen True, provides information on each rule\n\n\nReturns\nNone\n\n\n\n\n\n\naln.set_header_parsing_rules(verbose=True)\n\n--------------------------------------------------------------------------------\nRule &lt;fasta_ncbi_std&gt; generated 0 matches\n--------------------------------------------------------------------------------\n^&gt;(?P&lt;seqid&gt;(?P&lt;taxonomyid&gt;\\d+):(?P&lt;source&gt;ncbi):(?P&lt;seqnb&gt;\\d*))[\\s\\t]*(?P=seqnb)[\\s\\t](?P&lt;accession&gt;[\\w\\d]*)([\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P=source)[\\s\\t][\\s\\t]*(?P&lt;organism&gt;[\\w\\s\\-\\_/]*))?\n['seqid', 'taxonomyid', 'source', 'accession', 'seqnb', 'organism']\n{'seqid': '', 'taxonomyid': '', 'source': '', 'seqnb': '', 'accession': '', 'organism': ''}\n--------------------------------------------------------------------------------\nRule &lt;fastq_art_illumina_ncbi_std&gt; generated 0 matches\n--------------------------------------------------------------------------------\n^@(?P&lt;readid&gt;(?P&lt;reftaxonomyid&gt;\\d*):(?P&lt;refsource&gt;\\w*):(?P&lt;refseqnb&gt;\\d*)-(?P&lt;readnb&gt;\\d*(\\/\\d)?))$\n['readid', 'reftaxonomyid', 'refsource', 'refseqnb', 'readnb']\n{'readid': '', 'reftaxonomyid': '', 'refsource': '', 'refseqnb': '', 'readnb': ''}\n--------------------------------------------------------------------------------\nRule &lt;aln_art_illumina_ncbi_std&gt; generated 0 matches\n--------------------------------------------------------------------------------\n^&gt;(?P&lt;refseqid&gt;(?P&lt;reftaxonomyid&gt;\\d*):(?P&lt;refsource&gt;\\w*):(?P&lt;refseqnb&gt;\\d*))(\\s| )*(?P&lt;readid&gt;(?P=reftaxonomyid):(?P=refsource):(?P=refseqnb)-(?P&lt;readnb&gt;\\d*(\\/\\d(-\\d)?)?))(\\s|\\t)(?P&lt;aln_start_pos&gt;\\d*)(\\s|\\t)(?P&lt;refseq_strand&gt;(-|\\+))$\n['refseqid', 'reftaxonomyid', 'refsource', 'refseqnb', 'readid', 'readnb', 'aln_start_pos', 'refseq_strand']\n{'refseqid': '', 'reftaxonomyid': '', 'refsource': '', 'refseqnb': '', 'readid': '', 'readnb': '', 'aln_start_pos': '', 'refseq_strand': ''}\n--------------------------------------------------------------------------------\nRule &lt;aln_art_illumina-refseq-ncbi-std&gt; generated 7 matches\n--------------------------------------------------------------------------------\n^@SQ[\\t\\s]*(?P&lt;refseqid&gt;(?P&lt;reftaxonomyid&gt;\\d*):(?P&lt;refsource&gt;\\w*):(?P&lt;refseqnb&gt;\\d*))[\\t\\s]*(?P=refseqnb)[\\t\\s]*(?P&lt;refseq_accession&gt;[\\w\\d]*)[\\t\\s]*(?P=reftaxonomyid)[\\t\\s]*(?P=refsource)[\\t\\s](?P&lt;organism&gt;.*)[\\t\\s](?P&lt;refseq_length&gt;\\d*)$\n['refseqid', 'reftaxonomyid', 'refsource', 'refseqnb', 'refseq_accession', 'organism', 'refseq_length']\n{'refseqid': '2591237:ncbi:1', 'reftaxonomyid': '2591237', 'refsource': 'ncbi', 'refseqnb': '1', 'refseq_accession': 'MK211378', 'organism': 'Coronavirus BtRs-BetaCoV/YN2018D', 'refseq_length': '30213'}\n--------------------------------------------------------------------------------\nRule &lt;fasta_ncbi_cov&gt; generated 0 matches\n--------------------------------------------------------------------------------\n^&gt;(?P&lt;seqid&gt;(?P&lt;taxonomyid&gt;\\d+):(?P&lt;source&gt;ncbi):(?P&lt;seqnb&gt;\\d*))[\\s\\t]*\\[(?P&lt;accession&gt;[\\w\\d]*)\\]([\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P=source)[\\s\\t]*(?P=seqnb)[\\s\\t]*\\[(?P=accession)\\][\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P&lt;organism&gt;[\\w\\s\\-\\_\\/]*))?\n['seqid', 'taxonomyid', 'source', 'accession', 'seqnb', 'organism']\n{'seqid': '', 'taxonomyid': '', 'source': '', 'seqnb': '', 'accession': '', 'organism': ''}\n--------------------------------------------------------------------------------\nRule &lt;fasta_rhinolophus_ferrumequinum&gt; generated 0 matches\n--------------------------------------------------------------------------------\n^&gt;\\d[\\s\\t](?P&lt;seq_type&gt;dna_rm):(?P&lt;id_type&gt;[\\w\\_]*)[\\s\\w](?P=id_type):(?P&lt;assy&gt;[\\w\\d\\_]*)\\.(?P&lt;seq_level&gt;[\\w]*):\\d*:\\d*:(?P&lt;taxonomy&gt;\\d*):(?P&lt;id&gt;\\d*)[\\s    ]REF$\n['seq_type', 'id_type', 'assy', 'seq_level', 'taxonomy', 'id']\n{'seq_type': '', 'id_type': '', 'assy': '', 'seq_level': '', 'taxonomy': '', 'id': ''}\n--------------------------------------------------------------------------------\nSelected rule with most matches: aln_art_illumina-refseq-ncbi-std\n\n\n\nprint(aln.re_header_rule_name)\nprint(aln.re_header_pattern)\nprint(aln.re_header_keys)\n\naln_art_illumina-refseq-ncbi-std\n^@SQ[\\t\\s]*(?P&lt;refseqid&gt;(?P&lt;reftaxonomyid&gt;\\d*):(?P&lt;refsource&gt;\\w*):(?P&lt;refseqnb&gt;\\d*))[\\t\\s]*(?P=refseqnb)[\\t\\s]*(?P&lt;refseq_accession&gt;[\\w\\d]*)[\\t\\s]*(?P=reftaxonomyid)[\\t\\s]*(?P=refsource)[\\t\\s](?P&lt;organism&gt;.*)[\\t\\s](?P&lt;refseq_length&gt;\\d*)$\n['refseqid', 'reftaxonomyid', 'refsource', 'refseqnb', 'refseq_accession', 'organism', 'refseq_length']",
    "crumbs": [
      "CNN Virus",
      "data"
    ]
  },
  {
    "objectID": "cnn_virus_data.html#plain-text-based-data",
    "href": "cnn_virus_data.html#plain-text-based-data",
    "title": "data",
    "section": "Plain text based data",
    "text": "Plain text based data\nThe datasets provided by CNN Virus team are in plain text format, one sequence per line, with format as follows:\nAAAAAGATTTTGAGAGAGGTCGACCTGTCCTCCTAAAACGTTTACAAAAG  71  0\nCATGTAACGCAGCTTAGTCCGATCGTGGCTATAATCCGTCTTTCGATTTG  1   7\nAACAACATCTTGTTGATGATAACCGTCAAAGTGTTTTGGGTCTGGAGGGA  158 6\nAGTACCTGGAGAGCGTTAAGAAACACAAACGGCTGGATGTAGTGCCGCGC  6   7\nCCACGTCGATGAAGCTCCGACGAGAGTCGGCGCTGAGCCCGCGCACCTCC  71  6\nAGCTCGTGGATCTCCCCTCCTTCTGCAGTTTCAACATCAGAAGCCCTGAA  87  1\nThe first column is the k-base (k-mer) read, followed byt the specie label and the relative position.\nWhen using this type of data, the data pipeline consist of:\n\nCreating a TextFileDataset instance to load the data\nUsing the created dataset to in a Dataloader used for training or inference.\n\n\nsource\n\nTextFileDataset\n\n TextFileDataset (p2file:str|pathlib.Path)\n\nLoad data from text file and yield (BHE sequence tensor, (label OHE tensor, position OHE tensor))\n\n\n\n\nType\nDetails\n\n\n\n\np2file\nstr | pathlib.Path\npath to the file to read\n\n\n\nThis IterableDataset reads the file line by line and yields a tupple (seq_bhe, (lbl_ohe, pos_ohe)):\n\nseq_bhe: tensor of shape [k, 5] with the k-mer read in base-hot-encoded format\nlbl_ohe: tensor of shape [187] with the specie label in one-hot-encoded format\npos_ohe: tensor of shape [10] with the relative position label in one-hot-encoded format\n\nThe dataset is then used with a pytorch DataLoader to handle batching. When using the GPU, the pin_memory=True should be used to make transfer of data to the GPU faster.\nLet’s use a small data file to illustrate the process.\n\np2file = Path('data_dev/CNN_Virus_data/50mer_ds_100_seq')\nassert p2file.is_file()\n\nds = TextFileDataset(p2file)\ndl = DataLoader(ds, batch_size=8, num_workers=2, pin_memory=False)\n\nfor seq_batch, (lbl_batch, pos_batch) in dl:\n    print(seq_batch.shape, lbl_batch.shape, pos_batch.shape)\n\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\ntorch.Size([4, 50, 5]) torch.Size([4, 187]) torch.Size([4, 10])\ntorch.Size([4, 50, 5]) torch.Size([4, 187]) torch.Size([4, 10])\n\n\nThe read sequence tensor is BHE:\n\nseq_batch[:2, :3, :]\n\ntensor([[[1, 0, 0, 0, 0],\n         [1, 0, 0, 0, 0],\n         [0, 1, 0, 0, 0]],\n\n        [[1, 0, 0, 0, 0],\n         [0, 0, 0, 1, 0],\n         [1, 0, 0, 0, 0]]])\n\n\n\nlbl_batch[:2, :], lbl_batch.argmax(dim=1)[:2]\n\n(tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0.]]),\n tensor([94, 18]))",
    "crumbs": [
      "CNN Virus",
      "data"
    ]
  },
  {
    "objectID": "cnn_virus_data.html#aln-file-based",
    "href": "cnn_virus_data.html#aln-file-based",
    "title": "data",
    "section": "ALN file based",
    "text": "ALN file based\nAll simulated reads are stored in a .fastq and .aln file. We use the .aln file because it includes all metadata information required to retrieve both the specie and the relative position of each read.\nAlnFileDataset allows to load reads and metadata stored in ART Illumina simulated ALN file.\n\np2aln = pfs.data / 'ncbi/simreads/cov/single_1seq_150bp/single_1seq_150bp.aln'\naln = AlnFileReader(p2aln)\n\nfor i, d in enumerate(aln):\n    pass\nprint(f\"{i+1:,d} reads in this ALN file\")\n\n40,200 reads in this ALN file\n\n\n\np2aln = pfs.data / 'ncbi/simreads/cov/single_1seq_150bp/single_2seq_150bp.aln'\naln.reset_iterator()\nfor i, seq in enumerate(aln):\n    print(aln.parse_definition_line_with_position(seq['definition line']))\n    if i+1 &gt;= 3: break\n\nfor i, seq in enumerate(aln):\n    print(aln.parse_definition_line_with_position(seq['definition line']))\n    if i+1 &gt;= 3: break\n\n{'refseqid': '2591237:ncbi:1', 'reftaxonomyid': '2591237', 'refsource': 'ncbi', 'refseqnb': '1', 'readid': '2591237:ncbi:1-40200', 'readnb': '40200', 'aln_start_pos': '14370', 'refseq_strand': '+', 'read_pos': 5}\n{'refseqid': '2591237:ncbi:1', 'reftaxonomyid': '2591237', 'refsource': 'ncbi', 'refseqnb': '1', 'readid': '2591237:ncbi:1-40199', 'readnb': '40199', 'aln_start_pos': '15144', 'refseq_strand': '-', 'read_pos': 6}\n{'refseqid': '2591237:ncbi:1', 'reftaxonomyid': '2591237', 'refsource': 'ncbi', 'refseqnb': '1', 'readid': '2591237:ncbi:1-40198', 'readnb': '40198', 'aln_start_pos': '2971', 'refseq_strand': '-', 'read_pos': 1}\n{'refseqid': '2591237:ncbi:1', 'reftaxonomyid': '2591237', 'refsource': 'ncbi', 'refseqnb': '1', 'readid': '2591237:ncbi:1-40197', 'readnb': '40197', 'aln_start_pos': '15485', 'refseq_strand': '-', 'read_pos': 6}\n{'refseqid': '2591237:ncbi:1', 'reftaxonomyid': '2591237', 'refsource': 'ncbi', 'refseqnb': '1', 'readid': '2591237:ncbi:1-40196', 'readnb': '40196', 'aln_start_pos': '16221', 'refseq_strand': '-', 'read_pos': 6}\n{'refseqid': '2591237:ncbi:1', 'reftaxonomyid': '2591237', 'refsource': 'ncbi', 'refseqnb': '1', 'readid': '2591237:ncbi:1-40195', 'readnb': '40195', 'aln_start_pos': '18953', 'refseq_strand': '-', 'read_pos': 7}\n\n\n\nsource\n\nAlnFileDataset\n\n AlnFileDataset (p2file:str|pathlib.Path, label:int=118,\n                 return_metadata:bool=False, refseqid:str|None=None)\n\n*Load data and metadata from ALN file, yield BHE sequence, OHE label, OHE position tensors + metadata\nThe iterator yield tupple (read tensor,(label tensor, position tensor)):\n\nkmer read tensor in base hot encoded format (shape [k, 5])\nlabel tensor in one hot encoded format (shape [187])\nposition tensor in one hote encoded format (shape [10])\n\nIt also optionally returns a dictionary of the read metadata available in the ALN file.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np2file\nstr | pathlib.Path\n\npath to the file to read\n\n\nlabel\nint\n118\nlabel for this batch (assuming all reads are from the same species)\n\n\nreturn_metadata\nbool\nFalse\nyield each read metadata as a dictionary when True\n\n\nrefseqid\nstr | None\nNone\nrefseqid to filter reference sequence (default is no filtering)\n\n\n\nCreate a dataset from a ALN file, then pass it to the class Dataloader to create batches of data\n\nds = AlnFileDataset(p2aln, label=118)\n\ndl = DataLoader(ds, batch_size=16, shuffle=False)\n\nfor i, (s, (lbl,pos)) in enumerate(dl):\n    print(f\"Sample {i+1:000d}:\")\n    print(f\"  Shapes of each data tensor:\")\n    print(f\"    seq: {s.shape}, label: {lbl.shape}, pos: {pos.shape}\")\n    if i+1 &gt;= 3:break\n\nSample 1:\n  Shapes of each data tensor:\n    seq: torch.Size([16, 150, 5]), label: torch.Size([16, 187]), pos: torch.Size([16, 10])\nSample 2:\n  Shapes of each data tensor:\n    seq: torch.Size([16, 150, 5]), label: torch.Size([16, 187]), pos: torch.Size([16, 10])\nSample 3:\n  Shapes of each data tensor:\n    seq: torch.Size([16, 150, 5]), label: torch.Size([16, 187]), pos: torch.Size([16, 10])\n\n\nWhen an AlnFileDataset instance with return_metadata set as True is passed to DataLoader, the dataloader will yield (read_tensor,(lbl_tensor, pos_tensor), metadata) where:\n\nread_tensor is the batch of kmer read tensor in base hot encoded format (shape [bs, k, 5])\nlbl_tensor is the batch of label tensor in one hot encoded format (shape [bs, 187])\npos_tensor is the batch of position tensor in one hote encoded format (shape [bs, 10])\nmetadata is a dictionary of form key:list like:\n\n{\n    'aln_start_pos': ['14370', '15144', '2971'], \n    'readid': ['2591237:ncbi:1-40200', '2591237:ncbi:1-40199', '2591237:ncbi:1-40198'], \n    'readnb': ['40200', '40199', '40198'], \n    'refseq_strand': ['+', '-', '-'], \n    'refseqid': ['2591237:ncbi:1', '2591237:ncbi:1', '2591237:ncbi:1'], \n    'refseqnb': ['1', '1', '1'], \n    'refsource': ['ncbi', 'ncbi', 'ncbi'], \n    'reftaxonomyid': ['2591237', '2591237', '2591237'], \n    'read_pos': tensor([ 5,  6,  1])\n}\n\nds = AlnFileDataset(p2aln, label=118, return_metadata=True)\ndl = DataLoader(ds, batch_size=4, shuffle=False)\n\nfor i, (s, (lbl,pos), d) in enumerate(dl):\n    print(f\"Sample {i+1:000d}:\")\n    print(f\"  Shapes of each data tensor:\")\n    print(f\"    seq: {s.shape}, label: {lbl.shape}, pos: {pos.shape}\")\n    print(f\"  Metadata dictionary for the batch\")\n    print(f\"    {d['readid']} {d['refseqid']}\")\n    if i+1 &gt;= 3:break\n\nSample 1:\n  Shapes of each data tensor:\n    seq: torch.Size([4, 150, 5]), label: torch.Size([4, 187]), pos: torch.Size([4, 10])\n  Metadata dictionary for the batch\n    ['2591237:ncbi:1-40200', '2591237:ncbi:2-40199', '2591237:ncbi:1-40198', '2591237:ncbi:1-40197'] ['2591237:ncbi:1', '2591237:ncbi:2', '2591237:ncbi:1', '2591237:ncbi:1']\nSample 2:\n  Shapes of each data tensor:\n    seq: torch.Size([4, 150, 5]), label: torch.Size([4, 187]), pos: torch.Size([4, 10])\n  Metadata dictionary for the batch\n    ['2591237:ncbi:2-40196', '2591237:ncbi:1-40195', '2591237:ncbi:2-40194', '2591237:ncbi:2-40193'] ['2591237:ncbi:2', '2591237:ncbi:1', '2591237:ncbi:2', '2591237:ncbi:2']\nSample 3:\n  Shapes of each data tensor:\n    seq: torch.Size([4, 150, 5]), label: torch.Size([4, 187]), pos: torch.Size([4, 10])\n  Metadata dictionary for the batch\n    ['2591237:ncbi:1-40192', '2591237:ncbi:1-40191', '2591237:ncbi:1-40190', '2591237:ncbi:1-40189'] ['2591237:ncbi:1', '2591237:ncbi:1', '2591237:ncbi:1', '2591237:ncbi:1']\n\n\nAlnFileDataset also allows to filter reads to match one specific reference sequence. If this option is selected, only simulated reads generated from that specific reference sequence will be loaded and yielded.\n\np2aln_2seq = pfs.data / 'ncbi/simreads/cov/single_1seq_150bp/single_2seq_150bp.aln'\nassert p2aln_2seq.is_file()\n\nds = AlnFileDataset(p2aln_2seq, label=118, return_metadata=True, refseqid='2591237:ncbi:2')\ndl = DataLoader(ds, batch_size=3, shuffle=False)\n\nfor i, (s, (lbl,pos), d) in enumerate(dl):\n    print(f\"Sample {i+1:000d}:\")\n    print(f\"  Shapes of each data tensor:\")\n    print(f\"    seq: {s.shape}, label: {lbl.shape}, pos: {pos.shape}\")\n    print(f\"  Metadata dictionary for the batch\")\n    print(f\"    {d['readid']} from {d['refseqid']}\")\n    if i+1 &gt;= 3: \n        print(\"Breaking out of the loop\")\n        break\n\nFiltering reads to only keep those aligned to refseqid 2591237:ncbi:2\nSample 1:\n  Shapes of each data tensor:\n    seq: torch.Size([3, 150, 5]), label: torch.Size([3, 187]), pos: torch.Size([3, 10])\n  Metadata dictionary for the batch\n    ['2591237:ncbi:2-40199', '2591237:ncbi:2-40196', '2591237:ncbi:2-40194'] from ['2591237:ncbi:2', '2591237:ncbi:2', '2591237:ncbi:2']\nSample 2:\n  Shapes of each data tensor:\n    seq: torch.Size([3, 150, 5]), label: torch.Size([3, 187]), pos: torch.Size([3, 10])\n  Metadata dictionary for the batch\n    ['2591237:ncbi:2-40193', '2591237:ncbi:2-40185', '2591237:ncbi:2-40183'] from ['2591237:ncbi:2', '2591237:ncbi:2', '2591237:ncbi:2']\nSample 3:\n  Shapes of each data tensor:\n    seq: torch.Size([1, 150, 5]), label: torch.Size([1, 187]), pos: torch.Size([1, 10])\n  Metadata dictionary for the batch\n    ['2591237:ncbi:2-30794'] from ['2591237:ncbi:2']\nBreaking out of the loop",
    "crumbs": [
      "CNN Virus",
      "data"
    ]
  },
  {
    "objectID": "cnn_virus_data.html#handle-long-reads",
    "href": "cnn_virus_data.html#handle-long-reads",
    "title": "data",
    "section": "Handle long reads",
    "text": "Handle long reads\n\nPreprocessing\nCNN Virus handles 50-mer reads only. Longer reads need to be split into 50-mer reads by sliding a window of size 50 along the read sequence. The set of multiple 50-mer reads is then used as input to the model. After prediction on the set of 50-mer reads, the final prediction is obtained by filtering 50-mer prediction that yield a hight enough probability and then voting for the most predicted species.\nA k-mer (150-mer) read will be split in k-49 (101) 50-mer reads which will yield k-49 (101) probability tensors. The final prediction will be the species with the highest number of votes, as long as their respective probabilities &gt; 90%.\n\nsource\n\n\nsplit_kmer_batch_into_50mers\n\n split_kmer_batch_into_50mers (kmer_b:torch.Tensor,\n                               labels:tuple[torch.Tensor,torch.Tensor]|Non\n                               e=None)\n\n*Convert a batch of k-mer reads into 50-mer reads, by shifting the k-mer one base at a time.\nShapes:\n\nkmer_b: [b,k,5] -&gt; [b * (k - 49), 50, 5]\nlabel/position_b: [b, nb_class] -&gt; [b * (k - 49), nb_class]\n\nTechnical Note: we use advanced indexing of the tensor to create the 50-mer and roll them, with no loop.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nkmer_b\nTensor\n\ntensor representing a batch of k-mer reads, BHE format, shape [b, k, 5]\n\n\nlabels\ntuple[torch.Tensor, torch.Tensor] | None\nNone\noptional tuple with tensor for label and position batches\n\n\nReturns\ntuple\n\nbatch tensor for 50-mer reads, (optional) labels and positions batch tensor\n\n\n\nWe can test the function, with a test tensor designed to make the validation easier.\nWe create a tensor with a batch of b k-mer reads in the following format:\nread 1: 10001, 10002, 10003, .... 10149\nread 2: 20001, 20002, 20003, .... 20149\nread 3: 30001, 30002, 30003, .... 30149\n    ...\nWe then add an additional dimension to simulate the BHE encoding, by repeating the same value 5 times.\n\nb, k = 4, 150\n\ndef _create_kmer_batch(b:int, k:int)-&gt; torch.Tensor:\n    \"\"\"Create a batch of kmer reads, with a fake encoding in 5 dimensions (like BHE)\"\"\"\n    # Build a tensor of shape (b,k) with the pattern described above\n    kmer_b = torch.stack([(i+1) * 10000 + torch.arange(k) for i in range(b)], dim=0)\n    # simulate the BHE buy duplicating the 5 dimensions\n    kmer_b = torch.stack([kmer_b]*5, dim=2)\n    return kmer_b\n\nkmer_b = _create_kmer_batch(b,k)\nprint(kmer_b.shape)\n# Slice the tensor to only show the first and last three bases, and only one of the 5 BHE dimensions\nidx_bases = np.array([0,1,2,147,148,149])\nkmer_b[:, idx_bases, 0]\n\ntorch.Size([4, 150, 5])\n\n\ntensor([[10000, 10001, 10002, 10147, 10148, 10149],\n        [20000, 20001, 20002, 20147, 20148, 20149],\n        [30000, 30001, 30002, 30147, 30148, 30149],\n        [40000, 40001, 40002, 40147, 40148, 40149]])\n\n\nWe see that the function split_kmer_batch_into_50mers returns a tensor with shape [b, k-49, 50, 5] as expected.\n\nsplit_50mer_read_b, (split_50mer_label_b, split_50mer_position_b) = split_kmer_batch_into_50mers(kmer_b)\nprint(kmer_b.shape)\nprint(split_50mer_read_b.shape)\n\ntorch.Size([4, 150, 5])\ntorch.Size([404, 50, 5])\n\n\nLets slice only the first and last two 50-mer reads for each batch and a selection of bases to confirm that the batch is properly split.\n\ndim0_idxs = [0,1,99,100,101,102,200,201,202,203,301,302]\ndim1_idxs = [0,1,25,26,48,49]\ndim2_idxs = [0,1,2,3,4]\nprint(split_50mer_read_b[np.ix_(dim0_idxs, dim1_idxs, dim2_idxs)][:,:,0])\n\ntensor([[10000, 10001, 10025, 10026, 10048, 10049],\n        [10001, 10002, 10026, 10027, 10049, 10050],\n        [10099, 10100, 10124, 10125, 10147, 10148],\n        [10100, 10101, 10125, 10126, 10148, 10149],\n        [20000, 20001, 20025, 20026, 20048, 20049],\n        [20001, 20002, 20026, 20027, 20049, 20050],\n        [20099, 20100, 20124, 20125, 20147, 20148],\n        [20100, 20101, 20125, 20126, 20148, 20149],\n        [30000, 30001, 30025, 30026, 30048, 30049],\n        [30001, 30002, 30026, 30027, 30049, 30050],\n        [30099, 30100, 30124, 30125, 30147, 30148],\n        [30100, 30101, 30125, 30126, 30148, 30149]])\n\n\n\n\nPostprocessing\nThe model CNN Virus requires post inderence processing of the predictions: - before inference, each k-mer was split into \\(k-49\\) 50-mer, where were presented to the model for inference. Each k-mer led to \\(k-49\\) predictions (probability tensors) - now, we need to combine these $n = k-49 $ probability tensor into a single prediction for the original k-mer. This is done by first filtering all 50-reads that gave a max probability lower than a specific threshold (0.9 by default) and then combining the prediction by a simple vote\nNow we can build the function combine_predictions step by step.\nFirst, lets create a test probabilities tensor representing a batch of bs k-mers, each split into k-49 50-mers, with a probability tensor of shape [bs, k-49, nb_class].\n\ndef create_test_probs(bs=4, k=150, nb_class=187):\n    print(f\"Creating a batch of probabilities for {bs} {k}-mer reads with {nb_class} classes:\")\n\n    probs = torch.from_numpy(np.random.rand(bs * n ,nb_class)).reshape(-1,n,nb_class)\n    print(f\"probs {probs.shape}\")\n    return probs\n\nbs, k, nb_class = 4, 55, 10\nn = k - 49\nprobs = create_test_probs(bs, k, nb_class)\n\nCreating a batch of probabilities for 4 55-mer reads with 10 classes:\nprobs torch.Size([4, 6, 10])\n\n\nStep 1: Extract the predictions from probabilities for each 50-mer read, then filter the predictions with a probability lower than a threshold.\n\nthreshold = 0.8\npreds = probs.argmax(dim=2)\nprint(f\"preds {preds.shape}:\\n\",preds)\n\nINVALID = 9999\ninvalid_labels_filter = probs.max(dim=2).values &lt;= threshold\npreds[invalid_labels_filter] = INVALID\nprint(f\"preds {preds.shape}:\\n\",preds)\n\npreds torch.Size([4, 6]):\n tensor([[6, 6, 3, 7, 6, 8],\n        [6, 5, 1, 2, 6, 9],\n        [4, 3, 8, 4, 4, 3],\n        [4, 5, 8, 0, 6, 6]])\npreds torch.Size([4, 6]):\n tensor([[6, 6, 3, 7, 6, 8],\n        [6, 5, 1, 2, 6, 9],\n        [4, 3, 8, 4, 4, 3],\n        [4, 5, 8, 0, 6, 6]])\n\n\nStep 2: Extract the prediction unique values and their counts.\n\nCode explanation note:\nWe do not want to use a python loop. This is why we use torch.unique(preds) with return_inverse. The function returns a flat tensor with the unique values accross the entire preds and a tensor of the same shape as preds with the indices of the unique values in preds.\n\n    x = torch.tensor([[10, 30, 20, 30],[20,20,20,50]], dtype=torch.long)\n\n    uniques, inverse = torch.unique(x, sorted=True, return_inverse=True)\n\n    &gt; unique:  tensor([10, 20, 30, 50])\n      inverse: tensor([[0, 2, 1, 2],\n                       [1, 1, 1, 3]]))\n\n# Get unique values and their counts for the entire tensor\nunique_values, inverse_indices = torch.unique(preds, return_inverse=True)\ninverse_indices = inverse_indices.view(preds.shape)\nprint(f\"unique_values {unique_values.shape}:\\n\",unique_values)\nprint(f\"inverse_indices {inverse_indices.shape}:\\n\",inverse_indices)\n\nunique_values torch.Size([10]):\n tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\ninverse_indices torch.Size([4, 6]):\n tensor([[6, 6, 3, 7, 6, 8],\n        [6, 5, 1, 2, 6, 9],\n        [4, 3, 8, 4, 4, 3],\n        [4, 5, 8, 0, 6, 6]])\n\n\nStep 3: Compute the counts of unique values in each 50-mer read and store it in a tensor of shape (batch size, nb of unique values in the the shole tensor preds).\n\n# Create a tensor to hold the counts (shape (bs, nb_unique_values_across_the_batch))\ncounts = torch.zeros((preds.shape[0], unique_values.shape[0]), dtype=torch.int64)\n# Count occurrences of each unique value per row\ncounts.scatter_add_(dim=1, index=inverse_indices, src=torch.ones_like(inverse_indices, dtype=torch.int64))\nprint(f\"unique_values {unique_values.shape}:\\n\", unique_values)\nprint(f\"counts {counts.shape}:\\n\", counts)\n\nunique_values torch.Size([10]):\n tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\ncounts torch.Size([4, 10]):\n tensor([[0, 0, 0, 1, 0, 0, 3, 1, 1, 0],\n        [0, 1, 1, 0, 0, 1, 2, 0, 0, 1],\n        [0, 0, 0, 2, 3, 0, 0, 0, 1, 0],\n        [1, 0, 0, 0, 1, 1, 2, 0, 1, 0]])\n\n\nStep 4: Get the index of the most frequent value for each 50-mer read, excluding the INVALID placeholder, and extract the corresponding values into a tensor of shape (bs).\n\n# get value most voted per 50-read (vertical tensor), excluding the placeholder INVALID\nmost_voted_idxs = counts[:, :-1].argmax(dim=1)\nmost_voted_value = unique_values[most_voted_idxs][:, None]\nprint(f\"most_voted_value {most_voted_value.shape}:\\n\", most_voted_value)\n\nmost_voted_value torch.Size([4, 1]):\n tensor([[6],\n        [6],\n        [4],\n        [6]])\n\n\nWe put it all in the function combine_predictions\n\nsource\n\n\ncombine_predictions\n\n combine_predictions (label_probs:torch.Tensor, pos_probs:torch.Tensor,\n                      threshold:float=0.9)\n\n*Combine a batch of 50-mer probabilities into one batch of final prediction for label and position\nNote: the input must be of shape (batch_size, n, c) where n is k-49 and c is the nb of labels or positions*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlabel_probs\nTensor\n\nProbabilities for the labels classes for each 50-mer (shape: [bs, k-49,187])\n\n\npos_probs\nTensor\n\nProbabilities for the position classes for each 50-mer (shape: [bs, k-49,10])\n\n\nthreshold\nfloat\n0.9\nThreshold to consider a prediction as valid\n\n\nReturns\ntuple\n\nPredicted labels and positions for each read (shape: [bs, 187], [bs, 10])\n\n\n\nTesting the function with the same test tensor\n\ncombine_predictions(probs, probs[:, :,:10])\n\n(tensor([3, 1, 4, 6]), tensor([3, 1, 4, 6]))\n\n\nTesting the function with test tensor correponding to real probabilities\n\nbs, k = 1, 150\nlabel_probs = create_test_probs(bs, k, 187)\npos_probs = create_test_probs(bs, k, 10)\n\nprint('\\ncombine for a batch of data')\np = combine_predictions(label_probs,pos_probs)\nprint(p)\nprint('\\ncombine for a single sample (not a batch)')\np = combine_predictions(label_probs[0,:,:],pos_probs[0,:,:])\nprint(p)\n\nCreating a batch of probabilities for 1 150-mer reads with 187 classes:\nprobs torch.Size([1, 6, 187])\nCreating a batch of probabilities for 1 150-mer reads with 10 classes:\nprobs torch.Size([1, 6, 10])\n\ncombine for a batch of data\n(tensor(82), tensor(6))\n\ncombine for a single sample (not a batch)\nConverting probability tensors to 3 dimensions\n(tensor(82), tensor(6))",
    "crumbs": [
      "CNN Virus",
      "data"
    ]
  },
  {
    "objectID": "art.html",
    "href": "art.html",
    "title": "art",
    "section": "",
    "text": "ART is an open source package simmulation next generation read of genomes, available on the website of the National Institute of Environmental Health Sciences here. It is a command line interface package. This module makes the package accessible from a jupyter notebook\nTypical usage\n\nread simulation with paired reads:\n\nart_illumina -ss HS25 -sam -i file.fa -p -l 150 -f 20 -m 200 -s 10 -o paired_seq_1\n\nread simulation with single reads:\n\nart_illumina -ss HS25 -sam -i file.fa -l 150 -f 10 -o single_seq_1\n\n\nWhere the parameters are:\n  -f   --fcov   the fold of read coverage to be simulated or number of reads/read pairs generated for each amplicon\n  -i   --in     the filename of input DNA/RNA reference\n  -l   --len    the length of reads to be simulated\n  -m   --mflen  the mean size of DNA/RNA fragments for paired-end simulations\n  -o   --out    the prefix of output filename\n  -p   --paired indicate a paired-end read simulation or to generate reads from both ends of amplicons\n                NOTE: art will automatically switch to a mate-pair simulation if the given mean fragment size &gt;= 2000\n  -s   --sdev   the standard deviation of DNA/RNA fragment size for paired-end simulations.\n  -sam --samout indicate to generate SAM alignment file\n  -ss  --seqSys The name of Illumina sequencing system of the built-in profile used for simulation\n                NOTE: sequencing system ID names are:\n                GA1 - GenomeAnalyzer I (36bp,44bp), GA2 - GenomeAnalyzer II (50bp, 75bp)\n                HS10 - HiSeq 1000 (100bp),          HS20 - HiSeq 2000 (100bp),      HS25 - HiSeq 2500 (125bp, 150bp)\n                HSXn - HiSeqX PCR free (150bp),     HSXt - HiSeqX TruSeq (150bp),   MinS - MiniSeq TruSeq (50bp)\n                MSv1 - MiSeq v1 (250bp),            MSv3 - MiSeq v3 (250bp),        NS50 - NextSeq500 v2 (75bp)\nNotes:\n\nFor single-end simulation, ART requires input sequence file, output file prefix, read length, and read count/fold coverage.\nFor paired-end simulation (except for amplicon sequencing), ART also requires the parameter values of the mean and standard deviation of DNA/RNA fragment lengths\n\n\nsource\n\nArtIllumina\n\n ArtIllumina (path2app:str|pathlib.Path, input_dir:str|pathlib.Path,\n              output_dir:str|pathlib.Path=None,\n              app_in_system_path:bool=False)\n\nClass to handle all aspects of simulating sequencing with art_illumina\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath2app\nstr | pathlib.Path\n\nfull path to art_illumina application on the system\n\n\ninput_dir\nstr | pathlib.Path\n\nfull path to dir where input files are\n\n\noutput_dir\nstr | pathlib.Path\nNone\nfull path to dir where to save output files, if different from input_dir\n\n\napp_in_system_path\nbool\nFalse\nwhether art_illumina is in the system path or not\n\n\n\n\nUsage\n\nCreate an instance of ArtIllumina\nRun a simulation\nExport output files\n\nCreate an instance of ArtIllumina with: - the path to the application on the local system - the directories for input and output files (optional)\n\np2art = Path('/bin/art_illumina')\nassert p2art.exists()\np2data = Path('data_dev/ncbi/refsequences/cov')\nassert p2data.exists()\n\n\nart = ArtIllumina(\n    path2app=p2art,\n    input_dir=p2data,\n    )\n\nReady to operate with art: /bin/art_illumina\nInput files from : /home/vtec/projects/bio/metagentools/nbs-dev/data_dev/ncbi/refsequences/cov\nOutput files to :  /home/vtec/projects/bio/metagentools/nbs-dev/data_dev/ncbi/refsequences/cov\n\n\n\nnbdev.show_doc(ArtIllumina.sim_reads)\n\n\nsource\n\nArtIllumina.sim_reads\n\n ArtIllumina.sim_reads (input_file:str, output_seed:str,\n                        sim_type:str='single', read_length:int=150,\n                        fold:int=10, mean_read:int=None,\n                        std_read:int=None, ss:str='HS25',\n                        overwrite:bool=False, print_output:bool=True)\n\nSimulates reads with art_illumina. Output files saved in a separate directory\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_file\nstr\n\nname of the fasta file to use as input\n\n\noutput_seed\nstr\n\nseed to use for the output files\n\n\nsim_type\nstr\nsingle\ntype of read simmulation: ‘single’ or ‘paired’\n\n\nread_length\nint\n150\nlength of the read in bp\n\n\nfold\nint\n10\nfold\n\n\nmean_read\nint\nNone\nmean length of the read for paired reads\n\n\nstd_read\nint\nNone\nstd of the read length, for paired reads\n\n\nss\nstr\nHS25\nquality profile to use for simulation,\n\n\noverwrite\nbool\nFalse\noverwrite existing output files if true, raise error if false\n\n\nprint_output\nbool\nTrue\nif True, prints art ilumina’s CLI output\n\n\n\n\n\n\n\n\nRun a single read simulations\n\nProvide an input file and a seed for the names of the output files\nPrints out the log messages issued by art_illumina\n\n\ninput_fname = 'cov_virus_sequence_one.fa'\noutput_seed = 'single_1seq_150bp'\n\nart.sim_reads(\n    input_file=input_fname,\n    output_seed=output_seed,\n    sim_type=\"single\",\n    read_length=150,\n    fold=100,\n    overwrite=True\n)\n\nreturn code:  0 \n\n\n    ====================ART====================\n             ART_Illumina (2008-2016)          \n          Q Version 2.5.8 (June 6, 2016)       \n     Contact: Weichun Huang &lt;whduke@gmail.com&gt; \n    -------------------------------------------\n\n                  Single-end Simulation\n\nTotal CPU time used: 0.436844\n\nThe random seed for the run: 1738405239\n\nParameters used during run\n    Read Length:    150\n    Genome masking 'N' cutoff frequency:    1 in 150\n    Fold Coverage:            100X\n    Profile Type:             Combined\n    ID Tag:                   \n\nQuality Profile(s)\n    First Read:   HiSeq 2500 Length 150 R1 (built-in profile) \n\nOutput files\n\n  FASTQ Sequence File:\n    /home/vtec/projects/bio/metagentools/nbs-dev/data_dev/ncbi/refsequences/cov/single_1seq_150bp/single_1seq_150bp.fq\n\n  ALN Alignment File:\n    /home/vtec/projects/bio/metagentools/nbs-dev/data_dev/ncbi/refsequences/cov/single_1seq_150bp/single_1seq_150bp.aln\n\n\n\n\nRun a paired read simulations with the input file.\n\nart.print_last_output_file_excerpts()\n\n========================================================================================================================\nFile Name: single_1seq_150bp.fq.\n--------------------------------------------------------------------------------\n@2591237:ncbi:1-20100\nGTACCACAGATGTGCACTTTACGTCAGACATTTTAGACTGTACAGTAGCAACCTTGATACATGGTTTACCTCCAATACCTAACAACTTAATGTTAAGCTTGAAAGCATCAATACTACTCTTAGGAGGCAAAAGCCCCTGGGAGTTCATAT\n+\nCCCGGGGGG1GGCGJJGJJGJJGJGJJJJJJJGJJ=GGJGGJJGJJGCCJGJGGGJGCGCC=GGJCGCGJGJGGCC=GGCGGGGGGGGG8GG=GG8GGCCJGCCCGGCCCGG=CGGGGCGGCGGGGGGGGGGGGGGCGGGGCCGGGGGCG\n@2591237:ncbi:1-20099\nTACACCCTTTGCCAGCTCGCTATGAGCTGTAGCAACGAGTACCTTAAGTTTTTCCATAGGAACACTAAAAGTTGCTGAAAAGGTGTCGACATAAGCATCAAACATCTTAACAGAAACTTCAGTACTATCTCCAACATCTGATACGAGAGC\n+\n=CCG=GGGGGGGGJJGJJJCGJGJJJJJJGGJJJJJJGJJJJCJJGJCGGGJGGGGJGJJG(J=JGGGCG=G=CGGGGG=GGGCG8GGGGGGGC8C=GGCJ8G=CGGGGGGGGG=GGGG=1G8G==GCGGGGGCGGGGGGGGCCGCCCGC\n@2591237:ncbi:1-20098\nATGTCCTGCCTGTCAAGACCCAGAGATTGGACCTGAGCATAGTGTTGCAGATTATCACAACCACTCAAACATTGAAACTCGACTCCGCAAGGGAGGTAGGACTAGATGTTTTGGAGGCTGTGTGTTTGCCTATGTCGGCTGCTATAACAA\n+\nCCC1CGG1GGGGGJJJJCGJJ1JGJGJGJJGJGGJGJGGJJGJGJGJJGCGJCJ=JJGCGGGCJG1CGCGC=GCGGGCGCGG=GGCGGGGCG8GCGGGCGCCG=GCGGGGG(GGGCGGGGG=CGGGCGCGGC8CGGGCGCGCCGGGGGGG\n\n\n\n\ninput_fname = 'cov_virus_sequence_one.fa'\n\nart.sim_reads(\n    input_file=input_fname,\n    output_seed='paired_1seq_150bp',\n    sim_type=\"paired\",\n    read_length=150,\n    fold=100,\n    mean_read=200,\n    std_read=10,\n    overwrite=True\n)\n\nreturn code:  0 \n\n\n    ====================ART====================\n             ART_Illumina (2008-2016)          \n          Q Version 2.5.8 (June 6, 2016)       \n     Contact: Weichun Huang &lt;whduke@gmail.com&gt; \n    -------------------------------------------\n\n                  Paired-end sequencing simulation\n\nTotal CPU time used: 0.437712\n\nThe random seed for the run: 1738405243\n\nParameters used during run\n    Read Length:    150\n    Genome masking 'N' cutoff frequency:    1 in 150\n    Fold Coverage:            100X\n    Mean Fragment Length:     200\n    Standard Deviation:       10\n    Profile Type:             Combined\n    ID Tag:                   \n\nQuality Profile(s)\n    First Read:   HiSeq 2500 Length 150 R1 (built-in profile) \n    First Read:   HiSeq 2500 Length 150 R2 (built-in profile) \n\nOutput files\n\n  FASTQ Sequence Files:\n     the 1st reads: /home/vtec/projects/bio/metagentools/nbs-dev/data_dev/ncbi/refsequences/cov/paired_1seq_150bp/paired_1seq_150bp1.fq\n     the 2nd reads: /home/vtec/projects/bio/metagentools/nbs-dev/data_dev/ncbi/refsequences/cov/paired_1seq_150bp/paired_1seq_150bp2.fq\n\n  ALN Alignment Files:\n     the 1st reads: /home/vtec/projects/bio/metagentools/nbs-dev/data_dev/ncbi/refsequences/cov/paired_1seq_150bp/paired_1seq_150bp1.aln\n     the 2nd reads: /home/vtec/projects/bio/metagentools/nbs-dev/data_dev/ncbi/refsequences/cov/paired_1seq_150bp/paired_1seq_150bp2.aln\n\n\n\n\n\nart.print_last_output_file_excerpts()\n\n========================================================================================================================\nFile Name: paired_1seq_150bp2.fq.\n--------------------------------------------------------------------------------\n@2591237:ncbi:1-20100/2\nTTATAGCAGCCGACATAGGCAAACACACAGCCTCCAAAACATCTAGTCCTACCTCCCTTGCGGAGTCGAGTTTCAATGTTTGAGTGGTTGTGATAATCTGCAACACTATGCTCAGGTCCAATCTCTGGGTCTTGACAGGCAGGACATGGC\n+\nCCCGGGGGCGGGGGJJJJJGJJJJG8JJ=GJJCGGJCJ1GGCJJGCGGJJJJGGCJGGCGJJ=JCGGG=GGG(C=GCCGC=GGGGCGCGGGGGGGGGG=GCGCCJJJCGGGGCCGCGGCCGGCCGGGCC8CGGGCGC=GGGCGCGCCCCC\n@2591237:ncbi:1-20098/2\nATCATTACCGGTCTTCATCCAACACAGGCACCTACACACCTCAGCGTTGACACAAAATTTAAGACTGAGGGACTATGTGTTGACATACCAGGCATACCAAAGGACATGACCTACCGTAGACTCATCTCTATGATGGGTTTTAAAATGAAT\n+\n=CCGG=G1GGGGGCGJJGGJJJJ8JJJJ=JCGJJGJJGJJJJCJJGGGJJGGGGG=CJGCGGGCCJJ8CG8J=CGGGGCCGGGGGCGCCCGGGGGCGGCG=CJCJCJ=C=GGGCGGCGGGG=CGGGGGGCGGGCGGGGGGGCGCGCGGGC\n@2591237:ncbi:1-20096/2\nCGGTACTAGACATACCTATCAGCTTCGTGCAAGATCAGTTTCACCAAAACTTTTCATCAGACAAGAGGAAGTTCACCAAGAGCTCTACTCACCGCTTTTTCTCATTGTTGCTGCTCTAGTATTTATAATACTTTGCTTCACCATTAAGAG\n+\nCCCGGGGGGG=GC1JJJJJJJJ1JJJCGCJJJGCJGJCG(JGGJGJGJGJGGGCCJJCCJJGG8JCGGCGG=G=J8JCGG=8GCCGCGCGGG=C8GCGGG=CJJCJJCGGG1GGG=GG=GCGGC(GCCCGGG=GCGCCGCCGCGCG=GC=\n\n========================================================================================================================\nFile Name: paired_1seq_150bp1.fq.\n--------------------------------------------------------------------------------\n@2591237:ncbi:1-20100/1\nTGAAGGACCTACTACATGTGGGTACCTACCTACTAATGCTGTAGTGAAAATGCCATGTCCTGCCTGTCAAGACCCAGAGATTGGACCTGAGCATAGTGTTGCAGATTATCACAACCACTCAAACATTGAAACTCGACTCCGCAAGGGAGG\n+\nCC1CGGGGGGGGGJJJCGJGGJGJJJJGGCGJ=J1JGJJJJJGGGG1GJJGJGJGJGGCGGGGGJGGJJGGGJCCCGCGCGG=GGGCGGGGGGGGG=CG1JCGGC(GGCCGCC8GGGGGGCGGGCGGCCGCGCCCGCGGCCCGGGGC8GC\n@2591237:ncbi:1-20098/1\nTAGCTTCTTCGCGGGTGATAAACATATTAGCGTAACCATTGACTTGGTAATTCATTTTAAAACCCATCATAGAGATGAGTCTACGGTAGGTCATGTCCTTTGGTATGCCTGGTATGTCAACACATAGTCCCTCAGTTTAAAATTTTGTGT\n+\n1CCGGGGGGCGGGJJJGJGJJGGJJGJGGG(GJJJGJJCCJJJCJGJJGCGJGJGG(GGGGGGCJGGGJGGGCCGGGGGGGGGCGGCGGGGCGGGCG(GGJCCGGGGCGG=1GGCGCG=GCGCGGG=CGGGCCC1G(G(CG=GCGGGCGG\n@2591237:ncbi:1-20096/1\nGAATAGCAGAAAGGCTAAAAAGCACAAATAGAAGTCAATTAAAGTGAGCTCATTCATTCTGTCTTTCTCTTAATGGTGAAGCAAAGTATTATAAATACTAGAGCAGCAACAATGAGAAAAAGCGGTGAGTAGAGCTCTTGGTGAACTTCC\n+\nCCCGGGCGGGGGGJJGJJJJGJGJJGJGJ8JJGGJJCJGG=JJJCJC(JJGJJJGGJJGG1GCGGGJGCCJCGGCJGCJGCGCCCGGCGGGGGGG8GGGGJGGGG1GCCGGGCGGGCCGGCC=G=GGGGCCGG=CG=CGCGGGCGGGGGC\n\n\n\n\nart.list_all_output_files()\n\npaired_1seq_150bp\n- paired_1seq_150bp2.aln\n- paired_1seq_150bp2.fq\n- paired_1seq_150bp1.fq\n- paired_1seq_150bp1.aln\nsingle_1seq_150bp\n- single_1seq_150bp.fq\n- single_1seq_150bp.aln",
    "crumbs": [
      "General Code",
      "art"
    ]
  },
  {
    "objectID": "data_dev/readme.html",
    "href": "data_dev/readme.html",
    "title": "metagentorch",
    "section": "",
    "text": "Data directory for this package development\nThis directory includes all data required to validate and test this package code.\ndata_dev\n |--- CNN_Virus_data\n |     |--- 50mer_ds_100_seq\n |     |--- 150mer_ds_100_seq\n |     |--- train_short\n |     |--- val_short\n |     |--- weight_of_classes\n |--- ncbi\n |     |--- infer_results\n |     |     |--- cnn_virus\n |     |     |--- csv\n |     |     |--- xlsx\n |     |     |--- testdb.db\n |     |--- refsequences\n |     |     |--- cov\n |     |     |     |--cov_virus_sequence_one_metadata.json\n |     |     |     |--sequences_two_no_matching_rule.fa\n |     |     |     |--another_sequence.fa\n |     |     |     |--cov_virus_sequences_two.fa\n |     |     |     |--cov_virus_sequences_two_metadata.json\n |     |     |     |--cov_virus_sequence_one.fa\n |     |     |     |--single_1seq_150bp\n |     |     |     |    |--single_1seq_150bp.fq\n |     |     |     |    |--single_1seq_150bp.aln\n |     |     |     |--paired_1seq_150bp\n |     |     |     |    |--paired_1seq_150bp2.aln\n |     |     |     |    |--paired_1seq_150bp2.fq\n |     |     |     |    |--paired_1seq_150bp1.fq \n |     |     |     |    |--paired_1seq_150bp1.aln \n |     |--- simreads\n |     |     |--- cov\n |     |     |     |--- paired_1seq_50bp\n |     |     |     |      |--- paired_1seq_50bp_1.aln\n |     |     |     |      |--- paired_1seq_50bp_1.fq\n |     |     |     |--- single_1seq_50bp\n |     |     |     |      |--- single_1seq_50bp_1.aln\n |     |     |     |      |--- single_1seq_50bp_1.fq\n |     |     |--- cov\n |     |     |     |--single_1seq_50bp\n |     |     |     |    |--single_1seq_50bp.aln\n |     |     |     |    |--single_1seq_50bp.fq\n |     |     |     |--single_1seq_150bp\n |     |     |     |    |--single_1seq_150bp.fq\n |     |     |     |    |--single_1seq_150bp.aln\n |     |     |     |--paired_1seq_150bp\n |     |     |     |    |--paired_1seq_150bp2.aln\n |     |     |     |    |--paired_1seq_150bp2.fq\n |     |     |     |    |--paired_1seq_150bp1.fq\n |     |     |     |    |--paired_1seq_150bp1.aln\n |--- saved           \n |--- readme.md"
  },
  {
    "objectID": "data_dev/ncbi/readme.html",
    "href": "data_dev/ncbi/readme.html",
    "title": "metagentorch",
    "section": "",
    "text": "NCBI Data\nThis directory includes all data related to the work done with reference sequences from NCBI.\nThe data is organized in the following subfolders:\n\nrefsequences: reference CoV sequences downloaded from NCBI, and related metadata\nsimreads: all data from simulated reads, using ART Illumina simulator and the reference sequences\ninfer_results: results from the inference using models with the simulated reads\nds: datasets in proper format for training or inference/prediction using the CNN Virus model"
  },
  {
    "objectID": "data_dev/ncbi/infer_results/cov-ncbi/readme.html",
    "href": "data_dev/ncbi/infer_results/cov-ncbi/readme.html",
    "title": "stores the test database",
    "section": "",
    "text": "stores the test database"
  },
  {
    "objectID": "cnn_virus_architecture.html",
    "href": "cnn_virus_architecture.html",
    "title": "architecture",
    "section": "",
    "text": "The paper “A multi-task CNN learning model for taxonomic assignment of human viruses” proposes a CNN model applied to virus.\nThe module architecture includes:",
    "crumbs": [
      "CNN Virus",
      "architecture"
    ]
  },
  {
    "objectID": "cnn_virus_architecture.html#original-architecture",
    "href": "cnn_virus_architecture.html#original-architecture",
    "title": "architecture",
    "section": "Original Architecture",
    "text": "Original Architecture\nBelow are the functions to create model with the same architecture as the original paper.\n\nsource\n\ncreate_model_original\n\n create_model_original (load_parameters:bool=True,\n                        path2parameters:pathlib.Path|None=None,\n                        to_gpu:bool=True)\n\nBuild a CNN model as per CNN Virus paper\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nload_parameters\nbool\nTrue\nLoad pretrained weights when True\n\n\npath2parameters\npathlib.Path | None\nNone\nPath to pretrained weights, defaults to project CNN Virus weights\n\n\nto_gpu\nbool\nTrue\nMove model to GPU if possible when True\n\n\nReturns\nModel\n\nNew instance of an original paper architecture\n\n\n\n\nProjectFileSystem().data\n\nPath('/home/vtec/projects/bio/metagentorch/data')\n\n\nThis is the original paper’s model, taking 50-mer sequences and predicting:\n\nthe logits for 187 viruses\nthe logits for one of 10 regions in the original virus genome\n\n\nmodel = create_model_original(load_parameters=False)\n\nCreating CNN Model (Original)\nCreated randomly initialized model\n\n\n\nmodel.summary()\n\nModel: \"CNN_Virus\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input-seq           │ (None, 50, 5)     │          0 │ -                 │\n│ (InputLayer)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv-1 (Conv1D)     │ (None, 50, 512)   │     13,312 │ input-seq[0][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bn-1                │ (None, 50, 512)   │      2,048 │ conv-1[0][0]      │\n│ (BatchNormalizatio… │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ maxpool-1           │ (None, 25, 512)   │          0 │ bn-1[0][0]        │\n│ (MaxPooling1D)      │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv-2 (Conv1D)     │ (None, 25, 512)   │  1,311,232 │ maxpool-1[0][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bn-2                │ (None, 25, 512)   │      2,048 │ conv-2[0][0]      │\n│ (BatchNormalizatio… │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ maxpool-2           │ (None, 13, 512)   │          0 │ bn-2[0][0]        │\n│ (MaxPooling1D)      │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv-3 (Conv1D)     │ (None, 13, 1024)  │  3,671,040 │ maxpool-2[0][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv-4 (Conv1D)     │ (None, 13, 1024)  │  7,341,056 │ conv-3[0][0]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bn-3                │ (None, 13, 1024)  │      4,096 │ conv-4[0][0]      │\n│ (BatchNormalizatio… │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ maxpool-3           │ (None, 7, 1024)   │          0 │ bn-3[0][0]        │\n│ (MaxPooling1D)      │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ flatten (Flatten)   │ (None, 7168)      │          0 │ maxpool-3[0][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense-1 (Dense)     │ (None, 1024)      │  7,341,056 │ flatten[0][0]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bn-4                │ (None, 1024)      │      4,096 │ dense-1[0][0]     │\n│ (BatchNormalizatio… │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ do-1 (Dropout)      │ (None, 1024)      │          0 │ bn-4[0][0]        │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ labels (Dense)      │ (None, 187)       │    191,675 │ do-1[0][0]        │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concat              │ (None, 1211)      │          0 │ do-1[0][0],       │\n│ (Concatenate)       │                   │            │ labels[0][0]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense-2 (Dense)     │ (None, 1024)      │  1,241,088 │ concat[0][0]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bn-5                │ (None, 1024)      │      4,096 │ dense-2[0][0]     │\n│ (BatchNormalizatio… │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ pos (Dense)         │ (None, 10)        │     10,250 │ bn-5[0][0]        │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n\n\n\n Total params: 21,137,093 (80.63 MB)\n\n\n\n Trainable params: 21,128,901 (80.60 MB)\n\n\n\n Non-trainable params: 8,192 (32.00 KB)\n\n\n\n\nmodel = create_model_original(load_parameters=True)\n\nCreating CNN Model (Original)\nLoading parameters from pretrained_model.h5\nCreated pretrained model",
    "crumbs": [
      "CNN Virus",
      "architecture"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "This module includes all base classes, functions and other objects that are used across the package. It is imported by all other modules in the package.\ncore includes utility classes and functions to make it easier to work with the complex file systems adopted for the project, as well as base classes such as a file reader with additional functionality.",
    "crumbs": [
      "General Code",
      "core"
    ]
  },
  {
    "objectID": "core.html#handling-files-and-file-structure",
    "href": "core.html#handling-files-and-file-structure",
    "title": "core",
    "section": "Handling files and file structure",
    "text": "Handling files and file structure\nUtility classes to represent\n\nsource\n\nProjectFileSystem\n\n ProjectFileSystem (*args, **kwargs)\n\n*Represent a project file system, return paths to key directories, provide methods to manage the file system.\n\nPaths to key directories are based on whether the code is running locally or in the cloud.\nFirst time it is used on a local computer, it must be registered as local and a project root path must be set.\nA user configuration file is created in the user’s home directory to store the project root path and whether the machine is local or not.\n\n\nTechnical note: ProjectFileSystem is a simpleton class*\n\nReference Project File System:\nThis project adopts a unified file structure to make coding and colaboration easier. In addition, we can run the code locally (from a project-root directory) or in the cloud (colab, kaggle, others).\nThe unified file structure when running localy is:\n    project-root   \n        |--- data\n        |      |--- CNN_Virus_data  (all data from CNN Virus original paper)\n        |      |--- saved           (trained and finetuned models, saved preprocessed datasets)\n        |      |--- ....            (raw or pre-processed data from various sources, results, ... )  \n        |      \n        |--- nbs  (all reference and work notebooks)\n        |      |--- cnn_virus\n        |      |        |--- notebooks.ipynb\nWhen running on google colab, it is assumed that a google drive is mounted on the colab server instance, and that this google drive root includes a shortcut named Metagenomics and pointing to the project shared directory. The project shared directory is accessible here if you are an authorized project member.\nProjectFileSystem at work:\nIf you use this class for the first time on a local computer, read the two Important Notes below.\n\npfs = ProjectFileSystem()\n\nOnce created, the instance of ProjectFileSystem gives access to key directories’ paths:\n\nproject root: Path to the project root directory\ndata: Path to the data directory\nnbs: Path to the notebooks directory\n\nIt also provides additional information regarding the computer on which the code is running:\n\nos: a string providing the name of the operating system the code is running on\nis_colab: True if the code is running on google colab\nis_kaggle: True if the code is running on kaggle server (NOT IMPLEMENTED YET)\nis_local: True if the code is running on a computer registered as local\n\n\nfor p in [pfs.project_root, pfs.data, pfs.nbs]:\n    print(p)\n\n/home/vtec/projects/bio/metagentorch\n/home/vtec/projects/bio/metagentorch/data\n/home/vtec/projects/bio/metagentorch/nbs\n\n\n\nprint(f\"Operating System: {pfs.os}\")\nprint(f\"Local Computer: {pfs.is_local}, Colab: {pfs.is_colab}, Kaggle: {pfs.is_kaggle}\")\n\nOperating System: linux\nLocal Computer: True, Colab: False, Kaggle: False\n\n\n\nsource\n\n\nProjectFileSystem.info\n\n ProjectFileSystem.info ()\n\nPrint basic info on the file system and the device\n\npfs.info()\n\nRunning linux on local computer\nDevice's home directory: /home/vtec\nProject file structure:\n - Root ........ /home/vtec/projects/bio/metagentorch \n - Data Dir .... /home/vtec/projects/bio/metagentorch/data \n - Notebooks ... /home/vtec/projects/bio/metagentorch/nbs\n\n\n\nsource\n\n\nProjectFileSystem.readme\n\n ProjectFileSystem.readme (dir_path:pathlib.Path|None=None)\n\n*Display readme.md file or any other .md file in dir_path.\nThis provides a convenient way to get information on each direcotry content*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndir_path\npathlib.Path | None\nNone\nPath to the directory to inquire. If None, display readme file from project_root.\n\n\nReturns\nNone\n\n\n\n\n\n\npfs.readme(Path('data_dev'))\n\n\n\n\nReadMe file for directory /home/vtec/projects/bio/metagentorch/nbs-dev/data_dev:\n\n\n\n\n\nData directory for this package development\nThis directory includes all data required to validate and test this package code.\ndata_dev\n |--- CNN_Virus_data\n |     |--- 50mer_ds_100_seq\n |     |--- 150mer_ds_100_seq\n |     |--- train_short\n |     |--- val_short\n |     |--- weight_of_classes\n |--- ncbi\n |     |--- infer_results\n |     |     |--- cnn_virus\n |     |     |--- csv\n |     |     |--- xlsx\n |     |     |--- testdb.db\n |     |--- refsequences\n |     |     |--- cov\n |     |     |     |--cov_virus_sequence_one_metadata.json\n |     |     |     |--sequences_two_no_matching_rule.fa\n |     |     |     |--another_sequence.fa\n |     |     |     |--cov_virus_sequences_two.fa\n |     |     |     |--cov_virus_sequences_two_metadata.json\n |     |     |     |--cov_virus_sequence_one.fa\n |     |     |     |--single_1seq_150bp\n |     |     |     |    |--single_1seq_150bp.fq\n |     |     |     |    |--single_1seq_150bp.aln\n |     |     |     |--paired_1seq_150bp\n |     |     |     |    |--paired_1seq_150bp2.aln\n |     |     |     |    |--paired_1seq_150bp2.fq\n |     |     |     |    |--paired_1seq_150bp1.fq \n |     |     |     |    |--paired_1seq_150bp1.aln \n |     |--- simreads\n |     |     |--- cov\n |     |     |     |--- paired_1seq_50bp\n |     |     |     |      |--- paired_1seq_50bp_1.aln\n |     |     |     |      |--- paired_1seq_50bp_1.fq\n |     |     |     |--- single_1seq_50bp\n |     |     |     |      |--- single_1seq_50bp_1.aln\n |     |     |     |      |--- single_1seq_50bp_1.fq\n |     |     |--- cov\n |     |     |     |--single_1seq_50bp\n |     |     |     |    |--single_1seq_50bp.aln\n |     |     |     |    |--single_1seq_50bp.fq\n |     |     |     |--single_1seq_150bp\n |     |     |     |    |--single_1seq_150bp.fq\n |     |     |     |    |--single_1seq_150bp.aln\n |     |     |     |--paired_1seq_150bp\n |     |     |     |    |--paired_1seq_150bp2.aln\n |     |     |     |    |--paired_1seq_150bp2.fq\n |     |     |     |    |--paired_1seq_150bp1.fq\n |     |     |     |    |--paired_1seq_150bp1.aln\n |--- saved           \n |--- readme.md               \n\n\n\n\n\n\nImportant Note 1:\nWhen using the package on a local computer for the first time, you must register the computer as a local computer. Otherwise, ProjectFileSystem will raise an error. Once registered, the configuration file will be updated and ProjectFileSystem will detect that and run without error.\n\n\nsource\n\n\nProjectFileSystem.register_as_local\n\n ProjectFileSystem.register_as_local ()\n\nUpdate the configuration file to register the machine as local machine\n\ncfg = pfs.register_as_local()\n\n\nImportant Note 2:\nWhen using the package on a local computer for the first time, it is also required to set the project root directory. This is necessary to allow users to locate their local project folder anywhere they want. Once set, the path to the project root will be saved in the configuratin file.\n\n\nsource\n\n\nProjectFileSystem.set_project_root\n\n ProjectFileSystem.set_project_root (p2project:str|pathlib.Path,\n                                     data_dir:str='data')\n\nUpdate the configuration file to set the project root\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np2project\nstr | pathlib.Path\n\nstring or Path to the project directory. Can be absolute or relative to home\n\n\ndata_dir\nstr\ndata\nDirectory name for data under project root\n\n\nReturns\nConfigParser\n\n\n\n\n\n\npfs.set_project_root('/home/vtec/projects/bio/metagentorch/');\n\nProject Root set to:   /home/vtec/projects/bio/metagentorch\nData directory set to: /home/vtec/projects/bio/metagentorch/data\n\n\n\nsource\n\n\nProjectFileSystem.read_config\n\n ProjectFileSystem.read_config ()\n\nRead config from the configuration file if it exists and return an empty config if does not\n\ncfg = pfs.read_config()\ncfg['Infra']['registered_as_local']\n\n'True'\n\n\n\ncfg['Infra']['project_root']\n\n'/home/vtec/projects/bio/metagentorch'\n\n\n\ncfg['Infra']['data_dir']\n\n'data'\n\n\nTechnical Note for Developpers\nThe current notebook and all other development notebooks use a minimum set of data that comes with the repository under nbs-dev/data_dev instead of the standard data directory which is much too large for testing and developing.\nTherefore, when creating the instance of ProjectFileSystem, use the parameter config_file to pass a specific development configuration, also coming with the repository.\n\np2dev_cfg = PACKAGE_ROOT / 'nbs-dev/metagentorch-dev.cfg'\npfs = ProjectFileSystem(config_fname=p2dev_cfg)\npfs.info()\n\nRunning linux on local computer\nDevice's home directory: /home/vtec\nProject file structure:\n - Root ........ /home/vtec/projects/bio/metagentorch \n - Data Dir .... /home/vtec/projects/bio/metagentorch/nbs-dev/data_dev \n - Notebooks ... /home/vtec/projects/bio/metagentorch/nbs",
    "crumbs": [
      "General Code",
      "core"
    ]
  },
  {
    "objectID": "core.html#sqlite-database-helper-class",
    "href": "core.html#sqlite-database-helper-class",
    "title": "core",
    "section": "SQlite Database Helper Class",
    "text": "SQlite Database Helper Class\n\nsource\n\nSqliteDatabase\n\n SqliteDatabase (p2db:pathlib.Path)\n\n*Manage a SQLite db file, execute SQL queries, return results, provide context manager functionality.\nExample usage as a context manager\ndb_path = Path('your_database.db')\ndb = SqliteDb(db_path)\n\nwith db as database:\n    result = database.get_result(\"SELECT * FROM your_table\")\n    print(result)\n```*\n\n\n::: {#cell-31 .cell execution_count=26}\n``` {.python .cell-code}\np2db = pfs.data / 'ncbi/infer_results/cov-ncbi/testdb.db'\ndb = SqliteDatabase(p2db)\n:::\n\ndb.print_schema()\n\npredictions (table)\n columns: id, readid, refseqid, refsource, refseq_strand, taxonomyid, lbl_true, lbl_pred, pos_true, pos_pred, top_5_lbl_pred_0, top_5_lbl_pred_1, top_5_lbl_pred_2, top_5_lbl_pred_3, top_5_lbl_pred_4\n index: idx_preds\n   indexed columns: readid, refseqid, pos_true\n\nlabel_probabilities (table)\n columns: id, read_kmer_id, read_50mer_nb, prob_000, prob_001, prob_002, prob_003, prob_004, prob_005, prob_006, prob_007, prob_008, prob_009, prob_010, prob_011, prob_012, prob_013, prob_014, prob_015, prob_016, prob_017, prob_018, prob_019, prob_020, prob_021, prob_022, prob_023, prob_024, prob_025, prob_026, prob_027, prob_028, prob_029, prob_030, prob_031, prob_032, prob_033, prob_034, prob_035, prob_036, prob_037, prob_038, prob_039, prob_040, prob_041, prob_042, prob_043, prob_044, prob_045, prob_046, prob_047, prob_048, prob_049, prob_050, prob_051, prob_052, prob_053, prob_054, prob_055, prob_056, prob_057, prob_058, prob_059, prob_060, prob_061, prob_062, prob_063, prob_064, prob_065, prob_066, prob_067, prob_068, prob_069, prob_070, prob_071, prob_072, prob_073, prob_074, prob_075, prob_076, prob_077, prob_078, prob_079, prob_080, prob_081, prob_082, prob_083, prob_084, prob_085, prob_086, prob_087, prob_088, prob_089, prob_090, prob_091, prob_092, prob_093, prob_094, prob_095, prob_096, prob_097, prob_098, prob_099, prob_100, prob_101, prob_102, prob_103, prob_104, prob_105, prob_106, prob_107, prob_108, prob_109, prob_110, prob_111, prob_112, prob_113, prob_114, prob_115, prob_116, prob_117, prob_118, prob_119, prob_120, prob_121, prob_122, prob_123, prob_124, prob_125, prob_126, prob_127, prob_128, prob_129, prob_130, prob_131, prob_132, prob_133, prob_134, prob_135, prob_136, prob_137, prob_138, prob_139, prob_140, prob_141, prob_142, prob_143, prob_144, prob_145, prob_146, prob_147, prob_148, prob_149, prob_150, prob_151, prob_152, prob_153, prob_154, prob_155, prob_156, prob_157, prob_158, prob_159, prob_160, prob_161, prob_162, prob_163, prob_164, prob_165, prob_166, prob_167, prob_168, prob_169, prob_170, prob_171, prob_172, prob_173, prob_174, prob_175, prob_176, prob_177, prob_178, prob_179, prob_180, prob_181, prob_182, prob_183, prob_184, prob_185, prob_186\n index: idx_probs\n   indexed columns: read_kmer_id, read_50mer_nb\n\npreds_probs (view)\n columns: refseqid,lbl_true,lbl_pred,pos_true,pos_pred,top_5_lbl_pred_0,top_5_lbl_pred_1,top_5_lbl_pred_2,top_5_lbl_pred_3,top_5_lbl_pred_4,top_5_lbl_pred_0:1,top_5_lbl_pred_1:1,top_5_lbl_pred_2:1,top_5_lbl_pred_3:1,top_5_lbl_pred_4:1,top_5_lbl_pred_0:2,top_5_lbl_pred_1:2,top_5_lbl_pred_2:2,top_5_lbl_pred_3:2,top_5_lbl_pred_4:2,top_5_lbl_pred_0:3,top_5_lbl_pred_1:3,top_5_lbl_pred_2:3,top_5_lbl_pred_3:3,top_5_lbl_pred_4:3,top_5_lbl_pred_0:4,top_5_lbl_pred_1:4,top_5_lbl_pred_2:4,top_5_lbl_pred_3:4,top_5_lbl_pred_4:4,read_kmer_id,read_50mer_nb,prob_000,prob_001,prob_002,prob_003,prob_004,prob_005,prob_006,prob_007,prob_008,prob_009,prob_010,prob_011,prob_012,prob_013,prob_014,prob_015,prob_016,prob_017,prob_018,prob_019,prob_020,prob_021,prob_022,prob_023,prob_024,prob_025,prob_026,prob_027,prob_028,prob_029,prob_030,prob_031,prob_032,prob_033,prob_034,prob_035,prob_036,prob_037,prob_038,prob_039,prob_040,prob_041,prob_042,prob_043,prob_044,prob_045,prob_046,prob_047,prob_048,prob_049,prob_050,prob_051,prob_052,prob_053,prob_054,prob_055,prob_056,prob_057,prob_058,prob_059,prob_060,prob_061,prob_062,prob_063,prob_064,prob_065,prob_066,prob_067,prob_068,prob_069,prob_070,prob_071,prob_072,prob_073,prob_074,prob_075,prob_076,prob_077,prob_078,prob_079,prob_080,prob_081,prob_082,prob_083,prob_084,prob_085,prob_086,prob_087,prob_088,prob_089,prob_090,prob_091,prob_092,prob_093,prob_094,prob_095,prob_096,prob_097,prob_098,prob_099,prob_100,prob_101,prob_102,prob_103,prob_104,prob_105,prob_106,prob_107,prob_108,prob_109,prob_110,prob_111,prob_112,prob_113,prob_114,prob_115,prob_116,prob_117,prob_118,prob_119,prob_120,prob_121,prob_122,prob_123,prob_124,prob_125,prob_126,prob_127,prob_128,prob_129,prob_130,prob_131,prob_132,prob_133,prob_134,prob_135,prob_136,prob_137,prob_138,prob_139,prob_140,prob_141,prob_142,prob_143,prob_144,prob_145,prob_146,prob_147,prob_148,prob_149,prob_150,prob_151,prob_152,prob_153,prob_154,prob_155,prob_156,prob_157,prob_158,prob_159,prob_160,prob_161,prob_162,prob_163,prob_164,prob_165,prob_166,prob_167,prob_168,prob_169,prob_170,prob_171,prob_172,prob_173,prob_174,prob_175,prob_176,prob_177,prob_178,prob_179,prob_180,prob_181,prob_182,prob_183,prob_184,prob_185,prob_186",
    "crumbs": [
      "General Code",
      "core"
    ]
  },
  {
    "objectID": "core.html#other-utility-classes",
    "href": "core.html#other-utility-classes",
    "title": "core",
    "section": "Other utility classes",
    "text": "Other utility classes\n\nsource\n\nJsonDict\n\n JsonDict (p2json:str|pathlib.Path, dictionary:dict|None=None)\n\n*Dictionary whose current value is mirrored in a json file and can be initated from a json file\nJsonDict requires a path to json file at creation. An optional dict can be passed as argument.\nBehavior at creation:\n\nJsonDict(p2json, dict) will create a JsonDict with key-values from dict, and mirrored in p2json\nJsonDict(p2json) will create a JsonDict with empty dictionary and load json content if file exists\n\nOnce created, JsonDict instances behave exactly as a dictionary*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np2json\nstr | pathlib.Path\n\npath to the json file to mirror with the dictionary\n\n\ndictionary\ndict | None\nNone\noptional dictionary to initialize the JsonDict\n\n\n\nCreate a new dictionary mirrored to a JSON file:\n\nd = {'a': 1, 'b': 2, 'c': 3}\np2json = pfs.data / 'jsondict-test.json'\njsondict = JsonDict(p2json, d)\njsondict\n\ndict mirrored in /home/vtec/projects/bio/metagentorch/nbs-dev/data_dev/jsondict-test.json\n{'a': 1, 'b': 2, 'c': 3}\n\n\nOnce created, the JsonFile instance behaves exactly like a dictionary, with the added benefit that any change to the dictionary is automatically saved to the JSON file.\n\njsondict['a'], jsondict['b'], jsondict['c']\n\n(1, 2, 3)\n\n\n\nfor k, v in jsondict.items():\n    print(f\"key: {k}; value: {v}\")\n\nkey: a; value: 1\nkey: b; value: 2\nkey: c; value: 3\n\n\nAdding or removing a value from the dictionary works in the same way as for a normal dictionary. But the json file is automatically updated.\n\njsondict['d'] = 4\njsondict\n\ndict mirrored in /home/vtec/projects/bio/metagentorch/nbs-dev/data_dev/jsondict-test.json\n{'a': 1, 'b': 2, 'c': 3, 'd': 4}\n\n\n\nwith open(p2json, 'r') as fp:\n    print(fp.read())\n\n{\n    \"a\": 1,\n    \"b\": 2,\n    \"c\": 3,\n    \"d\": 4\n}\n\n\n\ndel jsondict['a']\njsondict\n\ndict mirrored in /home/vtec/projects/bio/metagentorch/nbs-dev/data_dev/jsondict-test.json\n{'b': 2, 'c': 3, 'd': 4}\n\n\n\nwith open(p2json, 'r') as fp:\n    print(fp.read())\n\n{\n    \"b\": 2,\n    \"c\": 3,\n    \"d\": 4\n}\n\n\n\nsource\n\n\nJsonFileReader\n\n JsonFileReader (path:str|pathlib.Path)\n\nMirror a JSON file and a dictionary\n\n\n\n\nType\nDetails\n\n\n\n\npath\nstr | pathlib.Path\npath to the json file\n\n\n\n\njd = JsonFileReader(pfs.data / 'test.json')\npprint(jd.d)\n\n{'item 1': {'keys': 'key key key key', 'pattern': 'pattern 1'},\n 'item 2': {'keys': 'key key key key', 'pattern': 'pattern 2'},\n 'item 3': {'keys': 'key key key key', 'pattern': 'pattern 3'}}\n\n\nNow we can add an item to the dictionary/json\n\nnew_item = {'keys': 'key key key key', 'pattern': 'another pattern'}\njd.add_item(key='another item', item=new_item)\n\n{'item 1': {'keys': 'key key key key', 'pattern': 'pattern 1'},\n 'item 2': {'keys': 'key key key key', 'pattern': 'pattern 2'},\n 'item 3': {'keys': 'key key key key', 'pattern': 'pattern 3'},\n 'another item': {'keys': 'key key key key', 'pattern': 'another pattern'}}\n\n\nAfter saving the updated JSON file, we can load it again and see the changes.\n\njd.save_to_file()\n\n\njd = JsonFileReader(pfs.data / 'test.json')\npprint(jd.d)\n\n{'another item': {'keys': 'key key key key', 'pattern': 'another pattern'},\n 'item 1': {'keys': 'key key key key', 'pattern': 'pattern 1'},\n 'item 2': {'keys': 'key key key key', 'pattern': 'pattern 2'},\n 'item 3': {'keys': 'key key key key', 'pattern': 'pattern 3'}}",
    "crumbs": [
      "General Code",
      "core"
    ]
  },
  {
    "objectID": "core.html#other-utility-functions",
    "href": "core.html#other-utility-functions",
    "title": "core",
    "section": "Other utility functions",
    "text": "Other utility functions\n\nsource\n\nlist_available_devices\n\n list_available_devices ()",
    "crumbs": [
      "General Code",
      "core"
    ]
  },
  {
    "objectID": "core.html#file-readers",
    "href": "core.html#file-readers",
    "title": "core",
    "section": "File Readers",
    "text": "File Readers\nBase classes to be extended in order to create readers for specific file formats.\n\nsource\n\nTextFileBaseReader\n\n TextFileBaseReader (path:str|pathlib.Path, nlines:int=1)\n\n*Iterator going through a text file by chunks of nlines lines. Iterator can be reset to file start.\nThe class is mainly intented to be extended, as it is for handling sequence files of various formats such as FastaFileReader.*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr | pathlib.Path\n\npath to the file\n\n\nnlines\nint\n1\nnumber of lines on one chunk\n\n\n\nOnce initialized, the iterator runs over each chunk of line(s) in the text file, sequentially.\n\npfs.data\n\nPath('/home/vtec/projects/bio/metagentorch/nbs-dev/data_dev')\n\n\n\np2textfile = pfs.data / 'CNN_Virus_data/train_short'\nit = TextFileBaseReader(path=p2textfile, nlines=3)\n\none_iteration = next(it)\n\nprint(one_iteration)\n\nTCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC  76  0\nATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG  4   9\nAATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA  181 0\n\n\n\nLet’s create a new instance of the file reader, and get several iterations.\n\nit = TextFileBaseReader(path=p2textfile, nlines=3)\n\none_iteration = next(it)\nprint(one_iteration)\n\nTCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC  76  0\nATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG  4   9\nAATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA  181 0\n\n\n\n\nanother_iteration = next(it)\nprint(another_iteration)\none_more_iteration = next(it)\nprint(one_more_iteration)\n\nGGAGCGGAGCCAACCCCTATGCTCACTTGCAACCCAAGGGGCGTTCCAGT  74  3\nTGGATCCTGCGCGGGACGTCCTTTGTCTACGTCCCGTCGGCGCATCCCGC  60  3\nGAGAGACTTACTAAAAAGCTGGCACTTACCATCAGTGTTTCACCTACATG  44  0\n\nACACACGACACTAGAGATAATGTGTCAGTGGATTATAAACAAACCAAGTT  43  7\nTTGTAGCATAAGAACTGGTCTTCGCTGAAATTCTTGTCTTGATCTCATCT  35  2\nTGGCCCTGCGGTCTGGGGCCCAGAAGCATATGTCAAGTCCTTTGAGAAGT  73  4\n\n\n\nIf we want to access the start of the file again, we need to re-initialize the file handle.\n\nsource\n\n\nTextFileBaseReader.reset_iterator\n\n TextFileBaseReader.reset_iterator ()\n\nReset the iterator to point to the first line in the file.\n\nit.reset_iterator()\none_iteration = next(it)\nprint(one_iteration)\nanother_iteration = next(it)\nprint(another_iteration)\n\nTCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC  76  0\nATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG  4   9\nAATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA  181 0\n\nGGAGCGGAGCCAACCCCTATGCTCACTTGCAACCCAAGGGGCGTTCCAGT  74  3\nTGGATCCTGCGCGGGACGTCCTTTGTCTACGTCCCGTCGGCGCATCCCGC  60  3\nGAGAGACTTACTAAAAAGCTGGCACTTACCATCAGTGTTTCACCTACATG  44  0\n\n\n\n\nsource\n\n\nTextFileBaseReader.print_first_chunks\n\n TextFileBaseReader.print_first_chunks (nchunks:int=3)\n\n*Print the first nchunk chunks of text from the file.\nAfter printing, the iterator is reset again to its first line.*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnchunks\nint\n3\nnumber of chunks to print\n\n\nReturns\nNone\n\n\n\n\n\n\nit = TextFileBaseReader(path=p2textfile, nlines=3)\n\nit.print_first_chunks(nchunks=3)\n\n3-line chunk 1\nTCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC  76  0\nATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG  4   9\nAATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA  181 0\n\n3-line chunk 2\nGGAGCGGAGCCAACCCCTATGCTCACTTGCAACCCAAGGGGCGTTCCAGT  74  3\nTGGATCCTGCGCGGGACGTCCTTTGTCTACGTCCCGTCGGCGCATCCCGC  60  3\nGAGAGACTTACTAAAAAGCTGGCACTTACCATCAGTGTTTCACCTACATG  44  0\n\n3-line chunk 3\nACACACGACACTAGAGATAATGTGTCAGTGGATTATAAACAAACCAAGTT  43  7\nTTGTAGCATAAGAACTGGTCTTCGCTGAAATTCTTGTCTTGATCTCATCT  35  2\nTGGCCCTGCGGTCTGGGGCCCAGAAGCATATGTCAAGTCCTTTGAGAAGT  73  4\n\n\n\n\nsource\n\n\nTextFileBaseReader.parse_text\n\n TextFileBaseReader.parse_text (txt:str, pattern:str|None=None)\n\nParse text using regex pattern with groups. Return a metadata dictionary.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntxt\nstr\n\ntext to parse\n\n\npattern\nstr | None\nNone\nIf None, uses standard regex pattern to extract metadata, otherwise, uses passed regex\n\n\nReturns\ndict\n\nparsed metadata in key/value format\n\n\n\n\ntext = '&gt;2591237:ncbi:1'\npattern = r\"^&gt;(?P&lt;id&gt;\\d+):(?P&lt;source&gt;ncbi):(?P&lt;nb&gt;\\d*)\"\np = re.compile(pattern)\nkeys = p.groupindex.keys()\n\nit.parse_text(text, pattern, keys)\n\n{'id': '2591237', 'nb': '1', 'source': 'ncbi'}",
    "crumbs": [
      "General Code",
      "core"
    ]
  },
  {
    "objectID": "core.html#extending-the-base-class",
    "href": "core.html#extending-the-base-class",
    "title": "core",
    "section": "Extending the base class",
    "text": "Extending the base class\nTextFileBaseReader is a base class, intended to be extended into specific file format readers.\nThe following methods will typically be extended to match data file and other structured text files formats:\n\n__next__ method in order to customize how the iterator parses files into “elements”. For instance, in a FASTA file, one element consists of two lines: a “definition line” and the sequence itself. Extending TextFileBaseReader allows to read pairs of lines sequentially and return an element as a dictionary. For instance, FastaFileReader iterates over each pairs of lines in a Fasta file and return each pair as a dictionary as follows:\n\n    {\n    'definition line': '&gt;2591237:ncbi:1 [MK211378]\\t2591237\\tncbi\\t1 [MK211378] '\n                       '2591237\\tCoronavirus BtRs-BetaCoV/YN2018D\\t\\tscientific '\n                       'name\\n',\n    'sequence':        'TATTAGGTTTTCTACCTACCCAGGA'\n    }\n\nMethods for parsing metadata from the file. For instance, parse_file method will handle how the reader will iterate over the full file and return a dictionary for the entire file.\nExtended classes will also define a specific attributes (text_to_parse_key, re_pattern, re_keys, …)\n\n\nsource\n\nTextFileBaseReader.set_parsing_rules\n\n TextFileBaseReader.set_parsing_rules (pattern:str|None=None,\n                                       verbose:bool=False)\n\n*Set the standard regex parsing rule for the file.\nRules can be set:\n\nmanually by passing specific custom values for pattern and keys\nautomatically, by testing all parsing rules saved in parsing_rule.json\n\nAutomatic selection of parsing rules works by testing each rule saved in parsing_rule.json on the first definition line of the fasta file, and selecting the one rule that generates the most metadata matches.\nRules consists of two parameters:\n\nThe regex pattern including one group for each metadata item, e.g (?P&lt;group_name&gt;regex_code)\nThe list of keys, i.e. the list with the name of each regex groups, used as key in the metadata dictionary\n\nThis method updates the three following class attributes: re_rule_name, re_pattern, re_keys*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npattern\nstr | None\nNone\nregex pattern to apply to parse the text, search in parsing rules json if None\n\n\nverbose\nbool\nFalse\nwhen True, provides information on each rule\n\n\nReturns\nNone\n\n\n\n\n\n\nImportant Note to Developpers\nMethod set_parsing_rules is there to allow TextFileBaseReader’s descendant classes to automatically select parsing rule by applying rules saved in a json file to a string extracted from the first element in the file.\nIt assumes that the iterator returns its elements as dictionaries {section_name:section, ...} and not as a pure string. The key self.text_to_parse_key will then be used to extract the text to parse for testing the rules. The base class iterator returns a simple string and self.text_to_parse_key is set to None.\nTo make setting up a default parsing rule for the reader instance, the iterator must return a dictionary and self.text_to_parse_key must be set to the key in the dictionary corresponding the the text to parse.\nSee implementation in FastaFileReader.\nCalling set_parsing_rules on a class that does not satisfy with these characteristics will do nothing and return a warning.\n\n\nit.set_parsing_rules()\n\n/tmp/ipykernel_7689/2017328125.py:164: UserWarning: \n            `text_to_parse_key` is not defined in this class. \n            It is not possible to set a parsing rule.\n            \n  warnings.warn(msg, category=UserWarning)",
    "crumbs": [
      "General Code",
      "core"
    ]
  },
  {
    "objectID": "cnn_virus_utils.html",
    "href": "cnn_virus_utils.html",
    "title": "utils",
    "section": "",
    "text": "source\n\nsetup_nb\n\n setup_nb (_dev=False)\n\nSets up colab or local environment and corresponding paths to data root directory and cnn virus data\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\n_dev\nbool\nFalse\n\n\n\nReturns\ntuple\n\n(colab, path to data root, path to data)\n\n\n\n\nON_COLAB, p2dataroot, p2data = setup_nb(_dev=True)\nprint('Notebook running on Colab?', ON_COLAB)\nprint(\"Path to data root:        \", p2dataroot.absolute())\nprint(\"Path to cnn virus data    \", p2data.absolute())\n\nRunning locally\nNotebook running on Colab? False\nPath to data root:         /home/vtec/projects/bio/metagentorch/nbs-dev/data_dev\nPath to cnn virus data     /home/vtec/projects/bio/metagentorch/nbs-dev/data_dev\n\n\n\nassert not ON_COLAB, 'ON_COLAB should be on False'\nassert p2dataroot.name == 'data_dev', f\"Should be 'data_dev` and not {p2dataroot.name}\"\nassert p2data.name == 'data_dev', f\"Should be 'data_dev` and not {p2data.name}\"\n\n\nsource\n\n\nupdate_dev_cfg_file\n\n update_dev_cfg_file ()\n\nUpdate the metagentorch configuration file used for development, to point to the correct package root and data directory\n\nupdate_dev_cfg_file()",
    "crumbs": [
      "CNN Virus",
      "utils"
    ]
  }
]