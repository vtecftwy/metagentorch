{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Base classes, functions and other objects used across the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from eccore.ipython import nb_setup\n",
    "from eccore.core import path_to_parent_dir\n",
    "from fastcore.test import test_fail\n",
    "from nbdev import nbdev_export, show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set autoreload mode\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "nb_setup()\n",
    "NBS_ROOT = path_to_parent_dir('nbs')\n",
    "assert NBS_ROOT.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sqlite3\n",
    "import sys\n",
    "import torch\n",
    "import warnings\n",
    "from configparser import ConfigParser\n",
    "from eccore.core import validate_path, safe_path\n",
    "from IPython.display import display, Markdown, HTML\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from sqlite3 import Connection, Cursor\n",
    "from typing import Any, Optional, Literal\n",
    "\n",
    "try: from google.colab import drive\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Retrieve the package root\n",
    "from metagentorch import __file__\n",
    "CODE_ROOT = Path(__file__).parents[0]\n",
    "PACKAGE_ROOT = Path(__file__).parents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module includes all base classes, functions and other objects that are used across the package. It is imported by all other modules in the package.\n",
    "\n",
    "`core` includes utility classes and functions to make it easier to work with the complex file systems adopted for the project, as well as base classes such as a file reader with additional functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Classes and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling files and file structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility classes to represent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ProjectFileSystem:\n",
    "    \"\"\"Represent a project file system, return paths to key directories, provide methods to manage the file system.\n",
    "\n",
    "    - Paths to key directories are based on whether the code is running locally or in the cloud.\n",
    "    - First time it is used on a local computer, it must be registered as local and a project root path must be set.\n",
    "    - A user configuration file is created in the user's home directory to store the project root path and whether the machine is local or not.\n",
    "\n",
    "    > Technical note: `ProjectFileSystem` is a simpleton class\n",
    "    \"\"\"\n",
    "\n",
    "    _instance = None\n",
    "    _config_dir = '.metagentorch'\n",
    "    _config_fname = 'metagentorch.cfg'\n",
    "    _shared_project_dir = 'Metagenomics'\n",
    "    \n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        # Create instance if it does not exist yet\n",
    "        if cls._instance is None:\n",
    "            cls.home = Path.home().resolve()\n",
    "            if kwargs.get('config_fname', None) is not None:\n",
    "                cls._p2config = kwargs['config_fname']\n",
    "            else:\n",
    "                cls._p2config = cls.home / cls._config_dir / cls._config_fname\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        mount_gdrive:bool=True,       # True to mount Google Drive if running on Colab\n",
    "        project_file:Path|None=None,  # Path to the project file. If None, use the one saved in the config file\n",
    "        config_fname:Path|None=None,  # Path to a configuration file. If None, use the default one in the user's home directory\n",
    "        ) -> None:\n",
    "\n",
    "        # Discover where the script is being run\n",
    "        self.is_colab = 'google.colab' in sys.modules       \n",
    "        if self.is_colab and mount_gdrive:\n",
    "            drive.mount('/content/gdrive')\n",
    "            self.gdrive = Path('/content/gdrive/MyDrive')\n",
    "        self.is_kaggle = 'kaggle_web_client' in sys.modules\n",
    "        if self.is_kaggle:\n",
    "            raise NotImplementedError(f\"ProjectFileSystem is not implemented for Kaggle yet\")\n",
    "        if not self.is_colab and not self.is_kaggle and not self.is_local:\n",
    "            msg = \"\"\"\n",
    "                    Code does not seem to run on the cloud but computer is not registered as local\n",
    "                    If you are running on a local computer, you must register it as local by running\n",
    "                    `ProjectFileSystem().register_as_local()`\n",
    "                    before you can use the ProjectFileSystem class.\n",
    "                    \"\"\"\n",
    "            warnings.warn(msg, UserWarning)\n",
    "\n",
    "        # Set key directory paths\n",
    "        self._project_root = Path()\n",
    "        data_dir = 'data'\n",
    "        if self.is_local or os.getenv(\"GITHUB_ACTIONS\") == \"true\":\n",
    "            cfg = self.read_config()\n",
    "            path_str = cfg.get('Infra', 'project_root', fallback=None)\n",
    "            data_dir = cfg.get('Infra', 'data_dir', fallback='data')\n",
    "            if path_str is None: \n",
    "                msg = \"\"\"\n",
    "                Project root is not yet set in config file.\n",
    "                To set it, use `ProjectFileSystem().set_project_root()`.\n",
    "                \"\"\"\n",
    "                warnings.warn(msg)\n",
    "            else:\n",
    "                self._project_root = Path(path_str)\n",
    "        self._data = self.project_root / data_dir\n",
    "        self._nbs = self.project_root / 'nbs'\n",
    "\n",
    "    def __call__(self) -> bool: return self.is_local\n",
    "\n",
    "    def info(self) -> None:\n",
    "        \"\"\"Print basic info on the file system and the device\"\"\"\n",
    "        print(f\"Running {self.os} on {self.running_on}\")\n",
    "        print(f\"Device's home directory: {self.home}\")\n",
    "        print(f\"Project file structure:\")\n",
    "        print(f\" - Root ........ {self.project_root} \\n - Data Dir .... {self.data} \\n - Notebooks ... {self.nbs}\")\n",
    "    \n",
    "    def read_config(self) -> ConfigParser:\n",
    "        \"\"\"Read config from the configuration file if it exists and return an empty config if does not\"\"\"\n",
    "        cfg = ConfigParser()\n",
    "        if self.p2config.is_file(): \n",
    "            cfg.read(self.p2config)\n",
    "        else:\n",
    "            cfg.add_section('Infra')\n",
    "        return cfg\n",
    "\n",
    "    def register_as_local(self) -> ConfigParser:\n",
    "        \"\"\"Update the configuration file to register the machine as local machine\"\"\"\n",
    "        cfg = self.read_config()\n",
    "        os.makedirs(self.home/self._config_dir, exist_ok=True)\n",
    "        cfg['Infra']['registered_as_local'] = 'True'\n",
    "        with open(self.p2config, 'w') as fp:\n",
    "            cfg.write(fp)\n",
    "        return cfg\n",
    "\n",
    "    def set_project_root(\n",
    "        self, \n",
    "        p2project: str|Path,      # string or Path to the project directory. Can be absolute or relative to home\n",
    "        data_dir: str = 'data'    # Directory name for data under project root\n",
    "        ) -> ConfigParser:\n",
    "        \"\"\"Update the configuration file to set the project root\"\"\"\n",
    "        # Build and validate the path to the project root\n",
    "        if isinstance(p2project, str): \n",
    "            p2project = Path(p2project)\n",
    "            if not p2project.is_absolute():\n",
    "                p2project = self.home / p2project\n",
    "        if not p2project.is_dir(): raise FileNotFoundError(f\"{p2project.absolute()} does not exist\")\n",
    "        \n",
    "        # Update the configuration file        \n",
    "        cfg = self.read_config()\n",
    "        os.makedirs(self.home/self._config_dir, exist_ok=True)\n",
    "        cfg['Infra']['project_root'] = str(p2project.absolute())\n",
    "        cfg['Infra']['data_dir'] = str(data_dir)\n",
    "        with open(self.p2config, 'w') as fp:\n",
    "            cfg.write(fp)\n",
    "        self._project_root = p2project\n",
    "        self._data = self.project_root / data_dir\n",
    "        print(f\"Project Root set to:   {self._project_root.absolute()}\")\n",
    "        print(f\"Data directory set to: {self._data.absolute()}\")\n",
    "        return cfg\n",
    "\n",
    "    @property\n",
    "    def os(self) -> str: return sys.platform\n",
    "\n",
    "    @property\n",
    "    def project_root(self) -> Path:\n",
    "        if self.is_local:\n",
    "            return self._project_root\n",
    "        elif self.is_colab:\n",
    "            return self.gdrive / self._shared_project_dir\n",
    "        elif os.getenv(\"GITHUB_ACTIONS\") == \"true\":\n",
    "            return self._project_root\n",
    "        elif self.is_kaggle:\n",
    "            raise NotImplementedError(f\"ProjectFileSystem is not implemented for Kaggle yet\")\n",
    "        else:\n",
    "            msg = \"\"\"\n",
    "                Not running locally, on Colab or on Kaggle. If running locally,\n",
    "                register the machine as local by running `ProjectFileSystem().register_as_local()`\n",
    "                \"\"\"\n",
    "            warnings.warn(msg)\n",
    "            # raise ValueError('Not running locally, on Colab or on Kaggle')\n",
    "            return self._project_root\n",
    "\n",
    "    @property\n",
    "    def data(self) -> Path: return self._data\n",
    "\n",
    "    @data.setter\n",
    "    def data(self, folder_name) -> None: self._data = self.project_root / folder_name\n",
    "    \n",
    "    @property\n",
    "    def nbs(self) -> Path: return self.project_root / 'nbs'        \n",
    "\n",
    "    @nbs.setter\n",
    "    def nbs(self,folder_name) -> Path: return self.project_root / folder_name\n",
    "\n",
    "    @property\n",
    "    def p2config(self) -> Path: \n",
    "        # return self.home / self._config_dir / self._config_fname\n",
    "        return self._p2config\n",
    "        \n",
    "    @property\n",
    "    def is_local(self) -> bool:\n",
    "        \"\"\"Return `True` if the current machine was registered as a local machine\"\"\"\n",
    "        cfg = self.read_config()\n",
    "        return cfg['Infra'].getboolean('registered_as_local', False)\n",
    "\n",
    "    @property\n",
    "    def running_on(self) -> str:\n",
    "        \"\"\"Return the device on which this is run: local, colab, kaggle, ...\"\"\"\n",
    "        if self.is_local: device = 'local computer'\n",
    "        elif self.is_colab: device = 'colab'\n",
    "        elif self.is_kaggle: device = 'kaggle'\n",
    "        else: device = 'unknown cloud server'\n",
    "        return device\n",
    "    \n",
    "    def readme(\n",
    "        self, \n",
    "        dir_path:Path|None=None, # Path to the directory to inquire. If None, display readme file from project_root.\n",
    "        ) -> None:\n",
    "        \"\"\"Display `readme.md` file or any other `.md` file in `dir_path`. \n",
    "\n",
    "        This provides a convenient way to get information on each direcotry content\n",
    "        \"\"\"\n",
    "        if dir_path is None: \n",
    "            path = self.data\n",
    "        elif validate_path(dir_path, path_type='dir'):\n",
    "            path = dir_path\n",
    "        else:\n",
    "            raise ValueError(f\"'dir_path' is not a directory: {dir_path.absolute()}\")\n",
    "\n",
    "        \n",
    "        if path.is_relative_to(self.project_root):\n",
    "            path_to_display = path.relative_to(self.project_root)\n",
    "        else:\n",
    "            path_to_display = path.absolute()\n",
    "        display(HTML('<hr>'))\n",
    "        # display(Markdown(f\"ReadMe file for directory `{path.relative_to(self.project_root)}`:\"))\n",
    "        display(Markdown(f\"ReadMe file for directory `{path_to_display}`:\"))\n",
    "        mdfiles = {p.stem: p for p in path.glob('*.md')}\n",
    "        if mdfiles:\n",
    "            mdfile = mdfiles.get('readme', None)\n",
    "            if mdfile is None:\n",
    "                mdfile = mdfiles.get(list(mdfiles.keys())[0])\n",
    "            display(HTML('<hr>'))\n",
    "            display(Markdown(filename=mdfile))\n",
    "            display(HTML('<hr>'))\n",
    "        else:\n",
    "            print('No markdown file in this folder')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L36){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem\n",
       "\n",
       ">      ProjectFileSystem (*args, **kwargs)\n",
       "\n",
       "*Represent a project file system, return paths to key directories, provide methods to manage the file system.\n",
       "\n",
       "- Paths to key directories are based on whether the code is running locally or in the cloud.\n",
       "- First time it is used on a local computer, it must be registered as local and a project root path must be set.\n",
       "- A user configuration file is created in the user's home directory to store the project root path and whether the machine is local or not.\n",
       "\n",
       "> Technical note: `ProjectFileSystem` is a simpleton class*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L36){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem\n",
       "\n",
       ">      ProjectFileSystem (*args, **kwargs)\n",
       "\n",
       "*Represent a project file system, return paths to key directories, provide methods to manage the file system.\n",
       "\n",
       "- Paths to key directories are based on whether the code is running locally or in the cloud.\n",
       "- First time it is used on a local computer, it must be registered as local and a project root path must be set.\n",
       "- A user configuration file is created in the user's home directory to store the project root path and whether the machine is local or not.\n",
       "\n",
       "> Technical note: `ProjectFileSystem` is a simpleton class*"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ProjectFileSystem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference Project File System**:\n",
    "\n",
    "This project adopts a unified file structure to make coding and colaboration easier. In addition, we can run the code locally (from a `project-root` directory) or in the cloud (colab, kaggle, others).\n",
    "\n",
    "The unified file structure when running localy is:\n",
    "```text\n",
    "    project-root   \n",
    "        |--- data\n",
    "        |      |--- CNN_Virus_data  (all data from CNN Virus original paper)\n",
    "        |      |--- saved           (trained and finetuned models, saved preprocessed datasets)\n",
    "        |      |--- ....            (raw or pre-processed data from various sources, results, ... )  \n",
    "        |      \n",
    "        |--- nbs  (all reference and work notebooks)\n",
    "        |      |--- cnn_virus\n",
    "        |      |        |--- notebooks.ipynb\n",
    "```\n",
    "\n",
    "When running on *google colab*, it is assumed that a google drive is mounted on the colab server instance, and that this google drive root includes a shortcut named `Metagenomics` and pointing to the project shared directory. The project shared directory is accessible [here](/https://drive.google.com/drive/folders/134uei5fmt08TpzhmjG4sW0FQ06kn2ZfZ) if you are an authorized project member."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ProjectFileSystem` at work**:\n",
    "\n",
    "If you use this class for the first time on a local computer, read the two **Important Notes** below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "pfs = ProjectFileSystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once created, the instance of `ProjectFileSystem` gives access to key directories' paths:\n",
    "\n",
    "- `project root`: `Path` to the project root directory\n",
    "- `data`: `Path` to the data directory\n",
    "- `nbs`: `Path` to the notebooks directory\n",
    "\n",
    "It also provides additional information regarding the computer on which the code is running:\n",
    "\n",
    "- `os`: a string providing the name of the operating system the code is running on\n",
    "- `is_colab`: True if the code is running on google colab\n",
    "- `is_kaggle`: True if the code is running on kaggle server (NOT IMPLEMENTED YET)\n",
    "- `is_local`: True if the code is running on a computer registered as local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vtec/projects/bio/metagentorch\n",
      "/home/vtec/projects/bio/metagentorch/data\n",
      "/home/vtec/projects/bio/metagentorch/nbs\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "for p in [pfs.project_root, pfs.data, pfs.nbs]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating System: linux\n",
      "Local Computer: True, Colab: False, Kaggle: False\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "print(f\"Operating System: {pfs.os}\")\n",
    "print(f\"Local Computer: {pfs.is_local}, Colab: {pfs.is_colab}, Kaggle: {pfs.is_kaggle}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L106){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.info\n",
       "\n",
       ">      ProjectFileSystem.info ()\n",
       "\n",
       "*Print basic info on the file system and the device*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L106){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.info\n",
       "\n",
       ">      ProjectFileSystem.info ()\n",
       "\n",
       "*Print basic info on the file system and the device*"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ProjectFileSystem.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running linux on local computer\n",
      "Device's home directory: /home/vtec\n",
      "Project file structure:\n",
      " - Root ........ /home/vtec/projects/bio/metagentorch \n",
      " - Data Dir .... /home/vtec/projects/bio/metagentorch/data \n",
      " - Notebooks ... /home/vtec/projects/bio/metagentorch/nbs\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "pfs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L211){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.readme\n",
       "\n",
       ">      ProjectFileSystem.readme (dir_path:pathlib.Path|None=None)\n",
       "\n",
       "*Display `readme.md` file or any other `.md` file in `dir_path`. \n",
       "\n",
       "This provides a convenient way to get information on each direcotry content*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dir_path | pathlib.Path \\| None | None | Path to the directory to inquire. If None, display readme file from project_root. |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L211){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.readme\n",
       "\n",
       ">      ProjectFileSystem.readme (dir_path:pathlib.Path|None=None)\n",
       "\n",
       "*Display `readme.md` file or any other `.md` file in `dir_path`. \n",
       "\n",
       "This provides a convenient way to get information on each direcotry content*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dir_path | pathlib.Path \\| None | None | Path to the directory to inquire. If None, display readme file from project_root. |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ProjectFileSystem.readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `/home/vtec/projects/bio/metagentorch/nbs-dev/data_dev`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Data directory for this package development \n",
       "This directory includes all  data required to validate and test this package code.\n",
       "\n",
       "```text\n",
       "data_dev\n",
       " |--- CNN_Virus_data\n",
       " |     |--- 50mer_ds_100_seq\n",
       " |     |--- 150mer_ds_100_seq\n",
       " |     |--- train_short\n",
       " |     |--- val_short\n",
       " |     |--- weight_of_classes\n",
       " |--- ncbi\n",
       " |     |--- infer_results\n",
       " |     |     |--- cnn_virus\n",
       " |     |     |--- csv\n",
       " |     |     |--- xlsx\n",
       " |     |     |--- testdb.db\n",
       " |     |--- refsequences\n",
       " |     |     |--- cov\n",
       " |     |     |     |--cov_virus_sequence_one_metadata.json\n",
       " |     |     |     |--sequences_two_no_matching_rule.fa\n",
       " |     |     |     |--another_sequence.fa\n",
       " |     |     |     |--cov_virus_sequences_two.fa\n",
       " |     |     |     |--cov_virus_sequences_two_metadata.json\n",
       " |     |     |     |--cov_virus_sequence_one.fa\n",
       " |     |     |     |--single_1seq_150bp\n",
       " |     |     |     |    |--single_1seq_150bp.fq\n",
       " |     |     |     |    |--single_1seq_150bp.aln\n",
       " |     |     |     |--paired_1seq_150bp\n",
       " |     |     |     |    |--paired_1seq_150bp2.aln\n",
       " |     |     |     |    |--paired_1seq_150bp2.fq\n",
       " |     |     |     |    |--paired_1seq_150bp1.fq \n",
       " |     |     |     |    |--paired_1seq_150bp1.aln \n",
       " |     |--- simreads\n",
       " |     |     |--- cov\n",
       " |     |     |     |--- paired_1seq_50bp\n",
       " |     |     |     |      |--- paired_1seq_50bp_1.aln\n",
       " |     |     |     |      |--- paired_1seq_50bp_1.fq\n",
       " |     |     |     |--- single_1seq_50bp\n",
       " |     |     |     |      |--- single_1seq_50bp_1.aln\n",
       " |     |     |     |      |--- single_1seq_50bp_1.fq\n",
       " |     |     |--- cov\n",
       " |     |     |     |--single_1seq_50bp\n",
       " |     |     |     |    |--single_1seq_50bp.aln\n",
       " |     |     |     |    |--single_1seq_50bp.fq\n",
       " |     |     |     |--single_1seq_150bp\n",
       " |     |     |     |    |--single_1seq_150bp.fq\n",
       " |     |     |     |    |--single_1seq_150bp.aln\n",
       " |     |     |     |--paired_1seq_150bp\n",
       " |     |     |     |    |--paired_1seq_150bp2.aln\n",
       " |     |     |     |    |--paired_1seq_150bp2.fq\n",
       " |     |     |     |    |--paired_1seq_150bp1.fq\n",
       " |     |     |     |    |--paired_1seq_150bp1.aln\n",
       " |--- saved           \n",
       " |--- readme.md               \n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|eval: false\n",
    "pfs.readme(Path('data_dev'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Important Note 1**:\n",
    ">\n",
    ">When using the package on a local computer for the **first time**, you must register the computer as a local computer. Otherwise, `ProjectFileSystem` will raise an error. Once registered, the configuration file will be updated and `ProjectFileSystem` will detect that and run without error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L122){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.register_as_local\n",
       "\n",
       ">      ProjectFileSystem.register_as_local ()\n",
       "\n",
       "*Update the configuration file to register the machine as local machine*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L122){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.register_as_local\n",
       "\n",
       ">      ProjectFileSystem.register_as_local ()\n",
       "\n",
       "*Update the configuration file to register the machine as local machine*"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ProjectFileSystem.register_as_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "cfg = pfs.register_as_local()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Important Note 2**:\n",
    ">\n",
    ">When using the package on a local computer for the **first time**, it is also required to *set the project root directory*. This is necessary to allow users to locate their local project folder anywhere they want. Once set, the path to the project root will be saved in the configuratin file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L131){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.set_project_root\n",
       "\n",
       ">      ProjectFileSystem.set_project_root (p2project:str|pathlib.Path,\n",
       ">                                          data_dir:str='data')\n",
       "\n",
       "*Update the configuration file to set the project root*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| p2project | str \\| pathlib.Path |  | string or Path to the project directory. Can be absolute or relative to home |\n",
       "| data_dir | str | data | Directory name for data under project root |\n",
       "| **Returns** | **ConfigParser** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L131){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.set_project_root\n",
       "\n",
       ">      ProjectFileSystem.set_project_root (p2project:str|pathlib.Path,\n",
       ">                                          data_dir:str='data')\n",
       "\n",
       "*Update the configuration file to set the project root*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| p2project | str \\| pathlib.Path |  | string or Path to the project directory. Can be absolute or relative to home |\n",
       "| data_dir | str | data | Directory name for data under project root |\n",
       "| **Returns** | **ConfigParser** |  |  |"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ProjectFileSystem.set_project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root set to:   /home/vtec/projects/bio/metagentorch\n",
      "Data directory set to: /home/vtec/projects/bio/metagentorch/data\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "pfs.set_project_root('/home/vtec/projects/bio/metagentorch/');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L113){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.read_config\n",
       "\n",
       ">      ProjectFileSystem.read_config ()\n",
       "\n",
       "*Read config from the configuration file if it exists and return an empty config if does not*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L113){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.read_config\n",
       "\n",
       ">      ProjectFileSystem.read_config ()\n",
       "\n",
       "*Read config from the configuration file if it exists and return an empty config if does not*"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ProjectFileSystem.read_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "cfg = pfs.read_config()\n",
    "cfg['Infra']['registered_as_local']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vtec/projects/bio/metagentorch'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "cfg['Infra']['project_root']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "cfg['Infra']['data_dir']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technical Note for Developpers**\n",
    "\n",
    "The current notebook and all other development notebooks use a minimum set of data that comes with the repository under `nbs-dev/data_dev` instead of the standard `data` directory which is much too large for testing and developing.\n",
    "\n",
    "Therefore, when creating the instance of `ProjectFileSystem`, use the parameter `config_file` to pass a specific development configuration, also coming with the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from metagentorch.cnn_virus.utils import update_dev_cfg_file\n",
    "ProjectFileSystem._instance = None # Required because ProjectFileSystem is a singleton\n",
    "update_dev_cfg_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running linux on local computer\n",
      "Device's home directory: /home/vtec\n",
      "Project file structure:\n",
      " - Root ........ /home/vtec/projects/bio/metagentorch \n",
      " - Data Dir .... /home/vtec/projects/bio/metagentorch/nbs-dev/data_dev \n",
      " - Notebooks ... /home/vtec/projects/bio/metagentorch/nbs\n"
     ]
    }
   ],
   "source": [
    "p2dev_cfg = PACKAGE_ROOT / 'nbs-dev/metagentorch-dev.cfg'\n",
    "pfs = ProjectFileSystem(config_fname=p2dev_cfg)\n",
    "pfs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQlite Database Helper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class SqliteDatabase:\n",
    "    \"\"\"Manage a SQLite db file, execute SQL queries, return results, provide context manager functionality.\n",
    "\n",
    "    Example usage as a context manager\n",
    "    \n",
    "    ```python\n",
    "    db_path = Path('your_database.db')\n",
    "    db = SqliteDb(db_path)\n",
    "\n",
    "    with db as database:\n",
    "        result = database.get_result(\"SELECT * FROM your_table\")\n",
    "        print(result)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p2db:Path) -> None:\n",
    "        self.p2db = p2db\n",
    "        self.conn = None\n",
    "\n",
    "    def connect(self) -> Connection:\n",
    "        \"\"\"Connect to the SQLite database\"\"\"\n",
    "        self.conn = sqlite3.connect(self.p2db)\n",
    "        return self.conn\n",
    "\n",
    "    def execute(self, sql:str) -> Cursor:\n",
    "        \"\"\"Execute an SQL query and return the cursor after execution\"\"\"\n",
    "        if self.conn is None: self.conn = self.connect()\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(sql)\n",
    "        return cursor\n",
    "\n",
    "    def get_result(self, sql:str) -> list[Any]:\n",
    "        \"\"\"Execute an SQL query and return the result\"\"\"\n",
    "        cursor = self.execute(sql)\n",
    "        result = cursor.fetchall()\n",
    "        return result\n",
    "\n",
    "    def get_dataframe(self, sql:str) -> pd.DataFrame:\n",
    "        \"\"\"Wraps pandas.read_sql_query\"\"\"\n",
    "        if self.conn is None: self.connect()\n",
    "        df = pd.read_sql_query(sql, self.conn)\n",
    "        return df\n",
    "    \n",
    "    def dataframe_to_table(\n",
    "        self, \n",
    "        df:pd.DataFrame,         # DataFrame to write to the database\n",
    "        table_name:str,          # Name of the database table to write to\n",
    "        if_exists:Literal['fail','replace','append']='append',  # One of 'fail', 'replace', 'append'\n",
    "        index:bool=False         # If True, write the DataFrame index as a column\n",
    "        )-> None:\n",
    "        \"\"\"Wraps pandas.DataFrame.to_sql\"\"\"\n",
    "        if self.conn is None: self.connect()\n",
    "        df.to_sql(table_name, self.conn, if_exists=if_exists, index=index)\n",
    "            \n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close the connection to the SQLite database\"\"\"\n",
    "        if self.conn is not None:\n",
    "            self.conn.close()\n",
    "            self.conn = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"Enter the runtime context related to this object.\"\"\"\n",
    "        self.connect()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb) -> None:\n",
    "        \"\"\"Exit the runtime context related to this object.\"\"\"\n",
    "        self.close()\n",
    "\n",
    "    def list_columns(self,\n",
    "                     name: str # name of a table or a view\n",
    "                    ) -> list[str]:\n",
    "        \"\"\"Returns the list of columns in the table or view `name`\"\"\"\n",
    "        query = f\"PRAGMA table_info({name})\"\n",
    "        cursor = self.execute(query)\n",
    "        cols = [row[1] for row in cursor.fetchall()]\n",
    "        return cols\n",
    "\n",
    "    def list_indexes(self) -> None:\n",
    "        \"\"\"List indexes in the database and the indexed columns\"\"\"\n",
    "        print(f\"List of indexes in database '{self.p2db.name}' and indexed columns:\")\n",
    "        if self.conn is None: self.connect()\n",
    "\n",
    "        query = f\"SELECT name, tbl_name FROM sqlite_master WHERE type='index' ;\"\n",
    "        indexes = self.get_result(sql=query)\n",
    "\n",
    "        for index in indexes:\n",
    "            print(f\"- {index[0]} for table '{index[1]}':\")\n",
    "            cursor = self.execute(f\"PRAGMA index_info({index[0]}) ;\")\n",
    "            for col_info in cursor.fetchall():\n",
    "                print(f\"    - {col_info[2]}\")\n",
    "\n",
    "    def print_schema(self) -> None:\n",
    "        # 'type', 'name', 'tbl_name', 'rootpage', 'sql'\n",
    "        # type can be 'table', 'view', 'index' or 'trigger'\n",
    "        query = \"\"\"\n",
    "        SELECT type, name\n",
    "        FROM sqlite_master \n",
    "        WHERE type IN ('table', 'view', 'trigger') \n",
    "        \"\"\"\n",
    "        cursor = self.execute(query)\n",
    "        for t, name in cursor.fetchall():\n",
    "            print(f\"{name} ({t})\")\n",
    "            if t == 'table':\n",
    "                print(' columns:',', '.join(self.list_columns(name)))\n",
    "                indexes = self.get_result(f\"PRAGMA index_list({name})\")\n",
    "                for index in indexes:\n",
    "                    index_name = index[1]\n",
    "                    print(f\" index: {index_name}\")\n",
    "                    print('   indexed columns:',', '.join([row[2] for row in self.get_result(f\"PRAGMA index_info({index_name})\")]))\n",
    "            if t =='view':\n",
    "                print(' columns:', ','.join(self.list_columns(name)))\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2db = pfs.data / 'ncbi/infer_results/cov-ncbi/testdb.db'\n",
    "db = SqliteDatabase(p2db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE IF NOT EXISTS predictions (\n",
      "    id INTEGER PRIMARY KEY,\n",
      "readid TEXT, refseqid TEXT, refsource TEXT, refseq_strand TEXT, taxonomyid TEXT, lbl_true INTEGER, lbl_pred INTEGER, pos_true INTEGER, pos_pred INTEGER, top_5_lbl_pred_0 INTEGER, top_5_lbl_pred_1 INTEGER, top_5_lbl_pred_2 INTEGER, top_5_lbl_pred_3 INTEGER, top_5_lbl_pred_4 INTEGER)\n",
      "CREATE INDEX IF NOT EXISTS idx_preds ON predictions (readid, refseqid, pos_true);\n",
      "Table `predictions` created with index.\n",
      "\n",
      "CREATE TABLE IF NOT EXISTS label_probabilities (\n",
      "    id INTEGER PRIMARY KEY,\n",
      "    read_kmer_id TEXT,\n",
      "    read_50mer_nb INTEGER,\n",
      "prob_000 REAL,  prob_001 REAL,  prob_002 REAL,  prob_003 REAL,  prob_004 REAL,  prob_005 REAL,  prob_006 REAL,  prob_007 REAL,  prob_008 REAL,  prob_009 REAL,  prob_010 REAL,  prob_011 REAL,  prob_012 REAL,  prob_013 REAL,  prob_014 REAL,  prob_015 REAL,  prob_016 REAL,  prob_017 REAL,  prob_018 REAL,  prob_019 REAL,  prob_020 REAL,  prob_021 REAL,  prob_022 REAL,  prob_023 REAL,  prob_024 REAL,  prob_025 REAL,  prob_026 REAL,  prob_027 REAL,  prob_028 REAL,  prob_029 REAL,  prob_030 REAL,  prob_031 REAL,  prob_032 REAL,  prob_033 REAL,  prob_034 REAL,  prob_035 REAL,  prob_036 REAL,  prob_037 REAL,  prob_038 REAL,  prob_039 REAL,  prob_040 REAL,  prob_041 REAL,  prob_042 REAL,  prob_043 REAL,  prob_044 REAL,  prob_045 REAL,  prob_046 REAL,  prob_047 REAL,  prob_048 REAL,  prob_049 REAL,  prob_050 REAL,  prob_051 REAL,  prob_052 REAL,  prob_053 REAL,  prob_054 REAL,  prob_055 REAL,  prob_056 REAL,  prob_057 REAL,  prob_058 REAL,  prob_059 REAL,  prob_060 REAL,  prob_061 REAL,  prob_062 REAL,  prob_063 REAL,  prob_064 REAL,  prob_065 REAL,  prob_066 REAL,  prob_067 REAL,  prob_068 REAL,  prob_069 REAL,  prob_070 REAL,  prob_071 REAL,  prob_072 REAL,  prob_073 REAL,  prob_074 REAL,  prob_075 REAL,  prob_076 REAL,  prob_077 REAL,  prob_078 REAL,  prob_079 REAL,  prob_080 REAL,  prob_081 REAL,  prob_082 REAL,  prob_083 REAL,  prob_084 REAL,  prob_085 REAL,  prob_086 REAL,  prob_087 REAL,  prob_088 REAL,  prob_089 REAL,  prob_090 REAL,  prob_091 REAL,  prob_092 REAL,  prob_093 REAL,  prob_094 REAL,  prob_095 REAL,  prob_096 REAL,  prob_097 REAL,  prob_098 REAL,  prob_099 REAL,  prob_100 REAL,  prob_101 REAL,  prob_102 REAL,  prob_103 REAL,  prob_104 REAL,  prob_105 REAL,  prob_106 REAL,  prob_107 REAL,  prob_108 REAL,  prob_109 REAL,  prob_110 REAL,  prob_111 REAL,  prob_112 REAL,  prob_113 REAL,  prob_114 REAL,  prob_115 REAL,  prob_116 REAL,  prob_117 REAL,  prob_118 REAL,  prob_119 REAL,  prob_120 REAL,  prob_121 REAL,  prob_122 REAL,  prob_123 REAL,  prob_124 REAL,  prob_125 REAL,  prob_126 REAL,  prob_127 REAL,  prob_128 REAL,  prob_129 REAL,  prob_130 REAL,  prob_131 REAL,  prob_132 REAL,  prob_133 REAL,  prob_134 REAL,  prob_135 REAL,  prob_136 REAL,  prob_137 REAL,  prob_138 REAL,  prob_139 REAL,  prob_140 REAL,  prob_141 REAL,  prob_142 REAL,  prob_143 REAL,  prob_144 REAL,  prob_145 REAL,  prob_146 REAL,  prob_147 REAL,  prob_148 REAL,  prob_149 REAL,  prob_150 REAL,  prob_151 REAL,  prob_152 REAL,  prob_153 REAL,  prob_154 REAL,  prob_155 REAL,  prob_156 REAL,  prob_157 REAL,  prob_158 REAL,  prob_159 REAL,  prob_160 REAL,  prob_161 REAL,  prob_162 REAL,  prob_163 REAL,  prob_164 REAL,  prob_165 REAL,  prob_166 REAL,  prob_167 REAL,  prob_168 REAL,  prob_169 REAL,  prob_170 REAL,  prob_171 REAL,  prob_172 REAL,  prob_173 REAL,  prob_174 REAL,  prob_175 REAL,  prob_176 REAL,  prob_177 REAL,  prob_178 REAL,  prob_179 REAL,  prob_180 REAL,  prob_181 REAL,  prob_182 REAL,  prob_183 REAL,  prob_184 REAL,  prob_185 REAL,  prob_186 REAL, FOREIGN KEY (read_kmer_id) REFERENCES predictions(readid))\n",
      "CREATE INDEX IF NOT EXISTS idx_probs ON label_probabilities (read_kmer_id, read_50mer_nb);\n",
      "Table `label_probabilities` created with index.\n",
      "\n",
      "CREATE VIEW IF NOT EXISTS preds_probs AS\n",
      "SELECT \n",
      "    lp.id,\n",
      "    p.refseqid,\n",
      "    p.lbl_true, p.lbl_pred,\n",
      "    p.pos_true, p.pos_pred,\n",
      "    p.top_5_lbl_pred_0,p.top_5_lbl_pred_1,p.top_5_lbl_pred_2,p.top_5_lbl_pred_3,p.top_5_lbl_pred_4,\n",
      "    lp.read_kmer_id, lp.read_50mer_nb,\n",
      "    lp.prob_000,lp.prob_001,lp.prob_002,lp.prob_003,lp.prob_004,lp.prob_005,lp.prob_006,lp.prob_007,lp.prob_008,lp.prob_009,lp.prob_010,lp.prob_011,lp.prob_012,lp.prob_013,lp.prob_014,lp.prob_015,lp.prob_016,lp.prob_017,lp.prob_018,lp.prob_019,lp.prob_020,lp.prob_021,lp.prob_022,lp.prob_023,lp.prob_024,lp.prob_025,lp.prob_026,lp.prob_027,lp.prob_028,lp.prob_029,lp.prob_030,lp.prob_031,lp.prob_032,lp.prob_033,lp.prob_034,lp.prob_035,lp.prob_036,lp.prob_037,lp.prob_038,lp.prob_039,lp.prob_040,lp.prob_041,lp.prob_042,lp.prob_043,lp.prob_044,lp.prob_045,lp.prob_046,lp.prob_047,lp.prob_048,lp.prob_049,lp.prob_050,lp.prob_051,lp.prob_052,lp.prob_053,lp.prob_054,lp.prob_055,lp.prob_056,lp.prob_057,lp.prob_058,lp.prob_059,lp.prob_060,lp.prob_061,lp.prob_062,lp.prob_063,lp.prob_064,lp.prob_065,lp.prob_066,lp.prob_067,lp.prob_068,lp.prob_069,lp.prob_070,lp.prob_071,lp.prob_072,lp.prob_073,lp.prob_074,lp.prob_075,lp.prob_076,lp.prob_077,lp.prob_078,lp.prob_079,lp.prob_080,lp.prob_081,lp.prob_082,lp.prob_083,lp.prob_084,lp.prob_085,lp.prob_086,lp.prob_087,lp.prob_088,lp.prob_089,lp.prob_090,lp.prob_091,lp.prob_092,lp.prob_093,lp.prob_094,lp.prob_095,lp.prob_096,lp.prob_097,lp.prob_098,lp.prob_099,lp.prob_100,lp.prob_101,lp.prob_102,lp.prob_103,lp.prob_104,lp.prob_105,lp.prob_106,lp.prob_107,lp.prob_108,lp.prob_109,lp.prob_110,lp.prob_111,lp.prob_112,lp.prob_113,lp.prob_114,lp.prob_115,lp.prob_116,lp.prob_117,lp.prob_118,lp.prob_119,lp.prob_120,lp.prob_121,lp.prob_122,lp.prob_123,lp.prob_124,lp.prob_125,lp.prob_126,lp.prob_127,lp.prob_128,lp.prob_129,lp.prob_130,lp.prob_131,lp.prob_132,lp.prob_133,lp.prob_134,lp.prob_135,lp.prob_136,lp.prob_137,lp.prob_138,lp.prob_139,lp.prob_140,lp.prob_141,lp.prob_142,lp.prob_143,lp.prob_144,lp.prob_145,lp.prob_146,lp.prob_147,lp.prob_148,lp.prob_149,lp.prob_150,lp.prob_151,lp.prob_152,lp.prob_153,lp.prob_154,lp.prob_155,lp.prob_156,lp.prob_157,lp.prob_158,lp.prob_159,lp.prob_160,lp.prob_161,lp.prob_162,lp.prob_163,lp.prob_164,lp.prob_165,lp.prob_166,lp.prob_167,lp.prob_168,lp.prob_169,lp.prob_170,lp.prob_171,lp.prob_172,lp.prob_173,lp.prob_174,lp.prob_175,lp.prob_176,lp.prob_177,lp.prob_178,lp.prob_179,lp.prob_180,lp.prob_181,lp.prob_182,lp.prob_183,lp.prob_184,lp.prob_185,lp.prob_186\n",
      "FROM \n",
      "    label_probabilities lp\n",
      "INNER JOIN \n",
      "    predictions p\n",
      "ON \n",
      "    lp.read_kmer_id = p.readid\n",
      "\n",
      "View `preds_probs` created.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Creating the table to test the class\n",
    "top_n = 5\n",
    "\n",
    "db.connect()\n",
    "# Create table for predictions and its index\n",
    "pred_cols_str = 'readid refseqid refsource refseq_strand taxonomyid'.split(' ')\n",
    "pred_cols_int = 'lbl_true lbl_pred pos_true pos_pred'.split(' ')\n",
    "top_pred_cols = [f\"top_{top_n}_lbl_pred_{i}\" for i in range(top_n)]\n",
    "query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS predictions (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "\"\"\"\n",
    "for col in pred_cols_str:\n",
    "    query += f\"{col} TEXT, \"\n",
    "for col in pred_cols_int:\n",
    "    query += f\"{col} INTEGER, \"\n",
    "for col in top_pred_cols:\n",
    "    query += f\"{col} INTEGER, \"\n",
    "query = query[:-2]+')'\n",
    "print(query)\n",
    "db.execute(query)\n",
    "\n",
    "query = \"CREATE INDEX IF NOT EXISTS idx_preds ON predictions (readid, refseqid, pos_true);\"\n",
    "print(query)\n",
    "db.execute(query)\n",
    "print('Table `predictions` created with index.')\n",
    "\n",
    "# Create table for probabilities (one per 50-mer in order to keep small nb or columns in table)\n",
    "query = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS label_probabilities (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    read_kmer_id TEXT,\n",
    "    read_50mer_nb INTEGER,\n",
    "\"\"\"\n",
    "query += ' '.join([f\"prob_{i:03d} REAL, \" for i in range(187)])\n",
    "query += \"FOREIGN KEY (read_kmer_id) REFERENCES predictions(readid)\"\n",
    "query += ')'\n",
    "print(query)\n",
    "db.execute(query)\n",
    "\n",
    "query = \"CREATE INDEX IF NOT EXISTS idx_probs ON label_probabilities (read_kmer_id, read_50mer_nb);\"\n",
    "print(query)\n",
    "db.execute(query)\n",
    "print(f'Table `label_probabilities` created with index.')\n",
    "\n",
    "# Create view joining predictions and label_probabilities\n",
    "view_name = 'preds_probs'\n",
    "\n",
    "# top prediction columns from table predictions:\n",
    "top_lbl_pred_n = ','.join([f\"p.top_5_lbl_pred_{i}\" for i in range(5)])\n",
    "\n",
    "# probabilities columns from table label_probabilities \n",
    "probs_n = ','.join([f\"lp.prob_{i:03d}\" for i in range(187)])\n",
    "\n",
    "query = f\"\"\"\n",
    "CREATE VIEW IF NOT EXISTS {view_name} AS\n",
    "SELECT \n",
    "    lp.id,\n",
    "    p.refseqid,\n",
    "    p.lbl_true, p.lbl_pred,\n",
    "    p.pos_true, p.pos_pred,\n",
    "    {top_lbl_pred_n},\n",
    "    lp.read_kmer_id, lp.read_50mer_nb,\n",
    "    {probs_n}\n",
    "FROM \n",
    "    label_probabilities lp\n",
    "INNER JOIN \n",
    "    predictions p\n",
    "ON \n",
    "    lp.read_kmer_id = p.readid\n",
    "\"\"\"\n",
    "print(query)\n",
    "db.execute(query)\n",
    "print(f'View `preds_probs` created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions (table)\n",
      " columns: id, readid, refseqid, refsource, refseq_strand, taxonomyid, lbl_true, lbl_pred, pos_true, pos_pred, top_5_lbl_pred_0, top_5_lbl_pred_1, top_5_lbl_pred_2, top_5_lbl_pred_3, top_5_lbl_pred_4\n",
      " index: idx_preds\n",
      "   indexed columns: readid, refseqid, pos_true\n",
      "\n",
      "label_probabilities (table)\n",
      " columns: id, read_kmer_id, read_50mer_nb, prob_000, prob_001, prob_002, prob_003, prob_004, prob_005, prob_006, prob_007, prob_008, prob_009, prob_010, prob_011, prob_012, prob_013, prob_014, prob_015, prob_016, prob_017, prob_018, prob_019, prob_020, prob_021, prob_022, prob_023, prob_024, prob_025, prob_026, prob_027, prob_028, prob_029, prob_030, prob_031, prob_032, prob_033, prob_034, prob_035, prob_036, prob_037, prob_038, prob_039, prob_040, prob_041, prob_042, prob_043, prob_044, prob_045, prob_046, prob_047, prob_048, prob_049, prob_050, prob_051, prob_052, prob_053, prob_054, prob_055, prob_056, prob_057, prob_058, prob_059, prob_060, prob_061, prob_062, prob_063, prob_064, prob_065, prob_066, prob_067, prob_068, prob_069, prob_070, prob_071, prob_072, prob_073, prob_074, prob_075, prob_076, prob_077, prob_078, prob_079, prob_080, prob_081, prob_082, prob_083, prob_084, prob_085, prob_086, prob_087, prob_088, prob_089, prob_090, prob_091, prob_092, prob_093, prob_094, prob_095, prob_096, prob_097, prob_098, prob_099, prob_100, prob_101, prob_102, prob_103, prob_104, prob_105, prob_106, prob_107, prob_108, prob_109, prob_110, prob_111, prob_112, prob_113, prob_114, prob_115, prob_116, prob_117, prob_118, prob_119, prob_120, prob_121, prob_122, prob_123, prob_124, prob_125, prob_126, prob_127, prob_128, prob_129, prob_130, prob_131, prob_132, prob_133, prob_134, prob_135, prob_136, prob_137, prob_138, prob_139, prob_140, prob_141, prob_142, prob_143, prob_144, prob_145, prob_146, prob_147, prob_148, prob_149, prob_150, prob_151, prob_152, prob_153, prob_154, prob_155, prob_156, prob_157, prob_158, prob_159, prob_160, prob_161, prob_162, prob_163, prob_164, prob_165, prob_166, prob_167, prob_168, prob_169, prob_170, prob_171, prob_172, prob_173, prob_174, prob_175, prob_176, prob_177, prob_178, prob_179, prob_180, prob_181, prob_182, prob_183, prob_184, prob_185, prob_186\n",
      " index: idx_probs\n",
      "   indexed columns: read_kmer_id, read_50mer_nb\n",
      "\n",
      "preds_probs (view)\n",
      " columns: refseqid,lbl_true,lbl_pred,pos_true,pos_pred,top_5_lbl_pred_0,top_5_lbl_pred_1,top_5_lbl_pred_2,top_5_lbl_pred_3,top_5_lbl_pred_4,top_5_lbl_pred_0:1,top_5_lbl_pred_1:1,top_5_lbl_pred_2:1,top_5_lbl_pred_3:1,top_5_lbl_pred_4:1,top_5_lbl_pred_0:2,top_5_lbl_pred_1:2,top_5_lbl_pred_2:2,top_5_lbl_pred_3:2,top_5_lbl_pred_4:2,top_5_lbl_pred_0:3,top_5_lbl_pred_1:3,top_5_lbl_pred_2:3,top_5_lbl_pred_3:3,top_5_lbl_pred_4:3,top_5_lbl_pred_0:4,top_5_lbl_pred_1:4,top_5_lbl_pred_2:4,top_5_lbl_pred_3:4,top_5_lbl_pred_4:4,read_kmer_id,read_50mer_nb,prob_000,prob_001,prob_002,prob_003,prob_004,prob_005,prob_006,prob_007,prob_008,prob_009,prob_010,prob_011,prob_012,prob_013,prob_014,prob_015,prob_016,prob_017,prob_018,prob_019,prob_020,prob_021,prob_022,prob_023,prob_024,prob_025,prob_026,prob_027,prob_028,prob_029,prob_030,prob_031,prob_032,prob_033,prob_034,prob_035,prob_036,prob_037,prob_038,prob_039,prob_040,prob_041,prob_042,prob_043,prob_044,prob_045,prob_046,prob_047,prob_048,prob_049,prob_050,prob_051,prob_052,prob_053,prob_054,prob_055,prob_056,prob_057,prob_058,prob_059,prob_060,prob_061,prob_062,prob_063,prob_064,prob_065,prob_066,prob_067,prob_068,prob_069,prob_070,prob_071,prob_072,prob_073,prob_074,prob_075,prob_076,prob_077,prob_078,prob_079,prob_080,prob_081,prob_082,prob_083,prob_084,prob_085,prob_086,prob_087,prob_088,prob_089,prob_090,prob_091,prob_092,prob_093,prob_094,prob_095,prob_096,prob_097,prob_098,prob_099,prob_100,prob_101,prob_102,prob_103,prob_104,prob_105,prob_106,prob_107,prob_108,prob_109,prob_110,prob_111,prob_112,prob_113,prob_114,prob_115,prob_116,prob_117,prob_118,prob_119,prob_120,prob_121,prob_122,prob_123,prob_124,prob_125,prob_126,prob_127,prob_128,prob_129,prob_130,prob_131,prob_132,prob_133,prob_134,prob_135,prob_136,prob_137,prob_138,prob_139,prob_140,prob_141,prob_142,prob_143,prob_144,prob_145,prob_146,prob_147,prob_148,prob_149,prob_150,prob_151,prob_152,prob_153,prob_154,prob_155,prob_156,prob_157,prob_158,prob_159,prob_160,prob_161,prob_162,prob_163,prob_164,prob_165,prob_166,prob_167,prob_168,prob_169,prob_170,prob_171,prob_172,prob_173,prob_174,prob_175,prob_176,prob_177,prob_178,prob_179,prob_180,prob_181,prob_182,prob_183,prob_184,prob_185,prob_186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db.print_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other utility classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class JsonDict(dict):\n",
    "    \"\"\"Dictionary whose current value is mirrored in a json file and can be initated from a json file\n",
    "    \n",
    "    `JsonDict` requires a path to json file at creation. An optional dict can be passed as argument.\n",
    "\n",
    "    Behavior at creation:\n",
    "    \n",
    "    - `JsonDict(p2json, dict)` will create a `JsonDict` with key-values from `dict`, and mirrored in `p2json`\n",
    "    - `JsonDict(p2json)` will create a `JsonDict` with empty dictionary and load json content if file exists\n",
    "\n",
    "    Once created, `JsonDict` instances behave exactly as a dictionary\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        p2json: str|Path,               # path to the json file to mirror with the dictionary \n",
    "        dictionary: dict|None = None    # optional dictionary to initialize the JsonDict\n",
    "        ):\n",
    "        \"\"\"Create dict from a passed dict or from json. Create the json file if required\"\"\"\n",
    "        self.p2json = Path(p2json) if isinstance(p2json, str) else p2json\n",
    "        if dictionary is None:\n",
    "            if self.p2json.is_file():\n",
    "                dictionary = self.load()\n",
    "                self.initial_dict_from_json = True\n",
    "            else:\n",
    "                dictionary = dict()\n",
    "                self.initial_dict_from_json = False\n",
    "        super().__init__(dictionary)\n",
    "        self.save()\n",
    "    \n",
    "    def __setitem__(self, __k:Any, v:Any) -> None:\n",
    "        super().__setitem__(__k, v)\n",
    "        self.save()\n",
    "\n",
    "    def __delitem__(self, k:Any):\n",
    "        super().__delitem__(k)\n",
    "        self.save()\n",
    "\n",
    "    def __repr__(self):\n",
    "        txt1 = f\"dict mirrored in {self.p2json.absolute()}\\n\"\n",
    "        txt2 = super().__repr__()\n",
    "        return txt1 + txt2\n",
    "\n",
    "    def load(self)->dict:\n",
    "        with open(self.p2json, 'r') as fp:\n",
    "            return json.load(fp)\n",
    "\n",
    "    def save(self):\n",
    "        with open(self.p2json, 'w') as fp:\n",
    "            json.dump(self, fp, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dictionary mirrored to a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict mirrored in /home/vtec/projects/bio/metagentorch/nbs-dev/data_dev/jsondict-test.json\n",
       "{'a': 1, 'b': 2, 'c': 3}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'a': 1, 'b': 2, 'c': 3}\n",
    "p2json = pfs.data / 'jsondict-test.json'\n",
    "jsondict = JsonDict(p2json, d)\n",
    "jsondict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once created, the `JsonFile` instance behaves exactly like a dictionary, with the added benefit that any change to the dictionary is automatically saved to the JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsondict['a'], jsondict['b'], jsondict['c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: a; value: 1\n",
      "key: b; value: 2\n",
      "key: c; value: 3\n"
     ]
    }
   ],
   "source": [
    "for k, v in jsondict.items():\n",
    "    print(f\"key: {k}; value: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding or removing a value from the dictionary works in the same way as for a normal dictionary. But the json file is automatically updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict mirrored in /home/vtec/projects/bio/metagentorch/nbs-dev/data_dev/jsondict-test.json\n",
       "{'a': 1, 'b': 2, 'c': 3, 'd': 4}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsondict['d'] = 4\n",
    "jsondict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"a\": 1,\n",
      "    \"b\": 2,\n",
      "    \"c\": 3,\n",
      "    \"d\": 4\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(p2json, 'r') as fp:\n",
    "    print(fp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict mirrored in /home/vtec/projects/bio/metagentorch/nbs-dev/data_dev/jsondict-test.json\n",
       "{'b': 2, 'c': 3, 'd': 4}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del jsondict['a']\n",
    "jsondict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"b\": 2,\n",
      "    \"c\": 3,\n",
      "    \"d\": 4\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(p2json, 'r') as fp:\n",
    "    print(fp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class JsonFileReader:\n",
    "    \"\"\"Mirror a JSON file and a dictionary\"\"\"\n",
    "    def __init__(self, \n",
    "                 path:str|Path # path to the json file\n",
    "                ):\n",
    "        self.path = safe_path(path)\n",
    "        with open(path, 'r') as fp:\n",
    "            self.d = json.load(fp)\n",
    "    \n",
    "    def add_item(self, \n",
    "                 key:str,  # key for the new item\n",
    "                 item:dict # new item to add to the json as a dict\n",
    "                ):\n",
    "        self.d[key] = item\n",
    "        return self.d\n",
    "\n",
    "    def save_to_file(self, path=None):\n",
    "        if path is None: \n",
    "            path = self.path\n",
    "        else:\n",
    "            path = safe_path(path)\n",
    "\n",
    "        with open(path, 'w') as fp:\n",
    "            json.dump(self.d, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L416){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### JsonFileReader\n",
       "\n",
       ">      JsonFileReader (path:str|pathlib.Path)\n",
       "\n",
       "*Mirror a JSON file and a dictionary*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| pathlib.Path | path to the json file |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L416){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### JsonFileReader\n",
       "\n",
       ">      JsonFileReader (path:str|pathlib.Path)\n",
       "\n",
       "*Mirror a JSON file and a dictionary*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| pathlib.Path | path to the json file |"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(JsonFileReader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'item 1': {'keys': 'key key key key', 'pattern': 'pattern 1'},\n",
      " 'item 2': {'keys': 'key key key key', 'pattern': 'pattern 2'},\n",
      " 'item 3': {'keys': 'key key key key', 'pattern': 'pattern 3'}}\n"
     ]
    }
   ],
   "source": [
    "jd = JsonFileReader(pfs.data / 'test.json')\n",
    "pprint(jd.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add an item to the dictionary/json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item 1': {'keys': 'key key key key', 'pattern': 'pattern 1'},\n",
       " 'item 2': {'keys': 'key key key key', 'pattern': 'pattern 2'},\n",
       " 'item 3': {'keys': 'key key key key', 'pattern': 'pattern 3'},\n",
       " 'another item': {'keys': 'key key key key', 'pattern': 'another pattern'}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_item = {'keys': 'key key key key', 'pattern': 'another pattern'}\n",
    "jd.add_item(key='another item', item=new_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the updated JSON file, we can load it again and see the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "jd.save_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'another item': {'keys': 'key key key key', 'pattern': 'another pattern'},\n",
      " 'item 1': {'keys': 'key key key key', 'pattern': 'pattern 1'},\n",
      " 'item 2': {'keys': 'key key key key', 'pattern': 'pattern 2'},\n",
      " 'item 3': {'keys': 'key key key key', 'pattern': 'pattern 3'}}\n"
     ]
    }
   ],
   "source": [
    "jd = JsonFileReader(pfs.data / 'test.json')\n",
    "pprint(jd.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "initial_json = {\n",
    "    'item 1': {'keys': 'key key key key', 'pattern': 'pattern 1'},\n",
    "    'item 2': {'keys': 'key key key key', 'pattern': 'pattern 2'},\n",
    "    'item 3': {'keys': 'key key key key', 'pattern': 'pattern 3'}\n",
    "    }\n",
    "with open(pfs.data / 'test.json', 'w') as fp:\n",
    "    json.dump(initial_json, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def list_available_devices():\n",
    "    # Check if CUDA is available\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "    # List all available CUDA devices\n",
    "    if cuda_available:\n",
    "        num_cuda_devices = torch.cuda.device_count()\n",
    "        print(f\"Number of CUDA devices: {num_cuda_devices}\")\n",
    "        for i in range(num_cuda_devices):\n",
    "            print(f\"CUDA Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "    # Check for CPU availability\n",
    "    cpu_available = torch.device('cpu')\n",
    "    print(f\"CPU available: {cpu_available}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Readers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base classes to be extended in order to create readers for specific file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextFileBaseReader:\n",
    "    \"\"\"Iterator going through a text file by chunks of `nlines` lines. Iterator can be reset to file start.\n",
    "    \n",
    "    The class is mainly intented to be extended, as it is for handling sequence files of various formats such as `FastaFileReader`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str|Path,  # path to the file\n",
    "        nlines: int=1,   # number of lines on one chunk\n",
    "    ):\n",
    "        self.path = safe_path(path)\n",
    "        self.nlines = nlines\n",
    "        self.fp = None\n",
    "        self.reset_iterator()\n",
    "        \n",
    "        # Attributes related to metadata parsing\n",
    "        # Currently assumes the iterator generates a dictionary with key/values\n",
    "        # TODO: extend to iterator output as simple string.\n",
    "        self.text_to_parse_key = None\n",
    "        self.parsing_rules_json = ProjectFileSystem().project_root / 'default_parsing_rules.json'\n",
    "        self.re_rule_name = None\n",
    "        self.re_pattern = None       # regex pattern to use to parse text\n",
    "        self.re_keys = None          # keys (re group names) to parse text\n",
    "\n",
    "    def reset_iterator(self) -> None:\n",
    "        \"\"\"Reset the iterator to point to the first line in the file.\"\"\"\n",
    "        if self.fp is not None:\n",
    "            self.fp.close()\n",
    "        self.fp = open(self.path, 'r')\n",
    "        self._chunk_nb = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def _safe_readline(self) -> str:\n",
    "        \"\"\"Read a new line and handle end of file tasks.\"\"\"\n",
    "        if self.fp is None: raise RuntimeError(f\"File {self.path} is not opened\")\n",
    "        line = self.fp.readline()\n",
    "        if line == '':\n",
    "            self.fp.close()\n",
    "            raise StopIteration()\n",
    "        return line\n",
    "\n",
    "    def __next__(self) -> str:\n",
    "        \"\"\"Return one chunk of `nlines` text lines at the time\"\"\"\n",
    "        lines = []\n",
    "        for i in range(self.nlines):\n",
    "            lines.append(self._safe_readline())\n",
    "        self._chunk_nb = self._chunk_nb + 1\n",
    "        return ''.join(lines)\n",
    "    \n",
    "    def print_first_chunks(\n",
    "        self, \n",
    "        nchunks:int=3,  # number of chunks to print\n",
    "    ) -> None:\n",
    "        \"\"\"Print the first `nchunk` chunks of text from the file.\n",
    "\n",
    "        After printing, the iterator is reset again to its first line.\n",
    "        \"\"\"\n",
    "        self.reset_iterator()\n",
    "        for i, chunk in enumerate(self.__iter__()):\n",
    "            if i > nchunks-1: break\n",
    "            print(f\"{self.nlines}-line chunk {i+1}\")\n",
    "            print(chunk)\n",
    "        self.reset_iterator()\n",
    "            \n",
    "    def _parse_text_fn(\n",
    "        self,\n",
    "        txt:str,         # text to parse\n",
    "        pattern:str,     # regex pattern to apply to parse the text, must include groups\n",
    "    )-> dict:            # parsed metadata in key/value format\n",
    "        \"\"\"Parsing metadata from string, using regex pattern. Return a metadata dictionary.\"\"\"\n",
    "        p = re.compile(pattern)\n",
    "        keys = list(p.groupindex.keys())\n",
    "        if len(keys)< 1: \n",
    "            raise ValueError(f\"Pattern must include at least one group\")\n",
    "\n",
    "        match = p.search(txt)\n",
    "        if match is not None:\n",
    "            metadata = match.groupdict()\n",
    "        else:\n",
    "            metadata = {k:'' for k in keys}\n",
    "        return metadata\n",
    "\n",
    "    def parse_text(\n",
    "        self,\n",
    "        txt:str,                    # text to parse\n",
    "        pattern:str|None=None,      # If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex\n",
    "        # keys:list[str]|None=None,   # If None, uses standard regex list of keys, otherwise, uses passed list of keys (str)\n",
    "    )-> dict:                       # parsed metadata in key/value format\n",
    "        \"\"\"Parse text using regex pattern with groups. Return a metadata dictionary.\"\"\"\n",
    "        if pattern is None:\n",
    "            if self.re_pattern is not None:\n",
    "                return self._parse_text_fn(txt, self.re_pattern)\n",
    "            else:\n",
    "                raise ValueError('attribute re_pattern and re_keys are still None')\n",
    "        else:\n",
    "            return self._parse_text_fn(txt, pattern)        \n",
    "        \n",
    "    def set_parsing_rules(\n",
    "        self,\n",
    "        pattern: str|None=None,     # regex pattern to apply to parse the text, search in parsing rules json if None\n",
    "        verbose: bool=False         # when True, provides information on each rule\n",
    "    )-> None:\n",
    "        \"\"\"Set the standard regex parsing rule for the file.\n",
    "        \n",
    "        Rules can be set:\n",
    "        \n",
    "        1. manually by passing specific custom values for `pattern` and `keys`\n",
    "        2. automatically, by testing all parsing rules saved in `parsing_rule.json` \n",
    "        \n",
    "        Automatic selection of parsing rules works by testing each rule saved in `parsing_rule.json` on the first \n",
    "        definition line of the fasta file, and selecting the one rule that generates the most metadata matches.\n",
    "        \n",
    "        Rules consists of two parameters:\n",
    "        \n",
    "        - The regex pattern including one `group` for each metadata item, e.g `(?P<group_name>regex_code)`\n",
    "        - The list of keys, i.e. the list with the name of each regex groups, used as key in the metadata dictionary\n",
    "        \n",
    "        This method updates the three following class attributes: `re_rule_name`, `re_pattern`, `re_keys`\n",
    "      \n",
    "        \"\"\"\n",
    "        # get the first definition line in the file to test the pattern\n",
    "        # in base class, text_to_parse_key is not defined and automatic rule selection cannot be used\n",
    "        # this must be handled in children classes\n",
    "        if self.text_to_parse_key is None:\n",
    "            msg = \"\"\"\n",
    "            `text_to_parse_key` is not defined in this class. \n",
    "            It is not possible to set a parsing rule. Must be define, e.g. 'definition line'\n",
    "            \"\"\"\n",
    "            warnings.warn(msg, category=UserWarning)\n",
    "            return\n",
    "\n",
    "        self.reset_iterator()\n",
    "        first_output = next(self)\n",
    "        text_to_parse = first_output[self.text_to_parse_key]\n",
    "        divider_line = f\"{'-'*80}\"\n",
    "\n",
    "        if pattern is not None:\n",
    "            re_keys = list(re.compile(pattern).groupindex.keys())\n",
    "            if len(re_keys) < 1: raise ValueError(f\"Pattern must include at least one group\")\n",
    "            try:\n",
    "                metadata_dict = self.parse_text(text_to_parse, pattern)\n",
    "                self.re_rule_name = 'Custom Rule'\n",
    "                self.re_pattern = pattern\n",
    "                self.re_keys = re_keys\n",
    "                if verbose:\n",
    "                    print(divider_line)\n",
    "                    print(f\"Custom rule was set for this instance.\")\n",
    "                    print(f\"{self.re_rule_name}: {self.re_pattern}\")\n",
    "            except Exception as err: \n",
    "                raise ValueError(f\"The pattern generates the following error:\\n{err}\")      \n",
    "        else:\n",
    "            # Load all existing rules from json file\n",
    "            with open(self.parsing_rules_json, 'r') as fp:\n",
    "                parsing_rules = json.load(fp)\n",
    "                \n",
    "            # test all existing rules and keep the one with highest number of matches\n",
    "            max_nbr_matches = 0\n",
    "            for k, v in parsing_rules.items():\n",
    "                re_pattern:str = v['pattern']\n",
    "                re_keys = list(re.compile(re_pattern).groupindex.keys())\n",
    "                try:\n",
    "                    metadata_dict = self.parse_text(text_to_parse, re_pattern)\n",
    "                    nbr_matches = len([k for k,v in metadata_dict.items() if v is not None and v !=''])\n",
    "                    if verbose:\n",
    "                        print(divider_line)\n",
    "                        # print(f\"Rule <{k}> generated {nbr_matches:,d} matches\")\n",
    "                        print(f\"{nbr_matches:,d} matches generated with rule <{k}> \")\n",
    "                        print(divider_line)\n",
    "                        print(re_pattern)\n",
    "                        print(re_keys)\n",
    "                        print(metadata_dict)\n",
    "\n",
    "                    if nbr_matches > max_nbr_matches:\n",
    "                        self.re_pattern = re_pattern\n",
    "                        self.re_keys = re_keys\n",
    "                        self.re_rule_name = k   \n",
    "                except Exception as err:\n",
    "                    if verbose:\n",
    "                        print(divider_line)\n",
    "                        print(f\"Rule <{k}> generated an error\")\n",
    "                        print(err)\n",
    "                    else:\n",
    "                        pass\n",
    "            if self.re_rule_name is None:\n",
    "                msg = \"\"\"\n",
    "        None of the saved parsing rules were able to extract metadata from the first line in this file.\n",
    "        You must set a custom rule (pattern + keys) before parsing text, by using:\n",
    "            `self.set_parsing_rules(custom_pattern)`\n",
    "                \"\"\"\n",
    "                warnings.warn(msg, category=UserWarning)\n",
    "            \n",
    "            if verbose:\n",
    "                print(divider_line)\n",
    "                print(f\"Selected rule with most matches: {self.re_rule_name}\")\n",
    "\n",
    "            # We used the iterator, now we need to reset it to make all lines available\n",
    "            self.reset_iterator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once initialized, the iterator runs over each chunk of line(s) in the text file, sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/vtec/projects/bio/metagentorch/nbs-dev/data_dev')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfs.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC\t76\t0\n",
      "ATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG\t4\t9\n",
      "AATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA\t181\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p2textfile = pfs.data / 'CNN_Virus_data/train_short'\n",
    "it = TextFileBaseReader(path=p2textfile, nlines=3)\n",
    "\n",
    "one_iteration = next(it)\n",
    "\n",
    "print(one_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new instance of the file reader, and get several iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC\t76\t0\n",
      "ATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG\t4\t9\n",
      "AATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA\t181\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "it = TextFileBaseReader(path=p2textfile, nlines=3)\n",
    "\n",
    "one_iteration = next(it)\n",
    "print(one_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GGAGCGGAGCCAACCCCTATGCTCACTTGCAACCCAAGGGGCGTTCCAGT\t74\t3\n",
      "TGGATCCTGCGCGGGACGTCCTTTGTCTACGTCCCGTCGGCGCATCCCGC\t60\t3\n",
      "GAGAGACTTACTAAAAAGCTGGCACTTACCATCAGTGTTTCACCTACATG\t44\t0\n",
      "\n",
      "ACACACGACACTAGAGATAATGTGTCAGTGGATTATAAACAAACCAAGTT\t43\t7\n",
      "TTGTAGCATAAGAACTGGTCTTCGCTGAAATTCTTGTCTTGATCTCATCT\t35\t2\n",
      "TGGCCCTGCGGTCTGGGGCCCAGAAGCATATGTCAAGTCCTTTGAGAAGT\t73\t4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "another_iteration = next(it)\n",
    "print(another_iteration)\n",
    "one_more_iteration = next(it)\n",
    "print(one_more_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to access the start of the file again, we need to re-initialize the file handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L468){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.reset_iterator\n",
       "\n",
       ">      TextFileBaseReader.reset_iterator ()\n",
       "\n",
       "*Reset the iterator to point to the first line in the file.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L468){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.reset_iterator\n",
       "\n",
       ">      TextFileBaseReader.reset_iterator ()\n",
       "\n",
       "*Reset the iterator to point to the first line in the file.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextFileBaseReader.reset_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC\t76\t0\n",
      "ATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG\t4\t9\n",
      "AATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA\t181\t0\n",
      "\n",
      "GGAGCGGAGCCAACCCCTATGCTCACTTGCAACCCAAGGGGCGTTCCAGT\t74\t3\n",
      "TGGATCCTGCGCGGGACGTCCTTTGTCTACGTCCCGTCGGCGCATCCCGC\t60\t3\n",
      "GAGAGACTTACTAAAAAGCTGGCACTTACCATCAGTGTTTCACCTACATG\t44\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "it.reset_iterator()\n",
    "one_iteration = next(it)\n",
    "print(one_iteration)\n",
    "another_iteration = next(it)\n",
    "print(another_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L495){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.print_first_chunks\n",
       "\n",
       ">      TextFileBaseReader.print_first_chunks (nchunks:int=3)\n",
       "\n",
       "*Print the first `nchunk` chunks of text from the file.\n",
       "\n",
       "After printing, the iterator is reset again to its first line.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| nchunks | int | 3 | number of chunks to print |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L495){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.print_first_chunks\n",
       "\n",
       ">      TextFileBaseReader.print_first_chunks (nchunks:int=3)\n",
       "\n",
       "*Print the first `nchunk` chunks of text from the file.\n",
       "\n",
       "After printing, the iterator is reset again to its first line.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| nchunks | int | 3 | number of chunks to print |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextFileBaseReader.print_first_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-line chunk 1\n",
      "TCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC\t76\t0\n",
      "ATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG\t4\t9\n",
      "AATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA\t181\t0\n",
      "\n",
      "3-line chunk 2\n",
      "GGAGCGGAGCCAACCCCTATGCTCACTTGCAACCCAAGGGGCGTTCCAGT\t74\t3\n",
      "TGGATCCTGCGCGGGACGTCCTTTGTCTACGTCCCGTCGGCGCATCCCGC\t60\t3\n",
      "GAGAGACTTACTAAAAAGCTGGCACTTACCATCAGTGTTTCACCTACATG\t44\t0\n",
      "\n",
      "3-line chunk 3\n",
      "ACACACGACACTAGAGATAATGTGTCAGTGGATTATAAACAAACCAAGTT\t43\t7\n",
      "TTGTAGCATAAGAACTGGTCTTCGCTGAAATTCTTGTCTTGATCTCATCT\t35\t2\n",
      "TGGCCCTGCGGTCTGGGGCCCAGAAGCATATGTCAAGTCCTTTGAGAAGT\t73\t4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "it = TextFileBaseReader(path=p2textfile, nlines=3)\n",
    "\n",
    "it.print_first_chunks(nchunks=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L542){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.parse_text\n",
       "\n",
       ">      TextFileBaseReader.parse_text (txt:str, pattern:str|None=None,\n",
       ">                                     keys:list[str]|None=None)\n",
       "\n",
       "*Parse text using regex pattern and keys. Return a metadata dictionary.\n",
       "\n",
       "The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
       "\n",
       "    {\n",
       "        'key_1': 'metadata 1',\n",
       "        'key_2': 'metadata 2',\n",
       "        ...\n",
       "    }*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| txt | str |  | text to parse |\n",
       "| pattern | str \\| None | None | If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex |\n",
       "| keys | list[str] \\| None | None | If None, uses standard regex list of keys, otherwise, uses passed list of keys (str) |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L542){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.parse_text\n",
       "\n",
       ">      TextFileBaseReader.parse_text (txt:str, pattern:str|None=None,\n",
       ">                                     keys:list[str]|None=None)\n",
       "\n",
       "*Parse text using regex pattern and keys. Return a metadata dictionary.\n",
       "\n",
       "The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
       "\n",
       "    {\n",
       "        'key_1': 'metadata 1',\n",
       "        'key_2': 'metadata 2',\n",
       "        ...\n",
       "    }*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| txt | str |  | text to parse |\n",
       "| pattern | str \\| None | None | If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex |\n",
       "| keys | list[str] \\| None | None | If None, uses standard regex list of keys, otherwise, uses passed list of keys (str) |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextFileBaseReader.parse_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2591237', 'nb': '1', 'source': 'ncbi'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '>2591237:ncbi:1'\n",
    "pattern = r\"^>(?P<id>\\d+):(?P<source>ncbi):(?P<nb>\\d*)\"\n",
    "p = re.compile(pattern)\n",
    "keys = p.groupindex.keys()\n",
    "\n",
    "it.parse_text(text, pattern, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending the base class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TextFileBaseReader` is a base class, intended to be extended into specific file format readers.\n",
    "\n",
    "The following methods will typically be extended to match data file and other structured text files formats:\n",
    "\n",
    "- `__next__` method in order to customize how the iterator parses files into \"elements\". For instance, in a FASTA file, one element consists of two lines: a *\"definition line\"* and the *sequence* itself. Extending `TextFileBaseReader` allows to read pairs of lines sequentially and return an element as a dictionary. For instance, `FastaFileReader` iterates over each pairs of lines in a Fasta file and return each pair as a dictionary as follows:\n",
    "\n",
    "```text\n",
    "    {\n",
    "    'definition line': '>2591237:ncbi:1 [MK211378]\\t2591237\\tncbi\\t1 [MK211378] '\n",
    "                       '2591237\\tCoronavirus BtRs-BetaCoV/YN2018D\\t\\tscientific '\n",
    "                       'name\\n',\n",
    "    'sequence':        'TATTAGGTTTTCTACCTACCCAGGA'\n",
    "    }\n",
    "```\n",
    "- Methods for parsing metadata from the file. For instance, `parse_file` method will handle how the reader will iterate over the full file and return a dictionary for the entire file. \n",
    "- Extended classes will also define a specific attributes (`text_to_parse_key`, `re_pattern`, `re_keys`, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L571){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.set_parsing_rules\n",
       "\n",
       ">      TextFileBaseReader.set_parsing_rules (pattern:str|None=None,\n",
       ">                                            keys:list[str]|None=None,\n",
       ">                                            verbose:bool=False)\n",
       "\n",
       "*Set the standard regex parsing rule for the file.\n",
       "\n",
       "Rules can be set:\n",
       "\n",
       "1. manually by passing specific custom values for `pattern` and `keys`\n",
       "2. automatically, by testing all parsing rules saved in `parsing_rule.json` \n",
       "\n",
       "Automatic selection of parsing rules works by testing each rule saved in `parsing_rule.json` on the first \n",
       "definition line of the fasta file, and selecting the one rule that generates the most metadata matches.\n",
       "\n",
       "Rules consists of two parameters:\n",
       "\n",
       "- The regex pattern including one `group` for each metadata item, e.g `(?P<group_name>regex_code)`\n",
       "- The list of keys, i.e. the list with the name of each regex groups, used as key in the metadata dictionary\n",
       "\n",
       "This method updates the three following class attributes: `re_rule_name`, `re_pattern`, `re_keys`*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| None | None | regex pattern to apply to parse the text, search in parsing rules json if None |\n",
       "| keys | list[str] \\| None | None | list of keys/group for regex, search in parsing rules json if None |\n",
       "| verbose | bool | False | when True, provides information on each rule |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#L571){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.set_parsing_rules\n",
       "\n",
       ">      TextFileBaseReader.set_parsing_rules (pattern:str|None=None,\n",
       ">                                            keys:list[str]|None=None,\n",
       ">                                            verbose:bool=False)\n",
       "\n",
       "*Set the standard regex parsing rule for the file.\n",
       "\n",
       "Rules can be set:\n",
       "\n",
       "1. manually by passing specific custom values for `pattern` and `keys`\n",
       "2. automatically, by testing all parsing rules saved in `parsing_rule.json` \n",
       "\n",
       "Automatic selection of parsing rules works by testing each rule saved in `parsing_rule.json` on the first \n",
       "definition line of the fasta file, and selecting the one rule that generates the most metadata matches.\n",
       "\n",
       "Rules consists of two parameters:\n",
       "\n",
       "- The regex pattern including one `group` for each metadata item, e.g `(?P<group_name>regex_code)`\n",
       "- The list of keys, i.e. the list with the name of each regex groups, used as key in the metadata dictionary\n",
       "\n",
       "This method updates the three following class attributes: `re_rule_name`, `re_pattern`, `re_keys`*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| None | None | regex pattern to apply to parse the text, search in parsing rules json if None |\n",
       "| keys | list[str] \\| None | None | list of keys/group for regex, search in parsing rules json if None |\n",
       "| verbose | bool | False | when True, provides information on each rule |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextFileBaseReader.set_parsing_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important Note to Developpers**\n",
    ">\n",
    "> Method `set_parsing_rules` is there to allow `TextFileBaseReader`'s descendant classes to automatically select parsing rule by applying rules saved in a json file to a string extracted from the first element in the file.\n",
    ">\n",
    "> It assumes that the iterator returns its elements as dictionaries `{section_name:section, ...}` and not as a pure string. The key `self.text_to_parse_key` will then be used to extract the text to parse for testing the rules.\n",
    "> The base class iterator returns a simple string and `self.text_to_parse_key` is set to `None`.\n",
    ">\n",
    "> To make setting up a default parsing rule for the reader instance, the iterator must return a dictionary and `self.text_to_parse_key` must be set to the key in the dictionary corresponding the the text to parse. \n",
    ">\n",
    "> See implementation in `FastaFileReader`.\n",
    ">\n",
    "> Calling `set_parsing_rules` on a class that does not satisfy with these characteristics will do nothing and return a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7689/2017328125.py:164: UserWarning: \n",
      "            `text_to_parse_key` is not defined in this class. \n",
      "            It is not possible to set a parsing rule.\n",
      "            \n",
      "  warnings.warn(msg, category=UserWarning)\n"
     ]
    }
   ],
   "source": [
    "it.set_parsing_rules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When any of the following classes and functions is called, it will raise an exception with an error message indicating how to handle the required code refactoring.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "DeprecationWarning                        Traceback (most recent call last)\n",
    "Input In [140], in <cell line: 1>()\n",
    "----> 1 TextFileBaseIterator(p2textfile)\n",
    "\n",
    "Input In [139], in TextFileBaseIterator.__init__(self, *args, **kwargs)\n",
    "      4 def __init__(self, *args, **kwargs):\n",
    "      5     msg = \\\"\\\"\\\"\n",
    "      6     `TextFileBaseIterator` is deprecated. \n",
    "      7     Use `TextFileBaseReader` instead, with same capabilities and more.\\\"\\\"\\\"\n",
    "----> 8     raise DeprecationWarning(msg)\n",
    "\n",
    "DeprecationWarning: \n",
    "        `TextFileBaseIterator` is deprecated. \n",
    "        Use `TextFileBaseReader` instead, with same capabilities and more.\" \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextFileBaseIterator:\n",
    "    \"\"\"`TextFileBaseIterator` is a deprecated class, to be replaced by `TextFileBaseReader`\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        msg = \"\"\"\n",
    "        `TextFileBaseIterator` is deprecated. \n",
    "        Use `TextFileBaseReader` instead, with same capabilities and more.\"\"\"\n",
    "        raise DeprecationWarning(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "p2textfile = Path('data_dev/train_short')\n",
    "test_fail(TextFileBaseIterator, msg=\"Should generate DeprecationWarning\", contains=\"`TextFileBaseIterator` is deprecated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
