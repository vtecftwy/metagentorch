{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp cnn_virus.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from eccore.ipython import nb_setup\n",
    "from eccore.core import files_in_tree\n",
    "from fastcore.test import test_fail\n",
    "from nbdev import show_doc, nbdev_export\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Set autoreload mode\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "nb_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import collections\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from functools import partial, partialmethod\n",
    "from pathlib import Path\n",
    "from typing import Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Set pytorch as backend\n",
    "os.environ['KERAS_BACKEND'] = 'torch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from eccore.core import safe_path, validate_path\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from metagentorch.bio import q_score2prob_error\n",
    "from metagentorch.core import ProjectFileSystem, TextFileBaseReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Retrieve the package root\n",
    "from metagentorch import __file__\n",
    "CODE_ROOT = Path(__file__).parents[0]\n",
    "PACKAGE_ROOT = Path(__file__).parents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.5.1 - Expected 2.5.1\n",
      "Keras version: 3.8.0 - Expected 3.8.0\n",
      "metagentorch package location: /home/vtec/projects/bio/metagentorch/metagentorch/__init__.py\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "print(f\"Pytorch version: {torch.__version__} - Expected 2.5.1\")\n",
    "print(f\"Keras version: {keras.__version__} - Expected 3.8.0\")\n",
    "print(f\"metagentorch package location: {__file__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "> Data preprocessing and transform tools for CNN Virus data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Virus project data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different types of files and datasets for this project. All data are located in directory `data`, under the project root. The following is an overview of the main types of data and where they sit in the directory tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an instance of `ProjectFileSystem` to access the data directory and its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Set configuration file to be the one in nbs-dev folder.\n",
    "# As ProjectFileSystem is a singleton class, this only needs to be done once per notebook\n",
    "p2dev_cfg = PACKAGE_ROOT / 'nbs-dev/metagentorch-dev.cfg'\n",
    "pfs = ProjectFileSystem(config_fname=p2dev_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running linux on local computer\n",
      "Device's home directory: /home/vtec\n",
      "Project file structure:\n",
      " - Root ........ /home/vtec/projects/bio/metagentorch \n",
      " - Data Dir .... /home/vtec/projects/bio/metagentorch/nbs-dev/data_dev \n",
      " - Notebooks ... /home/vtec/projects/bio/metagentorch/nbs\n"
     ]
    }
   ],
   "source": [
    "pfs = ProjectFileSystem()\n",
    "pfs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have setup this notebook to use the development data directory `metagentorch/nbs-dev/data_dev` and not the standard `metagentorch/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "assert pfs.data.name == 'data_dev' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each directory, a `readme.md` file or another `*.md` file can be added to provide a description of the directory content. \n",
    "\n",
    "These `readme.md` files can be conveniently accessed using the `.readme(path)` method available with the class `ProjectFileSystem` (from core module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `nbs-dev/data_dev`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Data directory for this package development \n",
       "This directory includes all  data required to validate and test this package code.\n",
       "\n",
       "```text\n",
       "data_dev\n",
       " |--- CNN_Virus_data\n",
       " |     |--- 50mer_ds_100_seq\n",
       " |     |--- 150mer_ds_100_seq\n",
       " |     |--- train_short\n",
       " |     |--- val_short\n",
       " |     |--- weight_of_classes\n",
       " |--- ncbi\n",
       " |     |--- infer_results\n",
       " |     |     |--- cnn_virus\n",
       " |     |     |--- csv\n",
       " |     |     |--- xlsx\n",
       " |     |     |--- testdb.db\n",
       " |     |--- refsequences\n",
       " |     |     |--- cov\n",
       " |     |     |     |--cov_virus_sequence_one_metadata.json\n",
       " |     |     |     |--sequences_two_no_matching_rule.fa\n",
       " |     |     |     |--another_sequence.fa\n",
       " |     |     |     |--cov_virus_sequences_two.fa\n",
       " |     |     |     |--cov_virus_sequences_two_metadata.json\n",
       " |     |     |     |--cov_virus_sequence_one.fa\n",
       " |     |     |     |--single_1seq_150bp\n",
       " |     |     |     |    |--single_1seq_150bp.fq\n",
       " |     |     |     |    |--single_1seq_150bp.aln\n",
       " |     |     |     |--paired_1seq_150bp\n",
       " |     |     |     |    |--paired_1seq_150bp2.aln\n",
       " |     |     |     |    |--paired_1seq_150bp2.fq\n",
       " |     |     |     |    |--paired_1seq_150bp1.fq \n",
       " |     |     |     |    |--paired_1seq_150bp1.aln \n",
       " |     |--- simreads\n",
       " |     |     |--- cov\n",
       " |     |     |     |--- paired_1seq_50bp\n",
       " |     |     |     |      |--- paired_1seq_50bp_1.aln\n",
       " |     |     |     |      |--- paired_1seq_50bp_1.fq\n",
       " |     |     |     |--- single_1seq_50bp\n",
       " |     |     |     |      |--- single_1seq_50bp_1.aln\n",
       " |     |     |     |      |--- single_1seq_50bp_1.fq\n",
       " |     |     |--- cov\n",
       " |     |     |     |--single_1seq_50bp\n",
       " |     |     |     |    |--single_1seq_50bp.aln\n",
       " |     |     |     |    |--single_1seq_50bp.fq\n",
       " |     |     |     |--single_1seq_150bp\n",
       " |     |     |     |    |--single_1seq_150bp.fq\n",
       " |     |     |     |    |--single_1seq_150bp.aln\n",
       " |     |     |     |--paired_1seq_150bp\n",
       " |     |     |     |    |--paired_1seq_150bp2.aln\n",
       " |     |     |     |    |--paired_1seq_150bp2.fq\n",
       " |     |     |     |    |--paired_1seq_150bp1.fq\n",
       " |     |     |     |    |--paired_1seq_150bp1.aln\n",
       " |--- saved           \n",
       " |--- readme.md               \n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pfs.readme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN Virus team provides training and validation/test datasets for their model. These datasets and the pretrained model parameters are are available on their Google drive shared directory [here](https://drive.google.com/open?id=1sj0-NCSKjLta_Geg6EMo26rChmtWcOiI). A set of shortened datasets are also available in the `data-dev` development data directory for testing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `nbs-dev/data_dev/CNN_Virus_data`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### CNN Virus data (development directory version)\n",
       "\n",
       "This directory includes a set of short data files used to test train and inference with the CNN Virus. \n",
       "\n",
       "#### File list and description:\n",
       "##### 50-mer \n",
       "50-mer reads and their labels, in *text format* with one line per sample. Each line consists of three components, separated by tabs: the 50-mer read or sequence, the virus species label and the position label:\n",
       "```text\n",
       "'TTACNAGCTCCAGTCTAAGATTGTAACTGGCCTTTTTAAAGATTGCTCTA    94    5\\n'\n",
       "``` \n",
       "Files:\n",
       "\n",
       "- `50mer_ds_100_seq`: small dataset with 100 reads\n",
       "- `5train_short`: small 1000-read subset from the original training dataset for experiments\n",
       "- `val_short`: small 500-read subset from the original validation dataset for experiments\n",
       "\n",
       "##### 150-mer\n",
       "150-mer reads and their labels in *text format* in a similar format as above:\n",
       "```text\n",
       "'TTCTTTCACCACCACAACCAGTCGGCCGTGGAGAGGCGTCGCCGCGTCTCGTTCGTCGAGGCCGATCGACTGCCGCATGAGAGCGGGTGGTATTCTTCCGAAGACGACGGAGACCGGGACGGTGATGAGGAAACTGGAGAGAGCCACAAC    6    0\\n'\n",
       "```\n",
       "Files:\n",
       "\n",
       "- `150mer_ds_100_reads`: small subset of 100 reads from original `ICTV_150mer_benchmarking` file\n",
       "\n",
       "##### Other files:\n",
       "\n",
       "- `virus_name_mapping`: mapping between virus species and their numerical label\n",
       "- `weight_of_classes`:  weights for each virus species class in the training dataset\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pfs.readme(pfs.data/'CNN_Virus_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class OriginalLabels:\n",
    "    \"\"\"Converts between labels and species name as per original training dataset\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        p2mapping:Path|None = None   # Path to the mapping file. Uses `virus_name_mapping` by default\n",
    "        ):\n",
    "        if p2mapping is None:\n",
    "            p2mapping = ProjectFileSystem().data / 'CNN_Virus_data/virus_name_mapping'\n",
    "        else:\n",
    "            p2mapping = safe_path(p2mapping)\n",
    "        if not p2mapping.is_file(): raise FileNotFoundError(f\"Mapping file not found at {p2mapping}\")\n",
    "        df = pd.read_csv(p2mapping, sep='\\t', header=None, names=['species', 'label'])\n",
    "        self._label2species = df['species'].to_list()\n",
    "        self._label2species.append('Unknown Virus Species')\n",
    "        self._species2label = {specie:label for specie, label in zip(df['species'], df['label'])}\n",
    "        self._species2label['Unknown Virus Species'] = len(self._label2species)\n",
    "\n",
    "    def search(self, s:str  # string to search through all original virus species\n",
    "                       ):\n",
    "        \"\"\"Prints all species whose name contains the passed string, with their numerical label\"\"\"\n",
    "        print('\\n'.join([f\"{k}. Label: {v}\" for k,v in self._species2label.items() if s in k.lower()]))\n",
    "\n",
    "    def label2species(self, n:int # label to convert to species name\n",
    "                      ):\n",
    "        \"\"\"Converts a numerical label into the correpsonding species label\"\"\"\n",
    "        return self._label2species[n]\n",
    "\n",
    "    def species2label(self, s:str  # string to convert to label\n",
    "                      ):\n",
    "        \"\"\"Converts a species name into the corresponding label number\"\"\"\n",
    "        return self._species2label[s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original data include 187 viruses, with label from 0 to 186. \n",
    "\n",
    "With the class method `.label2species(n)` and `.species2label(species)` we can convert between the label and the species name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 -> Variola_virus\n",
      " 94 -> Middle_East_respiratory_syndrome-related_coronavirus\n",
      "117 -> Severe_acute_respiratory_syndrome-related_coronavirus\n",
      "118 -> Yellow_fever_virus\n"
     ]
    }
   ],
   "source": [
    "species = OriginalLabels()\n",
    "for n in [0, 94, 117, 118]:\n",
    "    print(f\"{n:3d} -> {species.label2species(n)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variola_virus        -> 0\n",
      "Yellow_fever_virus   -> 118\n"
     ]
    }
   ],
   "source": [
    "for s in ['Variola_virus', 'Yellow_fever_virus']:\n",
    "    print(f\"{s:20s} -> {species.species2label(s)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking for a numerical specie label it is often more convenient to use a partial name, and the method `.search(species)` because we do not need to know the full specie name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L59){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### OriginalLabels.search\n",
       "\n",
       ">      OriginalLabels.search (s:str)\n",
       "\n",
       "*Prints all species whose name contains the passed string, with their numerical label*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| s | str | string to search through all original virus species |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L59){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### OriginalLabels.search\n",
       "\n",
       ">      OriginalLabels.search (s:str)\n",
       "\n",
       "*Prints all species whose name contains the passed string, with their numerical label*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| s | str | string to search through all original virus species |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(OriginalLabels.search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sandfly_fever_Naples_phlebovirus. Label: 35\n",
      "Crimean-Congo_hemorrhagic_fever_orthonairovirus. Label: 76\n",
      "Yellow_fever_virus. Label: 118\n",
      "Rift_Valley_fever_phlebovirus. Label: 156\n"
     ]
    }
   ],
   "source": [
    "species.search('fever')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCBI reference sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simulate reads using many reference sequences from the NCBI GenBank database. We group all reference sequences as well as all reads simulated from these reference sequences under the `ncbi` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfs = ProjectFileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `nbs-dev/data_dev/ncbi`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### NCBI Data\n",
       "\n",
       "This directory includes all data related to the work done with reference sequences from NCBI. \n",
       "\n",
       "The data is organized in the following subfolders:\n",
       "\n",
       "- `refsequences`: reference CoV sequences downloaded from NCBI, and related metadata\n",
       "- `simreads`: all data from simulated reads, using ART Illumina simulator and the reference sequences\n",
       "- `infer_results`: results from the inference using models with the simulated reads\n",
       "- `ds`: datasets in proper format for training or inference/prediction using the CNN Virus model\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pfs.readme(pfs.data / 'ncbi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `nbs-dev/data_dev/ncbi/refsequences`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No markdown file in this folder\n"
     ]
    }
   ],
   "source": [
    "pfs.readme(pfs.data / 'ncbi/refsequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `nbs-dev/data_dev/ncbi/simreads`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### NCBI simulated reads\n",
       "This directory includes all sets of simulated read sequence files generated from NCBI viral sequences using  ARC Illumina. \n",
       "\n",
       "```ascii\n",
       "this-directory\n",
       "    |--cov\n",
       "    |    |\n",
       "    |    |--single_10seq_50bp\n",
       "    |    |    |--single_10seq_50bp.fq\n",
       "    |    |    |--single_10seq_50bp.alnEnd\n",
       "    |    |-- ...\n",
       "    |    |--single_100seq_150bp\n",
       "    |    |    |--single_100seq_150bp.fq\n",
       "    |    |    |--single_100seq_150bp.aln\n",
       "    |    |--paired_100seq_50bp\n",
       "    |    |    |--paired_100seq_50bp2.aln\n",
       "    |    |    |--paired_100seq_50bp1.aln\n",
       "    |    |    |--paired_100seq_50bp2.fq\n",
       "    |    |    |--paired_100seq_50bp1.fq\n",
       "    |    |-- ...\n",
       "    |    |\n",
       "    |---yf\n",
       "    |    |\n",
       "    |    |--yf_AY968064-single-150bp\n",
       "    |    |    |--yf_AY968064-single-1seq-150bp.fq\n",
       "    |    |    |--yf_AY968064-single-1seq-150bp.aln\n",
       "    |    |\n",
       "    |--mRhiFer1\n",
       "    |    |--mRhiFer1_v1.p.dna_rm.primary_assembly.1\n",
       "    |    |    |--mRhiFer1_v1.p.dna_rm.primary_assembly.1.fq\n",
       "    |    |    |--mRhiFer1_v1.p.dna_rm.primary_assembly.1.aln\n",
       "    |    |\n",
       "\n",
       "```\n",
       "\n",
       "This directory includes several subdirectories, each for one virus, e.g. `cov` for corona, `yf` for yellow fever.\n",
       "\n",
       "In each virus subdirectory, several simreads directory includes simulated reads with various parameters, named as `<method>_<nb-seq>_<nb-bp>` where\"\n",
       "- `<method>` is either `single` or `paired` depending on the simulation method\n",
       "- `<nb-seq>` is the number of reference sequences used for simulation, and refers to the `fa` file used\n",
       "- `<nb-bp>` is the number of base pairs used to simulate reads\n",
       "\n",
       "\n",
       "Each sub-directory includes simreads files made using a simulation method and a specific number of reference sequences.\n",
       "- `xxx.fq` and `xxx.aln` files when method is `single`\n",
       "- `xxx1.fq`, `xxx2.fq`, `xxx1.aln` and `xxx2.aln` files when method is `paired`.\n",
       "\n",
       "Example:\n",
       "- `paired_10seq_50bp` means that the simreads were generated by using the `paired` method to simulate 50-bp reads, and using the `fa` file `cov_virus_sequences_010-seqs.fa`.\n",
       "- `single_100seq_50bp` means that the simreads were generated by using the `single` method to simulate 50-bp reads, and using the `fa` file `cov_virus_sequences_100-seqs.fa`. Note that this generated 20,660,104 reads !\n",
       "\n",
       "#### Simread file formats\n",
       "\n",
       "Simulated reads information is split between two files:\n",
       "- **FASTQ** (`.fq`) files providing the read sequences and their ASCII quality scores\n",
       "- **ALN** (`.aln`) files with alignment information\n",
       "\n",
       "##### FASTQ (`.fq`)\n",
       "FASTQ files generated by ART Illumina have the following structure (showing 5 reads), with 4 lines for each read:\n",
       "\n",
       "```ascii\n",
       "@2591237:ncbi:1-60400\n",
       "ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n",
       "+\n",
       "CCCBCGFGBGGGGGGGBGGGGGGGGG>GGG1G=/GGGGGGGGGGGGGGGG\n",
       "@2591237:ncbi:1-60399\n",
       "GATCAATGTGGCATCTACAATACAGACAGCATGAAGCACCACCAAAGGAC\n",
       "+\n",
       "BCBCCFGGGGGGGG1CGGGG<GGBGGGGGFGCGGGGGGDGGG/GG1GGGG\n",
       "@2591237:ncbi:1-60398\n",
       "ATCTACCAGTGGTAGATGGGTTCTTAATAATGAACATTATAGAGCTCTAC\n",
       "+\n",
       "CCCCCGGGEGG1GGF1G/GGEGGGGGGGGGGGGFFGGGGGGGGGGDGGDG\n",
       "@2591237:ncbi:1-60397\n",
       "CGTAAAGTAGAGGCTGTATGGTAGCTAGCACAAATGCCAGCACCAATAGG\n",
       "+\n",
       "BCCCCGGGFGGGGGGFGGGGFGG1GGGGGGG>GG1GGGGGGGGGGE<GGG\n",
       "@2591237:ncbi:1-60396\n",
       "GGTATCGGGTATCTCCTGCATCAATGCAAGGTCTTACAAAGATAAATACT\n",
       "+\n",
       "CBCCCGGG@CGGGGGGGGGGGG=GFGGGGDGGGFG1GGGGGGGG@GGGGG\n",
       "```\n",
       "The following information can be parsed from the each read sequence in the FASTQ file:\n",
       "\n",
       "- Line 1: `readid`, a unique ID for the read, under for format `@readid` \n",
       "- Line 2: `readseq`, the sequence of the read\n",
       "- Line 3: a separator `+`\n",
       "- Line 4: `read_qscores`, the base quality scores encoded in ASCII \n",
       "\n",
       "Example:\n",
       "```\n",
       "@2591237:ncbi:1-60400\n",
       "ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n",
       "+\n",
       "CCCBCGFGBGGGGGGGBGGGGGGGGG>GGG1G=/GGGGGGGGGGGGGGGG\n",
       "```\n",
       "- `readid` = `2591237:ncbi:1-60400`\n",
       "- `readseq` = `ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG`, a 50 bp read\n",
       "- `read_qscores` = `CCCBCGFGBGGGGGGGBGGGGGGGGG>GGG1G=/GGGGGGGGGGGGGGGG`\n",
       "\n",
       "\n",
       "#### ALN (`.aln`) \n",
       "ALN files generated by ART Illumina consist of :\n",
       "- a header with the ART-Ilumina command used for the simulation (`@CM`) and info on each of the reference sequences used for the simulations (`@SQ`). Header always starts with `##ART_Illumina` and ends with `##Header End` :\n",
       "- the body with 3 lines for each read:\n",
       "    1. definition line with `readid`, \n",
       "        - reference sequence identification number `refseqid`, \n",
       "        - the position in the read in the reference sequence `aln_start_pos` \n",
       "        - the strand the read was taken from `ref_seq_strand`. `+` for coding strand and `-` for template strand\n",
       "    2. aligned reference sequence, that is the sequence segment in the original reference corresponding to the read\n",
       "    3. aligned read sequence, that is the simmulated read sequence, where each bp corresponds to the reference sequence bp in the same position.\n",
       "\n",
       "Example of a ALN file generated by ART Illumina (showing 5 reads):\n",
       "\n",
       "```ascii\n",
       "##ART_Illumina    read_length    50\n",
       "@CM    /bin/art_illumina -i /home/vtec/projects/bio/metagentools/data/cov_data/cov_virus_sequences_ten.fa -ss HS25 -l 50 -f 100 -o /home/vtec/projects/bio/metagentools/data/cov_simreads/single_10seq_50bp/single_10seq_50bp -rs 1674660835\n",
       "@SQ    2591237:ncbi:1 1   MK211378    2591237    ncbi    1     Coronavirus BtRs-BetaCoV/YN2018D    30213\n",
       "@SQ    11128:ncbi:2   2   LC494191    11128    ncbi    2     Bovine coronavirus    30942\n",
       "@SQ    31631:ncbi:3   3   KY967361    31631    ncbi    3     Human coronavirus OC43        30661\n",
       "@SQ    277944:ncbi:4  4   LC654455    277944    ncbi    4     Human coronavirus NL63    27516\n",
       "@SQ    11120:ncbi:5   5   MN987231    11120    ncbi    5     Infectious bronchitis virus    27617\n",
       "@SQ    28295:ncbi:6   6   KU893866    28295    ncbi    6     Porcine epidemic diarrhea virus    28043\n",
       "@SQ    28295:ncbi:7   7   KJ645638    28295    ncbi    7     Porcine epidemic diarrhea virus    27998\n",
       "@SQ    28295:ncbi:8   8   KJ645678    28295    ncbi    8     Porcine epidemic diarrhea virus    27998\n",
       "@SQ    28295:ncbi:9   9   KR873434    28295    ncbi    9     Porcine epidemic diarrhea virus    28038\n",
       "@SQ    1699095:ncbi:10 10  KT368904    1699095    ncbi    10     Camel alphacoronavirus    27395\n",
       "##Header End\n",
       ">2591237:ncbi:1    2591237:ncbi:1-60400    14770    +\n",
       "ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n",
       "ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n",
       ">2591237:ncbi:1    2591237:ncbi:1-60399    17012    -\n",
       "GATCAATGTGGCATCTACAATACAGACAGCATGAAGCACCACCAAAGGAC\n",
       "GATCAATGTGGCATCTACAATACAGACAGCATGAAGCACCACCAAAGGAC\n",
       ">2591237:ncbi:1    2591237:ncbi:1-60398    9188    +\n",
       "ATCTACCAGTGGTAGATGGGTTCTTAATAATGAACATTATAGAGCTCTAC\n",
       "ATCTACCAGTGGTAGATGGGTTCTTAATAATGAACATTATAGAGCTCTAC\n",
       ".....\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pfs.readme(pfs.data / 'ncbi/simreads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model related data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfs = ProjectFileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `nbs-dev/data_dev/saved`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Saved data related to models\n",
       "\n",
       "This directory includes all data related to models and saved:\n",
       "- saved model parameters\n",
       "- saved datasets\n",
       "\n",
       "For example:\n",
       "- `cnn_virus_original/pretrained_model.h5` is the saved model parameters for the CNN Virus model\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pfs.readme(pfs.data / 'saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing sequence files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following classes make it easier to read and parse files of different formats into their underlying components to generated the training, validation, testing and inference datasets for the model.\n",
    "\n",
    "Each class inherits from `TextFileBaseReader` and adds:\n",
    "\n",
    "- One or several text parsing method(s) to parse metadata according to a specific format\n",
    "- A file parsing method to extract metadata from all elements in the file, returning it as a key:value dictionary and optionally save the metadata as a json file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FASTA file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extension of `TextFileBaseReader` class for fasta sequence files.\n",
    "\n",
    "Structure of a FASTA sequence file:\n",
    "```ascii\n",
    ">definition line - format varies from dataset to dataset\n",
    "sequence line: sequence of bases\n",
    "```\n",
    "Example for the NCBI datasets:\n",
    "```ascii\n",
    ">seqid accession taxonomyid source seqnb organism\n",
    "TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAAT ...\n",
    ">2591237:ncbi:1 MK211378\t2591237\tncbi\t1 Coronavirus BtRs-BetaCoV/YN2018D\n",
    "TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAAT ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2591237:ncbi:1\t1\tMK211378\t2591237\tncbi\tCoronavirus BtRs-BetaCoV/YN2018D ...\n",
      "TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAAT ...\n",
      ">11128:ncbi:2\t2\tLC494191\t11128\tncbi\tBovine coronavirus ...\n",
      "CATCCCGCTTCACTGATCTCTTGTTAGATCTTTTCATAATCTAAACTTTATAAAAACATCCACTCCCTGTAGTCTATGCC ...\n"
     ]
    }
   ],
   "source": [
    "p2fasta = pfs.data / 'ncbi/refsequences/cov/cov_virus_sequences_two.fa'\n",
    "\n",
    "fasta = TextFileBaseReader(p2fasta, nlines=1)\n",
    "for i, t in enumerate(fasta):\n",
    "    txt = t.replace('\\n', '')[:80] + ' ...'\n",
    "    print(f\"{txt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FastaFileReader(TextFileBaseReader):\n",
    "    \"\"\"Wrap a FASTA file and retrieve its content in raw format and parsed format\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str|Path,  # path to the Fasta file\n",
    "    ):\n",
    "        super().__init__(path, nlines=1)\n",
    "        self.text_to_parse_key = 'definition line'\n",
    "        self.set_parsing_rules(verbose=False)\n",
    "        \n",
    "    def __next__(self)-> dict[str, str]:   # `{'definition line': text in dfn line, 'sequence': full sequence as str}` \n",
    "        \"\"\"Return one definition line and the corresponding sequence\"\"\"\n",
    "        lines = []\n",
    "        for i in range(2):\n",
    "            lines.append(self._safe_readline())\n",
    "        dfn_line = lines[0].replace('\\n', '')   #remove the next line symbol at the end of the line\n",
    "        sequence = lines[1].replace('\\n', '')   #remove the next line symbol at the end of the line\n",
    "        self._chunk_nb = self._chunk_nb + 1\n",
    "        return {'definition line':dfn_line, 'sequence':f\"{sequence}\"}\n",
    "\n",
    "    @property\n",
    "    def read_nb(self)-> int:\n",
    "        return self._chunk_nb\n",
    "    \n",
    "    def print_first_chunks(\n",
    "        self, \n",
    "        nchunks:int=3,  # number of chunks to print out\n",
    "    ):\n",
    "        \"\"\"Print the first `nchunks` chunks of text from the file\"\"\"\n",
    "        self.reset_iterator()\n",
    "        for i, seq_dict in enumerate(self.__iter__()):\n",
    "            print(f\"\\nSequence {i+1}:\")\n",
    "            print(seq_dict['definition line'])\n",
    "            print(f\"{seq_dict['sequence'][:80]} ...\")\n",
    "            if i >= nchunks-1: break\n",
    "        self.reset_iterator()\n",
    "            \n",
    "    def parse_file(\n",
    "        self,\n",
    "        add_seq :bool=False,     # When True, add the full sequence to the parsed metadata dictionary\n",
    "        save_json: bool=False    # When True, save the file metadata as a json file of same stem name\n",
    "    )-> dict[str]:               # Metadata as Key/Values pairs\n",
    "        \"\"\"Read fasta file and return a dictionary with definition line metadata and optionally sequences\"\"\"\n",
    "    \n",
    "        self.reset_iterator()\n",
    "        parsed = {}\n",
    "        for d in self:\n",
    "            dfn_line = d['definition line']\n",
    "            seq = d['sequence']\n",
    "            metadata = self._parse_text_fn(dfn_line, self.re_pattern, self.re_keys)\n",
    "            if add_seq: metadata['sequence'] = seq         \n",
    "            parsed[metadata['seqid']] = metadata\n",
    "                        \n",
    "        if save_json:\n",
    "            p2json = self.path.parent / f\"{self.path.stem}_metadata.json\"\n",
    "            with open(p2json, 'w') as fp:\n",
    "                json.dump(parsed, fp, indent=4)\n",
    "                print(f\"Metadata for '{self.path.name}'> saved as <{p2json.name}> in  \\n{p2json.parent.absolute()}\\n\")\n",
    "\n",
    "        return parsed\n",
    "\n",
    "    def review(self):\n",
    "        \"\"\"Prints the first and last sequences and metadata in the fasta file and returns the nb or sequences\"\"\"\n",
    "\n",
    "        self.reset_iterator()\n",
    "        for i, seq in enumerate(self):\n",
    "            if i == 0:\n",
    "                first_dfn = seq['definition line']\n",
    "                first_sequence = seq['sequence'][:80] + ' ...'\n",
    "                first_meta = self.parse_text(seq['definition line'])\n",
    "        print(f\"There {'is' if i == 0 else 'are'} {i+1} sequences in this file\")\n",
    "        print('\\nFirst Sequence:')\n",
    "        print(first_dfn)\n",
    "        print(first_sequence)\n",
    "        print(first_meta)\n",
    "        if i != 0:\n",
    "            print('\\nLast Sequence:')\n",
    "            print(seq['definition line'])\n",
    "            print(seq['sequence'][:80] + ' ...')\n",
    "            print(self.parse_text(seq['definition line']))\n",
    "        return i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L75){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader\n",
       "\n",
       ">      FastaFileReader (path:str|pathlib.Path)\n",
       "\n",
       "*Wrap a FASTA file and retrieve its content in raw format and parsed format*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| pathlib.Path | path to the Fasta file |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L75){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader\n",
       "\n",
       ">      FastaFileReader (path:str|pathlib.Path)\n",
       "\n",
       "*Wrap a FASTA file and retrieve its content in raw format and parsed format*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| pathlib.Path | path to the Fasta file |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastaFileReader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an iterator, `FastaFileReader` returns a `dict` at each step, as follows:\n",
    "```python\n",
    "{\n",
    "    'definition line': 'string in file as the definition line for the sequence',\n",
    "    'sequence': 'the full sequence'\n",
    "}\n",
    "```\n",
    "\n",
    "Illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2591237:ncbi:1\t1\tMK211378\t2591237\tncbi\tCoronavirus BtRs-BetaCoV/YN2018D ...\n",
      "TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAAT ...\n"
     ]
    }
   ],
   "source": [
    "p2fasta = pfs.data / 'ncbi/refsequences/cov/cov_virus_sequences_two.fa'\n",
    "fasta = FastaFileReader(p2fasta)\n",
    "iteration_output = next(fasta)\n",
    "\n",
    "print(iteration_output['definition line'][:80], '...')\n",
    "print(iteration_output['sequence'][:80], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output type :     <class 'dict'>\n",
      "keys :            dict_keys(['definition line', 'sequence'])\n",
      "definition line : >2591237:ncbi:1\t1\tMK211378\t2591237\tncbi\tCoronavirus BtRs-BetaCoV/YN2018D ...'\n",
      "sequence :       'TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAATCTGTGTAGCTGTCGCTCGGC ...'\n"
     ]
    }
   ],
   "source": [
    "print(f\"output type :     {type(iteration_output)}\")\n",
    "print(f\"keys :            {iteration_output.keys()}\")\n",
    "print(f\"definition line : {iteration_output['definition line'][:80]} ...'\")\n",
    "print(f\"sequence :       '{iteration_output['sequence'][:100]} ...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `definition line` is a string, with tab separated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>2591237:ncbi:1\\t1\\tMK211378\\t2591237\\tncbi\\tCoronavirus BtRs-BetaCoV/YN2018D'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(iteration_output['definition line'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L136){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader.review\n",
       "\n",
       ">      FastaFileReader.review ()\n",
       "\n",
       "*Prints the first and last sequences and metadata in the fasta file and returns the nb or sequences*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L136){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader.review\n",
       "\n",
       ">      FastaFileReader.review ()\n",
       "\n",
       "*Prints the first and last sequences and metadata in the fasta file and returns the nb or sequences*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastaFileReader.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 sequences in this file\n",
      "\n",
      "First Sequence:\n",
      ">2591237:ncbi:1\t1\tMK211378\t2591237\tncbi\tCoronavirus BtRs-BetaCoV/YN2018D\n",
      "TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAAT ...\n",
      "{'accession': 'MK211378', 'organism': 'Coronavirus BtRs-BetaCoV/YN2018D', 'seqid': '2591237:ncbi:1', 'seqnb': '1', 'source': 'ncbi', 'taxonomyid': '2591237'}\n",
      "\n",
      "Last Sequence:\n",
      ">11128:ncbi:2\t2\tLC494191\t11128\tncbi\tBovine coronavirus\n",
      "CATCCCGCTTCACTGATCTCTTGTTAGATCTTTTCATAATCTAAACTTTATAAAAACATCCACTCCCTGTAGTCTATGCC ...\n",
      "{'accession': 'LC494191', 'organism': 'Bovine coronavirus', 'seqid': '11128:ncbi:2', 'seqnb': '2', 'source': 'ncbi', 'taxonomyid': '11128'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_seqs = fasta.review()\n",
    "nb_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L99){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader.print_first_chunks\n",
       "\n",
       ">      FastaFileReader.print_first_chunks (nchunks:int=3)\n",
       "\n",
       "*Print the first `nchunks` chunks of text from the file*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| nchunks | int | 3 | number of chunks to print out |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L99){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader.print_first_chunks\n",
       "\n",
       ">      FastaFileReader.print_first_chunks (nchunks:int=3)\n",
       "\n",
       "*Print the first `nchunks` chunks of text from the file*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| nchunks | int | 3 | number of chunks to print out |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastaFileReader.print_first_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is convenient to quickly discover and explore new fasta files in raw text format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequence 1:\n",
      ">2591237:ncbi:1\t1\tMK211378\t2591237\tncbi\tCoronavirus BtRs-BetaCoV/YN2018D\n",
      "TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAAT ...\n",
      "\n",
      "Sequence 2:\n",
      ">11128:ncbi:2\t2\tLC494191\t11128\tncbi\tBovine coronavirus\n",
      "CATCCCGCTTCACTGATCTCTTGTTAGATCTTTTCATAATCTAAACTTTATAAAAACATCCACTCCCTGTAGTCTATGCC ...\n"
     ]
    }
   ],
   "source": [
    "fasta = FastaFileReader(p2fasta)\n",
    "fasta.print_first_chunks(nchunks=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class also provides methods to parse metadata from the file content (definition line, headers, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regex pattern is used for parsing metadata fom the definition lines in the reference sequence fasta file.\n",
    "\n",
    "Below, we parse the data from the definition line of our Corona virus NCBI dataset (rule `fasta_ncbi_std`):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence 1:\n",
    "\n",
    "- Definition Line:\n",
    "```ascii\n",
    ">2591237:ncbi:1 [MK211378]\t2591237\tncbi\t1 [MK211378] 2591237\tCoronavirus YN2018D\t\tscientific name\n",
    "```\n",
    "- Metadata:\n",
    "    - `seqid` = `2591237:ncbi:1`\n",
    "    - `taxonomyid` = `2591237`\n",
    "    - `source` = `ncbi`\n",
    "    - `seqnb` = `1`\n",
    "    - `accession` = `MK211378`\n",
    "    - `species` = `Coronavirus BtRs-BetaCoV/YN2018D`\n",
    "\n",
    "Sequence 2:\n",
    "\n",
    "- Definition Line\n",
    "```ascii\n",
    "    >11128:ncbi:2 [LC494191]\n",
    "```\n",
    "\n",
    "- Metadata:\n",
    "    - `seqid` = `11128:ncbi:2`\n",
    "    - `taxonomyid` = `11128`\n",
    "    - `source` = `ncbi`\n",
    "    - `seqnb` = `2`\n",
    "    - `accession` = `LC494191`\n",
    "    - `species` = `''`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FastaFileReader` offers:\n",
    "- `parse_text` a method to parse the metadata\n",
    "- an option to set a default \"parsing rule\" for one instance with `set_parsing_rules`.\n",
    "- `parse_file` a method to parse the metadata from all sequences in the file and save it as a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.parse_text\n",
       "\n",
       ">      TextFileBaseReader.parse_text (txt:str, pattern:str|None=None,\n",
       ">                                     keys:list[str]|None=None)\n",
       "\n",
       "*Parse text using regex pattern and keys. Return a metadata dictionary.\n",
       "\n",
       "The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
       "\n",
       "    {\n",
       "        'key_1': 'metadata 1',\n",
       "        'key_2': 'metadata 2',\n",
       "        ...\n",
       "    }*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| txt | str |  | text to parse |\n",
       "| pattern | str \\| None | None | If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex |\n",
       "| keys | list[str] \\| None | None | If None, uses standard regex list of keys, otherwise, uses passed list of keys (str) |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.parse_text\n",
       "\n",
       ">      TextFileBaseReader.parse_text (txt:str, pattern:str|None=None,\n",
       ">                                     keys:list[str]|None=None)\n",
       "\n",
       "*Parse text using regex pattern and keys. Return a metadata dictionary.\n",
       "\n",
       "The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
       "\n",
       "    {\n",
       "        'key_1': 'metadata 1',\n",
       "        'key_2': 'metadata 2',\n",
       "        ...\n",
       "    }*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| txt | str |  | text to parse |\n",
       "| pattern | str \\| None | None | If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex |\n",
       "| keys | list[str] \\| None | None | If None, uses standard regex list of keys, otherwise, uses passed list of keys (str) |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastaFileReader.parse_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the parser function with specifically defined `pattern` and `keys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2591237:ncbi:1\t1\tMK211378\t2591237\tncbi\tCoronavirus BtRs-BetaCoV/YN2018D\n"
     ]
    }
   ],
   "source": [
    "fasta = FastaFileReader(p2fasta)\n",
    "dfn_line, sequence = next(fasta).values()\n",
    "print(dfn_line.replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = r\"^>(?P<seqid>(?P<taxonomyid>\\d+):(?P<source>ncbi):(?P<seqnb>\\d*))[\\s\\t]*\\[(?P<accession>[\\w\\d]*)\\]([\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P=source)[\\s\\t]*(?P=seqnb)[\\s\\t]*\\[(?P=accession)\\][\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P<species>[\\w\\s\\-\\_\\/]*))?\"\n",
    "pattern = r\"^>(?P<seqid>(?P<taxonomyid>\\d+):(?P<source>ncbi):(?P<seqnb>\\d*))[\\s\\t]*(?P=seqnb)[\\s\\t](?P<accession>[\\w\\d]*)([\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P=source)[\\s\\t][\\s\\t]*(?P<organism>[\\w\\s\\-\\_\\/]*))?\"\n",
    "\n",
    "keys = 'seqid taxonomyid accession source seqnb organism'.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accession': 'MK211378',\n",
       " 'organism': 'Coronavirus BtRs-BetaCoV/YN2018D',\n",
       " 'seqid': '2591237:ncbi:1',\n",
       " 'seqnb': '1',\n",
       " 'source': 'ncbi',\n",
       " 'taxonomyid': '2591237'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasta.parse_text(dfn_line, pattern=pattern, keys=keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a `FastaFileReader` instance is created, all existing rules in the file `default_parsing_rules.json` are tested on the first definition line of the fasta file and the one rule that parses the most matches will be selected automatically and saved in instance attributes `re_rule_name`, `re_pattern` and `re_keys`. \n",
    "\n",
    "`parse_file` extract metadata from each definition line in the fasta file and return a dictionary with all metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasta_ncbi_std\n",
      "^>(?P<seqid>(?P<taxonomyid>\\d+):(?P<source>ncbi):(?P<seqnb>\\d*))[\\s\\t]*(?P=seqnb)[\\s\\t](?P<accession>[\\w\\d]*)([\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P=source)[\\s\\t][\\s\\t]*(?P<organism>[\\w\\s\\-\\_/]*))?\n",
      "['seqid', 'taxonomyid', 'source', 'accession', 'seqnb', 'organism']\n"
     ]
    }
   ],
   "source": [
    "print(fasta.re_rule_name)\n",
    "print(fasta.re_pattern)\n",
    "print(fasta.re_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accession': 'MK211378',\n",
       " 'organism': 'Coronavirus BtRs-BetaCoV/YN2018D',\n",
       " 'seqid': '2591237:ncbi:1',\n",
       " 'seqnb': '1',\n",
       " 'source': 'ncbi',\n",
       " 'taxonomyid': '2591237'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasta.parse_text(dfn_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When another fasta file, which has another definition line structure, is used, another parsing rule is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1 dna_rm:primary_assembly primary_assembly:mRhiFer1_v1.p:1:1:124933378:1 REF\n"
     ]
    }
   ],
   "source": [
    "p2other = pfs.data / 'ncbi/refsequences/cov/another_sequence.fa'\n",
    "assert p2other.is_file()\n",
    "\n",
    "it2 = FastaFileReader(path=p2other)\n",
    "\n",
    "dfn_line, sequence = next(it2).values()\n",
    "print(dfn_line.replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasta_rhinolophus_ferrumequinum\n",
      "^>\\d[\\s\\t](?P<seq_type>dna_rm):(?P<id_type>[\\w\\_]*)[\\s\\w](?P=id_type):(?P<assy>[\\w\\d\\_]*)\\.(?P<seq_level>[\\w]*):\\d*:\\d*:(?P<taxonomy>\\d*):(?P<id>\\d*)[\\s\t]REF$\n",
      "['seq_type', 'id_type', 'assy', 'seq_level', 'taxonomy', 'id']\n"
     ]
    }
   ],
   "source": [
    "print(it2.re_rule_name)\n",
    "print(it2.re_pattern)\n",
    "print(it2.re_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'assy': 'mRhiFer1_v1',\n",
      " 'id': '1',\n",
      " 'id_type': 'primary_assembly',\n",
      " 'seq_level': 'p',\n",
      " 'seq_type': 'dna_rm',\n",
      " 'taxonomy': '124933378'}\n"
     ]
    }
   ],
   "source": [
    "pprint(it2.parse_text(dfn_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This rule selection is performed by the class method `set_parsing_rule`. The method can also be called with specific `pattern` and `keys` to force parsing rule not yet saved in the json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.set_parsing_rules\n",
       "\n",
       ">      TextFileBaseReader.set_parsing_rules (pattern:str|None=None,\n",
       ">                                            keys:list[str]|None=None,\n",
       ">                                            verbose:bool=False)\n",
       "\n",
       "*Set the standard regex parsing rule for the file.\n",
       "\n",
       "Rules can be set:\n",
       "\n",
       "1. manually by passing specific custom values for `pattern` and `keys`\n",
       "2. automatically, by testing all parsing rules saved in `parsing_rule.json` \n",
       "\n",
       "Automatic selection of parsing rules works by testing each rule saved in `parsing_rule.json` on the first \n",
       "definition line of the fasta file, and selecting the one rule that generates the most metadata matches.\n",
       "\n",
       "Rules consists of two parameters:\n",
       "\n",
       "- The regex pattern including one `group` for each metadata item, e.g `(?P<group_name>regex_code)`\n",
       "- The list of keys, i.e. the list with the name of each regex groups, used as key in the metadata dictionary\n",
       "\n",
       "This method updates the three following class attributes: `re_rule_name`, `re_pattern`, `re_keys`*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| None | None | regex pattern to apply to parse the text, search in parsing rules json if None |\n",
       "| keys | list[str] \\| None | None | list of keys/group for regex, search in parsing rules json if None |\n",
       "| verbose | bool | False | when True, provides information on each rule |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.set_parsing_rules\n",
       "\n",
       ">      TextFileBaseReader.set_parsing_rules (pattern:str|None=None,\n",
       ">                                            keys:list[str]|None=None,\n",
       ">                                            verbose:bool=False)\n",
       "\n",
       "*Set the standard regex parsing rule for the file.\n",
       "\n",
       "Rules can be set:\n",
       "\n",
       "1. manually by passing specific custom values for `pattern` and `keys`\n",
       "2. automatically, by testing all parsing rules saved in `parsing_rule.json` \n",
       "\n",
       "Automatic selection of parsing rules works by testing each rule saved in `parsing_rule.json` on the first \n",
       "definition line of the fasta file, and selecting the one rule that generates the most metadata matches.\n",
       "\n",
       "Rules consists of two parameters:\n",
       "\n",
       "- The regex pattern including one `group` for each metadata item, e.g `(?P<group_name>regex_code)`\n",
       "- The list of keys, i.e. the list with the name of each regex groups, used as key in the metadata dictionary\n",
       "\n",
       "This method updates the three following class attributes: `re_rule_name`, `re_pattern`, `re_keys`*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| None | None | regex pattern to apply to parse the text, search in parsing rules json if None |\n",
       "| keys | list[str] \\| None | None | list of keys/group for regex, search in parsing rules json if None |\n",
       "| verbose | bool | False | when True, provides information on each rule |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastaFileReader.set_parsing_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definition line: '>2591237:ncbi:1\t1\tMK211378\t2591237\tncbi\tCoronavirus BtRs-BetaCoV/YN2018'\n"
     ]
    }
   ],
   "source": [
    "fasta = FastaFileReader(p2fasta)\n",
    "dfn_line, sequence = next(fasta).values()\n",
    "print(f\"definition line: '{dfn_line[:-1]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic parsing works by testing each saved rule for the value of `definition line` in the first sequence in the fasta file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key for text to parse: definition line\n",
      "\n",
      "Text to parse for testing (extracted from first iteration):\n",
      ">2591237:ncbi:1\t1\tMK211378\t2591237\tncbi\tCoronavirus BtRs-BetaCoV/YN2018D\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_ncbi_std> generated 6 matches\n",
      "--------------------------------------------------------------------------------\n",
      "^>(?P<seqid>(?P<taxonomyid>\\d+):(?P<source>ncbi):(?P<seqnb>\\d*))[\\s\\t]*(?P=seqnb)[\\s\\t](?P<accession>[\\w\\d]*)([\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P=source)[\\s\\t][\\s\\t]*(?P<organism>[\\w\\s\\-\\_/]*))?\n",
      "['seqid', 'taxonomyid', 'source', 'accession', 'seqnb', 'organism']\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fastq_art_illumina_ncbi_std> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <aln_art_illumina_ncbi_std> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <aln_art_illumina-refseq-ncbi-std> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_ncbi_cov> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_rhinolophus_ferrumequinum> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Selected rule with most matches: fasta_ncbi_std\n"
     ]
    }
   ],
   "source": [
    "print(f\"key for text to parse: {fasta.text_to_parse_key}\\n\")\n",
    "fasta.reset_iterator()\n",
    "print('Text to parse for testing (extracted from first iteration):')\n",
    "print(next(fasta)[fasta.text_to_parse_key])\n",
    "print()\n",
    "fasta.set_parsing_rules(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no saved rule generates a match, `re_rule_name`, `re_pattern` and `re_keys` remain `None` and a warning message is issued to ask user to add a parsing rule manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vtec/projects/bio/metagentorch/metagentorch/core.py:678: UserWarning: \n",
      "        None of the saved parsing rules were able to extract metadata from the first line in this file.\n",
      "        You must set a custom rule (pattern + keys) before parsing text, by using:\n",
      "            `self.set_parsing_rules(custom_pattern, custom_list_of_keys)`\n",
      "                \n",
      "  warnings.warn(msg, category=UserWarning)\n"
     ]
    }
   ],
   "source": [
    "p2nomatch = pfs.data / 'ncbi/refsequences/cov/sequences_two_no_matching_rule.fa'\n",
    "fasta2 = FastaFileReader(p2nomatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasta2.re_rule_name is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we still can set a standard rule manually, by passing a re pattern and the corresponding list of keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Rule\n",
      "^>(?P<seqid>(?P<taxonomyid>\\d+):(?P<source>ncbi):(?P<seqnb>\\d*))\\s*(?P<text>[\\w\\s]*)$\n",
      "['seqid', 'taxonomyid', 'source', 'seqnb', 'text']\n"
     ]
    }
   ],
   "source": [
    "pat = r\"^>(?P<seqid>(?P<taxonomyid>\\d+):(?P<source>ncbi):(?P<seqnb>\\d*))\\s*(?P<text>[\\w\\s]*)$\"\n",
    "keys = \"seqid taxonomyid source seqnb text\".split()\n",
    "fasta2.set_parsing_rules(pattern=pat, keys=keys)\n",
    "\n",
    "print(fasta2.re_rule_name)\n",
    "print(fasta2.re_pattern)\n",
    "print(fasta2.re_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definition line: '>2591237:ncbi:1 this sequence does not match any saved parsing rul'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'seqid': '2591237:ncbi:1',\n",
       " 'seqnb': '1',\n",
       " 'source': 'ncbi',\n",
       " 'taxonomyid': '2591237',\n",
       " 'text': 'this sequence does not match any saved parsing rule'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasta2.reset_iterator()\n",
    "dfn_line, sequence = next(fasta2).values()\n",
    "print(f\"definition line: '{dfn_line[:-1]}'\")\n",
    "fasta2.parse_text(dfn_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L112){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader.parse_file\n",
       "\n",
       ">      FastaFileReader.parse_file (add_seq:bool=False, save_json:bool=False)\n",
       "\n",
       "*Read fasta file and return a dictionary with definition line metadata and optionally sequences*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| add_seq | bool | False | When True, add the full sequence to the parsed metadata dictionary |\n",
       "| save_json | bool | False | When True, save the file metadata as a json file of same stem name |\n",
       "| **Returns** | **dict** |  | **Metadata as Key/Values pairs** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L112){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader.parse_file\n",
       "\n",
       ">      FastaFileReader.parse_file (add_seq:bool=False, save_json:bool=False)\n",
       "\n",
       "*Read fasta file and return a dictionary with definition line metadata and optionally sequences*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| add_seq | bool | False | When True, add the full sequence to the parsed metadata dictionary |\n",
       "| save_json | bool | False | When True, save the file metadata as a json file of same stem name |\n",
       "| **Returns** | **dict** |  | **Metadata as Key/Values pairs** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastaFileReader.parse_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'11128:ncbi:2': {'accession': 'LC494191',\n",
      "                  'organism': 'Bovine coronavirus',\n",
      "                  'seqid': '11128:ncbi:2',\n",
      "                  'seqnb': '2',\n",
      "                  'source': 'ncbi',\n",
      "                  'taxonomyid': '11128'},\n",
      " '2591237:ncbi:1': {'accession': 'MK211378',\n",
      "                    'organism': 'Coronavirus BtRs-BetaCoV/YN2018D',\n",
      "                    'seqid': '2591237:ncbi:1',\n",
      "                    'seqnb': '1',\n",
      "                    'source': 'ncbi',\n",
      "                    'taxonomyid': '2591237'}}\n"
     ]
    }
   ],
   "source": [
    "fasta = FastaFileReader(p2fasta)\n",
    "pprint(fasta.parse_file())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for 'cov_virus_sequences_two.fa'> saved as <cov_virus_sequences_two_metadata.json> in  \n",
      "/home/vtec/projects/bio/metagentorch/nbs-dev/data_dev/ncbi/refsequences/cov\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasta.parse_file(save_json=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aln_art_illumina-refseq-ncbi-std': {'keys': 'refseqid '\n",
      "                                              'reftaxonomyid '\n",
      "                                              'refsource '\n",
      "                                              'refseqnb '\n",
      "                                              'refseq_accession '\n",
      "                                              'organism '\n",
      "                                              'refseq_length',\n",
      "                                      'pattern': '^@SQ[\\\\t\\\\s]*(?P<refseqid>(?P<reftaxonomyid>\\\\d*):(?P<refsource>\\\\w*):(?P<refseqnb>\\\\d*))[\\\\t\\\\s]*(?P=refseqnb)[\\\\t\\\\s]*(?P<refseq_accession>[\\\\w\\\\d]*)[\\\\t\\\\s]*(?P=reftaxonomyid)[\\\\t\\\\s]*(?P=refsource)[\\\\t\\\\s](?P<organism>.*)[\\\\t\\\\s](?P<refseq_length>\\\\d*)$'},\n",
      " 'aln_art_illumina_ncbi_std': {'keys': 'refseqid '\n",
      "                                       'reftaxonomyid '\n",
      "                                       'refsource '\n",
      "                                       'refseqnb '\n",
      "                                       'readid '\n",
      "                                       'readnb '\n",
      "                                       'aln_start_pos '\n",
      "                                       'refseq_strand',\n",
      "                               'pattern': '^>(?P<refseqid>(?P<reftaxonomyid>\\\\d*):(?P<refsource>\\\\w*):(?P<refseqnb>\\\\d*))(\\\\s|\\t'\n",
      "                                          ')*(?P<readid>(?P=reftaxonomyid):(?P=refsource):(?P=refseqnb)-(?P<readnb>\\\\d*(\\\\/\\\\d(-\\\\d)?)?))(\\\\s|\\\\t)(?P<aln_start_pos>\\\\d*)(\\\\s|\\\\t)(?P<refseq_strand>(-|\\\\+))$'},\n",
      " 'fasta_ncbi_cov': {'keys': 'seqid '\n",
      "                            'taxonomyid '\n",
      "                            'source '\n",
      "                            'accession '\n",
      "                            'seqnb '\n",
      "                            'organism',\n",
      "                    'pattern': '^>(?P<seqid>(?P<taxonomyid>\\\\d+):(?P<source>ncbi):(?P<seqnb>\\\\d*))[\\\\s\\\\t]*\\\\[(?P<accession>[\\\\w\\\\d]*)\\\\]([\\\\s\\\\t]*(?P=taxonomyid)[\\\\s\\\\t]*(?P=source)[\\\\s\\\\t]*(?P=seqnb)[\\\\s\\\\t]*\\\\[(?P=accession)\\\\][\\\\s\\\\t]*(?P=taxonomyid)[\\\\s\\\\t]*(?P<organism>[\\\\w\\\\s\\\\-\\\\_\\\\/]*))?'},\n",
      " 'fasta_ncbi_std': {'keys': 'seqid '\n",
      "                            'taxonomyid '\n",
      "                            'source '\n",
      "                            'accession '\n",
      "                            'seqnb '\n",
      "                            'organism',\n",
      "                    'pattern': '^>(?P<seqid>(?P<taxonomyid>\\\\d+):(?P<source>ncbi):(?P<seqnb>\\\\d*))[\\\\s\\\\t]*(?P=seqnb)[\\\\s\\\\t](?P<accession>[\\\\w\\\\d]*)([\\\\s\\\\t]*(?P=taxonomyid)[\\\\s\\\\t]*(?P=source)[\\\\s\\\\t][\\\\s\\\\t]*(?P<organism>[\\\\w\\\\s\\\\-\\\\_/]*))?'},\n",
      " 'fasta_rhinolophus_ferrumequinum': {'keys': 'seq_type '\n",
      "                                             'id_type '\n",
      "                                             'assy '\n",
      "                                             'seq_level '\n",
      "                                             'taxonomy '\n",
      "                                             'id',\n",
      "                                     'pattern': '^>\\\\d[\\\\s\\\\t](?P<seq_type>dna_rm):(?P<id_type>[\\\\w\\\\_]*)[\\\\s\\\\w](?P=id_type):(?P<assy>[\\\\w\\\\d\\\\_]*)\\\\.(?P<seq_level>[\\\\w]*):\\\\d*:\\\\d*:(?P<taxonomy>\\\\d*):(?P<id>\\\\d*)[\\\\s\\t'\n",
      "                                                ']REF$'},\n",
      " 'fastq_art_illumina_ncbi_std': {'keys': 'readid '\n",
      "                                         'reftaxonomyid '\n",
      "                                         'refsource '\n",
      "                                         'refseqnb '\n",
      "                                         'readnb',\n",
      "                                 'pattern': '^@(?P<readid>(?P<reftaxonomyid>\\\\d*):(?P<refsource>\\\\w*):(?P<refseqnb>\\\\d*)-(?P<readnb>\\\\d*(\\\\/\\\\d)?))$'}}\n"
     ]
    }
   ],
   "source": [
    "with open('../default_parsing_rules.json', 'r') as fp:\n",
    "    pprint(json.load(fp), width=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for 'cov_virus_sequence_one.fa'> saved as <cov_virus_sequence_one_metadata.json> in  \n",
      "/home/vtec/projects/bio/metagentorch/nbs-dev/data_dev/ncbi/refsequences/cov\n",
      "\n",
      "{'2591237:ncbi:1': {'accession': 'MK211378',\n",
      "                    'organism': 'Coronavirus BtRs-BetaCoV/YN2018D',\n",
      "                    'seqid': '2591237:ncbi:1',\n",
      "                    'seqnb': '1',\n",
      "                    'source': 'ncbi',\n",
      "                    'taxonomyid': '2591237'}}\n"
     ]
    }
   ],
   "source": [
    "p2fasta = pfs.data / 'ncbi/refsequences/cov/cov_virus_sequence_one.fa'\n",
    "fasta = FastaFileReader(p2fasta)\n",
    "fasta_meta = fasta.parse_file(save_json=True)\n",
    "pprint(fasta_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FASTQ file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extension of `TextFileBaseReader` class for fastq sequence files.\n",
    "\n",
    "Structure of a FASTQ sequence file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@2591237:ncbi:1-40200\n",
      "TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAAC\n",
      "+\n",
      "CCCGGGCGGGGGCJGJJJGJJGJJJGJGGJGJJJGJGGGGGGGGCJGJGGGGGJJJJGCCGGGGGJCGCGJGJCG=GGGG\n",
      "@2591237:ncbi:1-40199\n",
      "TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGAGTCATTTGA\n",
      "+\n",
      "=CCGGGGCGGGGGJJGJJGJGJG=GJJGJCGJJJCJ=JJJJGGJJCJGJGG=JGC1JJGG8GCJCGGGCGG(GCGGCGC=\n",
      "@2591237:ncbi:1-40198\n",
      "TAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTCTTCATCATCTAACTCTGAATATTTATTCTTAGTT\n",
      "+\n",
      "C=CGGGGGGGGGGCJJJJ=JJJJJJJJJJJGGJJJJ1GJJ8GJJGGGJGGJJC=JJGGGCCGG88GG=GGGGGGCJGGGG\n"
     ]
    }
   ],
   "source": [
    "p2fastq = pfs.data / 'ncbi/simreads/cov/single_1seq_150bp/single_1seq_150bp.fq'\n",
    "\n",
    "fastq = TextFileBaseReader(p2fastq, nlines=1)\n",
    "for i, t in enumerate(fastq):\n",
    "    txt = t.replace('\\n', '')[:80]\n",
    "    print(f\"{txt}\")\n",
    "    if i >= 11: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FastqFileReader(TextFileBaseReader):\n",
    "    \"\"\"Iterator going through a fastq file's sequences and return each section + prob error as a dict\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        path:str|Path,   # path to the fastq file\n",
    "    )-> dict:           # key/value with keys: definition line; sequence; q score; prob error\n",
    "        self.nlines = 4\n",
    "        super().__init__(path, nlines=self.nlines)\n",
    "        self.text_to_parse_key = 'definition line'\n",
    "        self.set_parsing_rules(verbose=False)        \n",
    "    \n",
    "    def __next__(self):\n",
    "        \"\"\"Return definition line, sequence and quality scores\"\"\"\n",
    "        lines = []\n",
    "        for i in range(self.nlines):\n",
    "            lines.append(self._safe_readline().replace('\\n', ''))\n",
    "        \n",
    "        output = {\n",
    "            'definition line':lines[0], \n",
    "            'sequence':f\"{lines[1]}\", \n",
    "            'read_qscores': f\"{lines[3]}\",\n",
    "        }\n",
    "        output['probs error'] = np.array([q_score2prob_error(q) for q in output['read_qscores']])\n",
    "        self._chunk_nb = self._chunk_nb + 1\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def read_nb(self)-> int:\n",
    "        return self._chunk_nb\n",
    "    \n",
    "    def print_first_chunks(\n",
    "        self, \n",
    "        nchunks:int=3,  # number of chunks to print out\n",
    "    ):\n",
    "        \"\"\"Print the first `nchunks` chunks of text from the file\"\"\"\n",
    "        for i, seq_dict in enumerate(self.__iter__()):\n",
    "            print(f\"\\nSequence {i+1}:\")\n",
    "            print(seq_dict['definition line'])\n",
    "            print(f\"{seq_dict['sequence'][:80]} ...\")\n",
    "            if i >= nchunks: break\n",
    "            \n",
    "    def parse_file(\n",
    "        self,\n",
    "        add_readseq :bool=False,    # When True, add the full sequence to the parsed metadata dictionary\n",
    "        add_qscores:bool=False,     # Add the read ASCII Q Scores to the parsed dictionary when True\n",
    "        add_probs_error:bool=False, # Add the read probability of error to the parsed dictionary when True\n",
    "        save_json: bool=False       # When True, save the file metadata as a json file of same stem name\n",
    "    )-> dict[str]:                  # Metadata as Key/Values pairs\n",
    "        \"\"\"Read fastq file, return a dict with definition line metadata and optionally read sequence and q scores, ...\"\"\"\n",
    "    \n",
    "        self.reset_iterator()\n",
    "        parsed = {}\n",
    "        for d in self:\n",
    "            dfn_line = d['definition line']\n",
    "            seq, q_scores, prob_e = d['sequence'], d['read_qscores'], d['probs error']\n",
    "            metadata = self._parse_text_fn(dfn_line, self.re_pattern, self.re_keys)\n",
    "            if add_readseq: metadata['readseq'] = seq         \n",
    "            if add_qscores: metadata['read_qscores'] = q_scores\n",
    "            if add_probs_error: metadata['probs error'] = prob_e\n",
    "            parsed[metadata['readid']] = metadata \n",
    "                        \n",
    "        if save_json:\n",
    "            p2json = self.path.parent / f\"{self.path.stem}_metadata.json\"\n",
    "            with open(p2json, 'w') as fp:\n",
    "                json.dump(parsed, fp, indent=4)\n",
    "                print(f\"Metadata for '{self.path.name}'> saved as <{p2json.name}> in  \\n{p2json.parent.absolute()}\\n\")\n",
    "\n",
    "        return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L159){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastqFileReader\n",
       "\n",
       ">      FastqFileReader (path:str|pathlib.Path)\n",
       "\n",
       "*Iterator going through a fastq file's sequences and return each section + prob error as a dict*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| pathlib.Path | path to the fastq file |\n",
       "| **Returns** | **dict** | **key/value with keys: definition line; sequence; q score; prob error** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L159){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastqFileReader\n",
       "\n",
       ">      FastqFileReader (path:str|pathlib.Path)\n",
       "\n",
       "*Iterator going through a fastq file's sequences and return each section + prob error as a dict*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| pathlib.Path | path to the fastq file |\n",
       "| **Returns** | **dict** | **key/value with keys: definition line; sequence; q score; prob error** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastqFileReader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['definition line', 'sequence', 'read_qscores', 'probs error'])\n",
      "Definition line:  @2591237:ncbi:1-40200\n",
      "Read sequence:    TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAACTTACATAGCTCGCGTCTCAGTTTCAAGGAACTTTTAGTGTATGCTGCTGATCCAGCCATGCATGCAGCTT\n",
      "Q scores (ASCII): CCCGGGCGGGGGCJGJJJGJJGJJJGJGGJGJJJGJGGGGGGGGCJGJGGGGGJJJJGCCGGGGGJCGCGJGJCG=GGGG=CGGGGGG1GCGCGGGGCCGJC8GGGGGGGGGGGCGGGGGGGGGGGC8GGGGGGCGGC1GGGCGGGGGCC\n",
      "Prob error:       0.0004,0.0004,0.0004,0.0002,0.0002,0.0002,0.0004,0.0002,0.0002,0.0002,0.0002,0.0002,0.0004,0.0001,0.0002,0.0001,0.0001,0.0001,0.0002,0.0001,0.0001,0.0002,0.0001,0.0001,0.0001,0.0002,0.0001,0.0002,0.0002,0.0001,0.0002,0.0001,0.0001,0.0001,0.0002,0.0001,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0004,0.0001,0.0002,0.0001,0.0002,0.0002,0.0002,0.0002,0.0002,0.0001,0.0001,0.0001,0.0001,0.0002,0.0004,0.0004,0.0002,0.0002,0.0002,0.0002,0.0002,0.0001,0.0004,0.0002,0.0004,0.0002,0.0001,0.0002,0.0001,0.0004,0.0002,0.0016,0.0002,0.0002,0.0002,0.0002,0.0016,0.0004,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0251,0.0002,0.0004,0.0002,0.0004,0.0002,0.0002,0.0002,0.0002,0.0004,0.0004,0.0002,0.0001,0.0004,0.0050,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0004,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0004,0.0050,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0004,0.0002,0.0002,0.0004,0.0251,0.0002,0.0002,0.0002,0.0004,0.0002,0.0002,0.0002,0.0002,0.0002,0.0004,0.0004\n"
     ]
    }
   ],
   "source": [
    "fastq = FastqFileReader(p2fastq)\n",
    "iteration_output = next(fastq)\n",
    "\n",
    "print(type(iteration_output))\n",
    "print(iteration_output.keys())\n",
    "print(f\"Definition line:  {iteration_output['definition line']}\")\n",
    "print(f\"Read sequence:    {iteration_output['sequence']}\")\n",
    "print(f\"Q scores (ASCII): {iteration_output['read_qscores']}\")\n",
    "print(f\"Prob error:       {','.join([f'{p:.4f}' for p in iteration_output['probs error']])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Five largest probabilities of error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00158489, 0.00501187, 0.00501187, 0.02511886, 0.02511886])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(iteration_output['probs error'])[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 80, 127, 102, 138,  88])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(iteration_output['probs error'])[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'readid': '2591237:ncbi:1-40200',\n",
       " 'readnb': '40200',\n",
       " 'refseqnb': '1',\n",
       " 'refsource': 'ncbi',\n",
       " 'reftaxonomyid': '2591237'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn_line = iteration_output['definition line']\n",
    "meta = fastq.parse_text(dfn_line)\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['definition line', 'sequence', 'read_qscores', 'probs error'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastq = FastqFileReader(p2fastq)\n",
    "next(fastq).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L200){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastqFileReader.parse_file\n",
       "\n",
       ">      FastqFileReader.parse_file (add_readseq:bool=False,\n",
       ">                                  add_qscores:bool=False,\n",
       ">                                  add_probs_error:bool=False,\n",
       ">                                  save_json:bool=False)\n",
       "\n",
       "*Read fastq file, return a dict with definition line metadata and optionally read sequence and q scores, ...*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| add_readseq | bool | False | When True, add the full sequence to the parsed metadata dictionary |\n",
       "| add_qscores | bool | False | Add the read ASCII Q Scores to the parsed dictionary when True |\n",
       "| add_probs_error | bool | False | Add the read probability of error to the parsed dictionary when True |\n",
       "| save_json | bool | False | When True, save the file metadata as a json file of same stem name |\n",
       "| **Returns** | **dict** |  | **Metadata as Key/Values pairs** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L200){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastqFileReader.parse_file\n",
       "\n",
       ">      FastqFileReader.parse_file (add_readseq:bool=False,\n",
       ">                                  add_qscores:bool=False,\n",
       ">                                  add_probs_error:bool=False,\n",
       ">                                  save_json:bool=False)\n",
       "\n",
       "*Read fastq file, return a dict with definition line metadata and optionally read sequence and q scores, ...*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| add_readseq | bool | False | When True, add the full sequence to the parsed metadata dictionary |\n",
       "| add_qscores | bool | False | Add the read ASCII Q Scores to the parsed dictionary when True |\n",
       "| add_probs_error | bool | False | Add the read probability of error to the parsed dictionary when True |\n",
       "| save_json | bool | False | When True, save the file metadata as a json file of same stem name |\n",
       "| **Returns** | **dict** |  | **Metadata as Key/Values pairs** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastqFileReader.parse_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2591237:ncbi:1-40200\n",
      "{'readid': '2591237:ncbi:1-40200',\n",
      " 'readnb': '40200',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n",
      "2591237:ncbi:1-40199\n",
      "{'readid': '2591237:ncbi:1-40199',\n",
      " 'readnb': '40199',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n",
      "2591237:ncbi:1-40198\n",
      "{'readid': '2591237:ncbi:1-40198',\n",
      " 'readnb': '40198',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n",
      "2591237:ncbi:1-40197\n",
      "{'readid': '2591237:ncbi:1-40197',\n",
      " 'readnb': '40197',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n"
     ]
    }
   ],
   "source": [
    "parsed = fastq.parse_file(add_readseq=False, add_qscores=False, add_probs_error=False)\n",
    "for i, (k, v) in enumerate(parsed.items()):\n",
    "    print(k)\n",
    "    pprint(v)\n",
    "    if i >=3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>readid</th>\n",
       "      <th>readnb</th>\n",
       "      <th>refseqnb</th>\n",
       "      <th>refsource</th>\n",
       "      <th>reftaxonomyid</th>\n",
       "      <th>readseq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-40200</th>\n",
       "      <td>2591237:ncbi:1-40200</td>\n",
       "      <td>40200</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-40199</th>\n",
       "      <td>2591237:ncbi:1-40199</td>\n",
       "      <td>40199</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-40198</th>\n",
       "      <td>2591237:ncbi:1-40198</td>\n",
       "      <td>40198</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>TAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-40197</th>\n",
       "      <td>2591237:ncbi:1-40197</td>\n",
       "      <td>40197</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>TAATCACTGATAGCAGCATTGCCATCCTGAGCAAAGAAGAAGTGTT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-40196</th>\n",
       "      <td>2591237:ncbi:1-40196</td>\n",
       "      <td>40196</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>CTAATGTCAGTACGCCTACAATGCCTGCATCACGCATAGCATCGCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-40195</th>\n",
       "      <td>2591237:ncbi:1-40195</td>\n",
       "      <td>40195</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>AAGCTGAAGCATACATAACACAGTCCTTAAGCCGATAACCAGACAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-40194</th>\n",
       "      <td>2591237:ncbi:1-40194</td>\n",
       "      <td>40194</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>AGTGGAAGAACTTCACCGTCAAGATGAAACTCGACGGGGCTCTCCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-40193</th>\n",
       "      <td>2591237:ncbi:1-40193</td>\n",
       "      <td>40193</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>GCGTCTCGAGTGCTTCGAGTTCACCGTTCTTGAGAACAACCTCCTC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-40192</th>\n",
       "      <td>2591237:ncbi:1-40192</td>\n",
       "      <td>40192</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>CTGGTAGTATCTAAGGCTCCACTGAAATACTTGTACTTGTTATATA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-40191</th>\n",
       "      <td>2591237:ncbi:1-40191</td>\n",
       "      <td>40191</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>GTCTCTATCTGTAGTACTATGACAAATAGACAGTTTCATCAGAAAT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    readid readnb refseqnb refsource  \\\n",
       "2591237:ncbi:1-40200  2591237:ncbi:1-40200  40200        1      ncbi   \n",
       "2591237:ncbi:1-40199  2591237:ncbi:1-40199  40199        1      ncbi   \n",
       "2591237:ncbi:1-40198  2591237:ncbi:1-40198  40198        1      ncbi   \n",
       "2591237:ncbi:1-40197  2591237:ncbi:1-40197  40197        1      ncbi   \n",
       "2591237:ncbi:1-40196  2591237:ncbi:1-40196  40196        1      ncbi   \n",
       "2591237:ncbi:1-40195  2591237:ncbi:1-40195  40195        1      ncbi   \n",
       "2591237:ncbi:1-40194  2591237:ncbi:1-40194  40194        1      ncbi   \n",
       "2591237:ncbi:1-40193  2591237:ncbi:1-40193  40193        1      ncbi   \n",
       "2591237:ncbi:1-40192  2591237:ncbi:1-40192  40192        1      ncbi   \n",
       "2591237:ncbi:1-40191  2591237:ncbi:1-40191  40191        1      ncbi   \n",
       "\n",
       "                     reftaxonomyid  \\\n",
       "2591237:ncbi:1-40200       2591237   \n",
       "2591237:ncbi:1-40199       2591237   \n",
       "2591237:ncbi:1-40198       2591237   \n",
       "2591237:ncbi:1-40197       2591237   \n",
       "2591237:ncbi:1-40196       2591237   \n",
       "2591237:ncbi:1-40195       2591237   \n",
       "2591237:ncbi:1-40194       2591237   \n",
       "2591237:ncbi:1-40193       2591237   \n",
       "2591237:ncbi:1-40192       2591237   \n",
       "2591237:ncbi:1-40191       2591237   \n",
       "\n",
       "                                                                readseq  \n",
       "2591237:ncbi:1-40200  TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCG...  \n",
       "2591237:ncbi:1-40199  TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATT...  \n",
       "2591237:ncbi:1-40198  TAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTC...  \n",
       "2591237:ncbi:1-40197  TAATCACTGATAGCAGCATTGCCATCCTGAGCAAAGAAGAAGTGTT...  \n",
       "2591237:ncbi:1-40196  CTAATGTCAGTACGCCTACAATGCCTGCATCACGCATAGCATCGCA...  \n",
       "2591237:ncbi:1-40195  AAGCTGAAGCATACATAACACAGTCCTTAAGCCGATAACCAGACAA...  \n",
       "2591237:ncbi:1-40194  AGTGGAAGAACTTCACCGTCAAGATGAAACTCGACGGGGCTCTCCA...  \n",
       "2591237:ncbi:1-40193  GCGTCTCGAGTGCTTCGAGTTCACCGTTCTTGAGAACAACCTCCTC...  \n",
       "2591237:ncbi:1-40192  CTGGTAGTATCTAAGGCTCCACTGAAATACTTGTACTTGTTATATA...  \n",
       "2591237:ncbi:1-40191  GTCTCTATCTGTAGTACTATGACAAATAGACAGTTTCATCAGAAAT...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = fastq.parse_file(add_readseq=True)\n",
    "df = pd.DataFrame(metadata).T\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_ncbi_std> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fastq_art_illumina_ncbi_std> generated 5 matches\n",
      "--------------------------------------------------------------------------------\n",
      "^@(?P<readid>(?P<reftaxonomyid>\\d*):(?P<refsource>\\w*):(?P<refseqnb>\\d*)-(?P<readnb>\\d*(\\/\\d)?))$\n",
      "['readid', 'reftaxonomyid', 'refsource', 'refseqnb', 'readnb']\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <aln_art_illumina_ncbi_std> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <aln_art_illumina-refseq-ncbi-std> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_ncbi_cov> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_rhinolophus_ferrumequinum> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Selected rule with most matches: fastq_art_illumina_ncbi_std\n"
     ]
    }
   ],
   "source": [
    "fastq.set_parsing_rules(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALN Alignment Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extension of `TextFileBaseReader` class for ALN read/sequence alignment files.\n",
    "\n",
    "Structure of a ALN sequence file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##ART_Illumina\tread_length\t150\n",
      "@CM\t/usr/bin/art_illumina -i /home/vtec/projects/bio/metagentools/data/ncbi/refs\n",
      "@SQ\t2591237:ncbi:1\t1\tMK211378\t2591237\tncbi\tCoronavirus BtRs-BetaCoV/YN2018D\t3021\n",
      "##Header End\n",
      ">2591237:ncbi:1\t2591237:ncbi:1-40200\t14370\t+\n",
      "TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAAC\n",
      "TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAAC\n",
      ">2591237:ncbi:1\t2591237:ncbi:1-40199\t15144\t-\n",
      "TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGATTCATTTGA\n",
      "TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGAGTCATTTGA\n",
      ">2591237:ncbi:1\t2591237:ncbi:1-40198\t2971\t-\n",
      "TAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTCTTCATCATCTAACTCTGAATATTTATTCTTAGTT\n",
      "TAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTCTTCATCATCTAACTCTGAATATTTATTCTTAGTT\n"
     ]
    }
   ],
   "source": [
    "p2aln = pfs.data / 'ncbi/simreads/cov/single_1seq_150bp/single_1seq_150bp.aln'\n",
    "assert p2aln.is_file()\n",
    "\n",
    "aln = TextFileBaseReader(p2aln, nlines=1)\n",
    "for i, t in enumerate(aln):\n",
    "    txt = t.replace('\\n', '')[:80]\n",
    "    print(f\"{txt}\")\n",
    "    if i >= 12: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AlnFileReader(TextFileBaseReader):\n",
    "    \"\"\"Iterator going through an ALN file\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        path:str|Path,   # path to the aln file\n",
    "    )-> dict:            # key/value with keys: \n",
    "        \"\"\"Set TextFileBaseReader attributes and specific class attributes\"\"\"\n",
    "        self.nlines = 1\n",
    "        super().__init__(path, nlines=self.nlines)\n",
    "        self.header = self.read_header()\n",
    "        self.nlines = 3\n",
    "        self.text_to_parse_key = 'definition line'\n",
    "        self.set_parsing_rules(verbose=False)\n",
    "        self.set_header_parsing_rules(verbose=False)\n",
    "        self.ref_sequences = self.parse_header_reference_sequences()\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"Return definition line, sequence and quality scores\"\"\"\n",
    "        lines = []\n",
    "        for i in range(self.nlines):\n",
    "            lines.append(self._safe_readline().replace('\\n', ''))\n",
    "\n",
    "        output = {\n",
    "            'definition line':lines[0], \n",
    "            'ref_seq_aligned':f\"{lines[1]}\", \n",
    "            'read_seq_aligned': f\"{lines[2]}\",\n",
    "        }   \n",
    "        return output\n",
    "    \n",
    "    def read_header(self):\n",
    "        \"\"\"Read ALN file Header and return each section parsed in a dictionary\"\"\"\n",
    "        \n",
    "        header = {}\n",
    "        if self.fp is not None:\n",
    "            self.fp.close()\n",
    "        self.fp = open(self.path, 'r')\n",
    "        \n",
    "        line = self._safe_readline().replace('\\n', '')\n",
    "        if not line.startswith('##ART_Illumina'): \n",
    "            raise ValueError(f\"Header of this file does not start with ##ART_Illumina\")\n",
    "        line = self._safe_readline().replace('\\n', '')\n",
    "        if not line.startswith('@CM'): \n",
    "            raise ValueError(f\"First header line should start with @CM\")\n",
    "        else: \n",
    "            header['command'] = line[3:].replace('\\t', '').strip()\n",
    "\n",
    "        refseqs = []\n",
    "        while True:\n",
    "            line = self._safe_readline().replace('\\n', '')\n",
    "            if line.startswith('##Header End'): break\n",
    "            else:\n",
    "                refseqs.append(line)\n",
    "        header['reference sequences'] = refseqs\n",
    "        \n",
    "        return header\n",
    "    \n",
    "    def reset_iterator(self):\n",
    "        \"\"\"Reset the iterator to point to the first line in the file, by recreating a new file handle.\n",
    "        \n",
    "        `AlnFileReader` requires a specific `reset_iterator` method, in order to skip the header every time it is reset\n",
    "        \"\"\"\n",
    "        if self.fp is not None:\n",
    "            self.fp.close()\n",
    "        self.fp = open(self.path, 'r')\n",
    "        while True:\n",
    "            line = self._safe_readline().replace('\\n', '')\n",
    "            if line.startswith('##Header End'): break\n",
    "\n",
    "    def parse_definition_line_with_position(\n",
    "        self, \n",
    "        dfn_line:str    # fefinition line string to be parsed\n",
    "        )-> dict:       # parsed metadata in key/value format + relative position of the read\n",
    "        \"\"\"Parse definition line and adds relative position\"\"\"\n",
    "        read_meta = self.parse_text(dfn_line)\n",
    "        read_refseqid = read_meta['refseqid']\n",
    "        read_start_pos = int(read_meta['aln_start_pos'])\n",
    "        read_refseq_lentgh = int(self.ref_sequences[read_refseqid]['refseq_length'])\n",
    "        read_meta['read_pos'] = (read_start_pos *10)// read_refseq_lentgh + 1\n",
    "        return read_meta\n",
    "    \n",
    "    def parse_file(\n",
    "        self, \n",
    "        add_ref_seq_aligned:bool=False,   # Add the reference sequence aligned to the parsed dictionary when True\n",
    "        add_read_seq_aligned:bool=False,  # Add the read sequence aligned to the parsed dictionary when True\n",
    "    )-> dict[str]: \n",
    "        # Key/Values. Keys: \n",
    "        # `readid`,`seqid`,`seq_nbr`,`read_nbr`,`aln_start_pos`,`ref_seq_strand`\n",
    "        # optionaly `ref_seq_aligned`,`read_seq_aligned`\n",
    "        \"\"\"Read ALN file, return a dict w/ alignment info for each read and optionaly aligned reference sequence & read\"\"\"\n",
    "        self.reset_iterator()\n",
    "        parsed = {}\n",
    "        for d in self:\n",
    "            dfn_line = d['definition line']\n",
    "            ref_seq_aligned, read_seq_aligned = d['ref_seq_aligned'], d['read_seq_aligned']\n",
    "            metadata = self.parse_text(dfn_line)\n",
    "            if add_ref_seq_aligned: metadata['ref_seq_aligned'] = ref_seq_aligned         \n",
    "            if add_read_seq_aligned: metadata['read_seq_aligned'] = read_seq_aligned\n",
    "            parsed[metadata['readid']] = metadata \n",
    "        return parsed\n",
    "\n",
    "    def parse_header_reference_sequences(\n",
    "        self,\n",
    "        pattern:str|None=None,     # regex pattern to apply to parse the reference sequence info\n",
    "        keys:list[str]|None=None,  # list of keys: keys are both regex match group names and corresponding output dict keys \n",
    "        )->dict[str]:                  # parsed metadata in key/value format\n",
    "        \"\"\"Extract metadata from all header reference sequences\"\"\"\n",
    "        if pattern is None and keys is None:\n",
    "            pattern, keys = self.re_header_pattern, self.re_header_keys\n",
    "        parsed = {}\n",
    "        for seq_dfn_line in self.header['reference sequences']:\n",
    "            metadata = self.parse_text(seq_dfn_line, pattern, keys)\n",
    "            parsed[metadata['refseqid']] = metadata\n",
    "            \n",
    "        return parsed       \n",
    "        \n",
    "    def set_header_parsing_rules(\n",
    "        self,\n",
    "        pattern: str|bool=None,   # regex pattern to apply to parse the text, search in parsing rules json if None\n",
    "        keys: list[str]=None,     # list of keys/group for regex, search in parsing rules json if None\n",
    "        verbose: bool=False       # when True, provides information on each rule\n",
    "    )-> None:\n",
    "        \"\"\"Set the regex parsing rule for reference sequence in ALN header.\n",
    "               \n",
    "        Updates 3 class attributes: `re_header_rule_name`, `re_header_pattern`, `re_header_keys`\n",
    "        \n",
    "        TODO: refactor this and the method in Core: to use a single function for the common part and a parameter for the text_to_parse \n",
    "        \"\"\"\n",
    "        \n",
    "        P2JSON = Path(f\"{PACKAGE_ROOT}/default_parsing_rules.json\")\n",
    "        \n",
    "        self.re_header_rule_name = None\n",
    "        self.re_header_pattern = None\n",
    "        self.re_header_keys = None\n",
    "        \n",
    "        # get the first reference sequence definition line in header\n",
    "        text_to_parse = self.header['reference sequences'][0]\n",
    "        divider_line = f\"{'-'*80}\"\n",
    "\n",
    "        if pattern is not None and keys is not None:  # When specific pattern and keys are passed\n",
    "            try:\n",
    "                metadata_dict = self.parse_text(text_to_parse, pattern, keys)\n",
    "                self.re_header_rule_name = 'Custom Rule'\n",
    "                self.re_header_pattern = pattern\n",
    "                self.re_header_keys = keys\n",
    "                if verbose:\n",
    "                    print(divider_line)\n",
    "                    print(f\"Custom rule was set for header in this instance.\")\n",
    "            except Exception as err: \n",
    "                raise ValueError(f\"The pattern generates the following error:\\n{err}\")\n",
    "                \n",
    "        else:  # automatic rule selection among rules saved in json file\n",
    "            # Load all existing rules from json file\n",
    "            with open(P2JSON, 'r') as fp:\n",
    "                parsing_rules = json.load(fp)\n",
    "                \n",
    "            # test all existing rules and keep the one with highest number of matches\n",
    "            max_nbr_matches = 0\n",
    "            for k, v in parsing_rules.items():\n",
    "                re_header_pattern = v['pattern']\n",
    "                re_header_keys = v['keys'].split(' ')\n",
    "                try:\n",
    "                    metadata_dict = self.parse_text(text_to_parse, re_header_pattern, re_header_keys)\n",
    "                    nbr_matches = len(metadata_dict)\n",
    "                    if verbose:\n",
    "                        print(divider_line)\n",
    "                        print(f\"Rule <{k}> generated {nbr_matches:,d} matches\")\n",
    "                        print(divider_line)\n",
    "                        print(re_header_pattern)\n",
    "                        print(re_header_keys)\n",
    "\n",
    "                    if len(metadata_dict) > max_nbr_matches:\n",
    "                        self.re_header_pattern = re_header_pattern\n",
    "                        self.re_header_keys = re_header_keys\n",
    "                        self.re_header_rule_name = k    \n",
    "                except Exception as err:\n",
    "                    if verbose:\n",
    "                        print(divider_line)\n",
    "                        print(f\"Rule <{k}> generated an error\")\n",
    "                        print(err)\n",
    "                    else:\n",
    "                        pass\n",
    "            if self.re_header_rule_name is None:\n",
    "                msg = \"\"\"\n",
    "        None of the saved parsing rules were able to extract metadata from the first line in this file.\n",
    "        You must set a custom rule (pattern + keys) before parsing text, by using:\n",
    "            `self.set_parsing_rules(custom_pattern, custom_list_of_keys)`\n",
    "                \"\"\"\n",
    "                warnings.warn(msg, category=UserWarning)\n",
    "            \n",
    "            if verbose:\n",
    "                print(divider_line)\n",
    "                print(f\"Selected rule with most matches: {self.re_header_rule_name}\")\n",
    "\n",
    "            # We used the iterator, now we need to reset it to make all lines available\n",
    "            self.reset_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L229){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader\n",
       "\n",
       ">      AlnFileReader (path:str|pathlib.Path)\n",
       "\n",
       "*Iterator going through an ALN file*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| pathlib.Path | path to the aln file |\n",
       "| **Returns** | **dict** | **key/value with keys:** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L229){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader\n",
       "\n",
       ">      AlnFileReader (path:str|pathlib.Path)\n",
       "\n",
       "*Iterator going through an ALN file*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| pathlib.Path | path to the aln file |\n",
       "| **Returns** | **dict** | **key/value with keys:** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AlnFileReader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aln = AlnFileReader(p2aln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AlnFileReader` iterator returns elements one by one, as dictionaries with each data line related to the read, accessible through the following keys: \n",
    "\n",
    "- key `'definition line'`: **read definition line**, including read metadata \n",
    "- key `'ref_seq_aligned'`: **aligned reference sequence**, that is the sequence segment in the original reference corresponding to the read\n",
    "- key `'read_seq_aligned'`: **aligned read**, that is the simmulated read sequence, where each bp corresponds to the reference sequence bp in the same position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['definition line', 'ref_seq_aligned', 'read_seq_aligned'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_iteration = next(aln)\n",
    "one_iteration.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'definition line': '>2591237:ncbi:1\\t2591237:ncbi:1-40200\\t14370\\t+',\n",
      " 'read_seq_aligned': 'TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAACTTACATAGCTCGCGTCTCAGTTTCAAGGAACTTTTAGTGTATGCTGCTGATCCAGCCATGCATGCAGCTT',\n",
      " 'ref_seq_aligned': 'TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAACTTACATAGCTCGCGTCTCAGTTTCAAGGAACTTTTAGTGTATGCTGCTGATCCAGCCATGCATGCAGCTT'}\n"
     ]
    }
   ],
   "source": [
    "pprint(one_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn_line, ref_seq_aligned, read_seq_aligned = one_iteration.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>2591237:ncbi:1\\t2591237:ncbi:1-40200\\t14370\\t+'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAACTTACATAGCTCGCGTCTCAG'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_seq_aligned[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAACTTACATAGCTCGCGTCTCAG'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_seq_aligned[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'definition line': '>2591237:ncbi:1\\t2591237:ncbi:1-40199\\t15144\\t-',\n",
      " 'read_seq_aligned': 'TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGAGTCATTTGAGTTATAGTAGGGATGACATTACGCTTAGTATACGCGAAAAGTGCATCTTGATCCTCATAACTCATTGAGT',\n",
      " 'ref_seq_aligned': 'TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGATTCATTTGAGTTATAGTAGGGATGACATTACGCTTAGTATACGCGAAAAGTGCATCTTGATCCTCATAACTCATTGAGT'}\n"
     ]
    }
   ],
   "source": [
    "another_iteration = next(aln)\n",
    "pprint(another_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2591237:ncbi:1\t2591237:ncbi:1-40200\t14370\t+\n",
      "TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAAC ...\n",
      "TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAAC ...\n",
      "\n",
      ">2591237:ncbi:1\t2591237:ncbi:1-40199\t15144\t-\n",
      "TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGATTCATTTGA ...\n",
      "TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGAGTCATTTGA ...\n",
      "\n",
      ">2591237:ncbi:1\t2591237:ncbi:1-40198\t2971\t-\n",
      "TAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTCTTCATCATCTAACTCTGAATATTTATTCTTAGTT ...\n",
      "TAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTCTTCATCATCTAACTCTGAATATTTATTCTTAGTT ...\n",
      "\n",
      ">2591237:ncbi:1\t2591237:ncbi:1-40197\t15485\t-\n",
      "TAATCACTGATAGCAGCATTGCCATCCTGAGCAAAGAAGAAGTGTTTTAGTTCAACAGAACTTCCTTCCTTAAAGAAACC ...\n",
      "TAATCACTGATAGCAGCATTGCCATCCTGAGCAAAGAAGAAGTGTTTTAGTTCAACAGAACTTCCTTCCTTAAAGAAACC ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aln.reset_iterator()\n",
    "for i, d in enumerate(aln):\n",
    "    print(d['definition line'])\n",
    "    print(d['ref_seq_aligned'][:80], '...')\n",
    "    print(d['read_seq_aligned'][:80], '...\\n')\n",
    "    if i >= 3: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once instantiated, the `AlnFileReader` iterator gives access to the file's header information through `header` instance attribute. It is a dictionary with two keys: `'command'` and `'reference sequences'`:\n",
    "\n",
    "```\n",
    "    {'command':             'art-illumina command used to create the reads',\n",
    "     'reference sequences': ['@SQ metadata on reference sequence 1 used for the reads',\n",
    "                             '@SQ metadata on reference sequence 2 used for the reads', \n",
    "                             ...\n",
    "                            ]\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/art_illumina -i /home/vtec/projects/bio/metagentools/data/ncbi/refsequences/cov/cov_refseq_001-seq1.fa -ss HS25 -l 150 -f 200 -o /home/vtec/projects/bio/metagentools/data/ncbi/simreads/cov/single_1seq_150bp/single_1seq_150bp -rs 1723893089\n"
     ]
    }
   ],
   "source": [
    "print(aln.header['command'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@SQ\t2591237:ncbi:1\t1\tMK211378\t2591237\tncbi\tCoronavirus BtRs-BetaCoV/YN2018D\t30213\n"
     ]
    }
   ],
   "source": [
    "for seq_info in aln.header['reference sequences']:\n",
    "    print(seq_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **read definition line** includes key metadata, which need to be parsed using the appropriate parsing rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.parse_text\n",
       "\n",
       ">      TextFileBaseReader.parse_text (txt:str, pattern:str|None=None,\n",
       ">                                     keys:list[str]|None=None)\n",
       "\n",
       "*Parse text using regex pattern and keys. Return a metadata dictionary.\n",
       "\n",
       "The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
       "\n",
       "    {\n",
       "        'key_1': 'metadata 1',\n",
       "        'key_2': 'metadata 2',\n",
       "        ...\n",
       "    }*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| txt | str |  | text to parse |\n",
       "| pattern | str \\| None | None | If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex |\n",
       "| keys | list[str] \\| None | None | If None, uses standard regex list of keys, otherwise, uses passed list of keys (str) |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/core.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.parse_text\n",
       "\n",
       ">      TextFileBaseReader.parse_text (txt:str, pattern:str|None=None,\n",
       ">                                     keys:list[str]|None=None)\n",
       "\n",
       "*Parse text using regex pattern and keys. Return a metadata dictionary.\n",
       "\n",
       "The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
       "\n",
       "    {\n",
       "        'key_1': 'metadata 1',\n",
       "        'key_2': 'metadata 2',\n",
       "        ...\n",
       "    }*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| txt | str |  | text to parse |\n",
       "| pattern | str \\| None | None | If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex |\n",
       "| keys | list[str] \\| None | None | If None, uses standard regex list of keys, otherwise, uses passed list of keys (str) |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AlnFileReader.parse_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "pattern, keys = aln.re_pattern, aln.re_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aln_start_pos': '14370',\n",
       " 'readid': '2591237:ncbi:1-40200',\n",
       " 'readnb': '40200',\n",
       " 'refseq_strand': '+',\n",
       " 'refseqid': '2591237:ncbi:1',\n",
       " 'refseqnb': '1',\n",
       " 'refsource': 'ncbi',\n",
       " 'reftaxonomyid': '2591237'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aln.parse_text(dfn_line, pattern, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L297){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.parse_definition_line_with_position\n",
       "\n",
       ">      AlnFileReader.parse_definition_line_with_position (dfn_line:str)\n",
       "\n",
       "*Parse definition line and adds relative position*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| dfn_line | str | fefinition line string to be parsed |\n",
       "| **Returns** | **dict** | **parsed metadata in key/value format + relative position of the read** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L297){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.parse_definition_line_with_position\n",
       "\n",
       ">      AlnFileReader.parse_definition_line_with_position (dfn_line:str)\n",
       "\n",
       "*Parse definition line and adds relative position*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| dfn_line | str | fefinition line string to be parsed |\n",
       "| **Returns** | **dict** | **parsed metadata in key/value format + relative position of the read** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AlnFileReader.parse_definition_line_with_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon instance creation, `AlnFileReader` automatically checks the `default_parsing_rules.json` file for a workable rule among saved rules. Saved rules include the rule for ART Illumina ALN files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aln_art_illumina_ncbi_std'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aln.re_rule_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is therefore not required to pass a specific `pattern` and `keys` parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aln_start_pos': '14370',\n",
       " 'readid': '2591237:ncbi:1-40200',\n",
       " 'readnb': '40200',\n",
       " 'refseq_strand': '+',\n",
       " 'refseqid': '2591237:ncbi:1',\n",
       " 'refseqnb': '1',\n",
       " 'refsource': 'ncbi',\n",
       " 'reftaxonomyid': '2591237'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aln.parse_text(dfn_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ART Ilumina ALN files definition lines consist of:\n",
    "\n",
    "- The **read** ID: `readid`, e.g. `2591237:ncbi:1-20100`\n",
    "- the **read** number (order in the file): `readnb`, e.g. `20100`\n",
    "- The **read** start position in the reference sequence: `aln_start_pos`, e.g. `23878`\n",
    "- The **reference sequence** ID: `readid`, e.g. `2591237:ncbi:1-20100`\n",
    "- The **reference sequence** number: `refseqnb`, e.g. `1`\n",
    "- The **reference sequence** source: `refsource`, e.g. `ncbi`\n",
    "- The **reference sequence** taxonomy: `reftaxonomyid`, e.g. `2591237`\n",
    "- The **reference sequence** strand:  `refseq_strand` wich is either `+` or  `-`,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L309){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.parse_file\n",
       "\n",
       ">      AlnFileReader.parse_file (add_ref_seq_aligned:bool=False,\n",
       ">                                add_read_seq_aligned:bool=False)\n",
       "\n",
       "*Read ALN file, return a dict w/ alignment info for each read and optionaly aligned reference sequence & read*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| add_ref_seq_aligned | bool | False | Add the reference sequence aligned to the parsed dictionary when True |\n",
       "| add_read_seq_aligned | bool | False | Add the read sequence aligned to the parsed dictionary when True |\n",
       "| **Returns** | **dict** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L309){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.parse_file\n",
       "\n",
       ">      AlnFileReader.parse_file (add_ref_seq_aligned:bool=False,\n",
       ">                                add_read_seq_aligned:bool=False)\n",
       "\n",
       "*Read ALN file, return a dict w/ alignment info for each read and optionaly aligned reference sequence & read*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| add_ref_seq_aligned | bool | False | Add the reference sequence aligned to the parsed dictionary when True |\n",
       "| add_read_seq_aligned | bool | False | Add the read sequence aligned to the parsed dictionary when True |\n",
       "| **Returns** | **dict** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AlnFileReader.parse_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2591237:ncbi:1-40200\n",
      "{'aln_start_pos': '14370',\n",
      " 'readid': '2591237:ncbi:1-40200',\n",
      " 'readnb': '40200',\n",
      " 'refseq_strand': '+',\n",
      " 'refseqid': '2591237:ncbi:1',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n",
      "2591237:ncbi:1-40199\n",
      "{'aln_start_pos': '15144',\n",
      " 'readid': '2591237:ncbi:1-40199',\n",
      " 'readnb': '40199',\n",
      " 'refseq_strand': '-',\n",
      " 'refseqid': '2591237:ncbi:1',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n",
      "2591237:ncbi:1-40198\n",
      "{'aln_start_pos': '2971',\n",
      " 'readid': '2591237:ncbi:1-40198',\n",
      " 'readnb': '40198',\n",
      " 'refseq_strand': '-',\n",
      " 'refseqid': '2591237:ncbi:1',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n",
      "2591237:ncbi:1-40197\n",
      "{'aln_start_pos': '15485',\n",
      " 'readid': '2591237:ncbi:1-40197',\n",
      " 'readnb': '40197',\n",
      " 'refseq_strand': '-',\n",
      " 'refseqid': '2591237:ncbi:1',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n",
      "2591237:ncbi:1-40196\n",
      "{'aln_start_pos': '16221',\n",
      " 'readid': '2591237:ncbi:1-40196',\n",
      " 'readnb': '40196',\n",
      " 'refseq_strand': '-',\n",
      " 'refseqid': '2591237:ncbi:1',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n"
     ]
    }
   ],
   "source": [
    "parsed = aln.parse_file()\n",
    "\n",
    "for i, (k, v) in enumerate(parsed.items()):\n",
    "    print(k)\n",
    "    pprint(v)\n",
    "    if i > 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L329){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.parse_header_reference_sequences\n",
       "\n",
       ">      AlnFileReader.parse_header_reference_sequences (pattern:str|None=None,\n",
       ">                                                      keys:list[str]|None=None)\n",
       "\n",
       "*Extract metadata from all header reference sequences*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| None | None | regex pattern to apply to parse the reference sequence info |\n",
       "| keys | list[str] \\| None | None | list of keys: keys are both regex match group names and corresponding output dict keys |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L329){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.parse_header_reference_sequences\n",
       "\n",
       ">      AlnFileReader.parse_header_reference_sequences (pattern:str|None=None,\n",
       ">                                                      keys:list[str]|None=None)\n",
       "\n",
       "*Extract metadata from all header reference sequences*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| None | None | regex pattern to apply to parse the reference sequence info |\n",
       "| keys | list[str] \\| None | None | list of keys: keys are both regex match group names and corresponding output dict keys |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AlnFileReader.parse_header_reference_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2591237:ncbi:1': {'organism': 'Coronavirus BtRs-BetaCoV/YN2018D',\n",
      "                    'refseq_accession': 'MK211378',\n",
      "                    'refseq_length': '30213',\n",
      "                    'refseqid': '2591237:ncbi:1',\n",
      "                    'refseqnb': '1',\n",
      "                    'refsource': 'ncbi',\n",
      "                    'reftaxonomyid': '2591237'}}\n"
     ]
    }
   ],
   "source": [
    "pprint(aln.parse_header_reference_sequences())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L344){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.set_header_parsing_rules\n",
       "\n",
       ">      AlnFileReader.set_header_parsing_rules (pattern:str|bool=None,\n",
       ">                                              keys:list[str]=None,\n",
       ">                                              verbose:bool=False)\n",
       "\n",
       "*Set the regex parsing rule for reference sequence in ALN header.\n",
       "\n",
       "Updates 3 class attributes: `re_header_rule_name`, `re_header_pattern`, `re_header_keys`\n",
       "\n",
       "TODO: refactor this and the method in Core: to use a single function for the common part and a parameter for the text_to_parse*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| bool | None | regex pattern to apply to parse the text, search in parsing rules json if None |\n",
       "| keys | list | None | list of keys/group for regex, search in parsing rules json if None |\n",
       "| verbose | bool | False | when True, provides information on each rule |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L344){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.set_header_parsing_rules\n",
       "\n",
       ">      AlnFileReader.set_header_parsing_rules (pattern:str|bool=None,\n",
       ">                                              keys:list[str]=None,\n",
       ">                                              verbose:bool=False)\n",
       "\n",
       "*Set the regex parsing rule for reference sequence in ALN header.\n",
       "\n",
       "Updates 3 class attributes: `re_header_rule_name`, `re_header_pattern`, `re_header_keys`\n",
       "\n",
       "TODO: refactor this and the method in Core: to use a single function for the common part and a parameter for the text_to_parse*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| bool | None | regex pattern to apply to parse the text, search in parsing rules json if None |\n",
       "| keys | list | None | list of keys/group for regex, search in parsing rules json if None |\n",
       "| verbose | bool | False | when True, provides information on each rule |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AlnFileReader.set_header_parsing_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_ncbi_std> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fastq_art_illumina_ncbi_std> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <aln_art_illumina_ncbi_std> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <aln_art_illumina-refseq-ncbi-std> generated 7 matches\n",
      "--------------------------------------------------------------------------------\n",
      "^@SQ[\\t\\s]*(?P<refseqid>(?P<reftaxonomyid>\\d*):(?P<refsource>\\w*):(?P<refseqnb>\\d*))[\\t\\s]*(?P=refseqnb)[\\t\\s]*(?P<refseq_accession>[\\w\\d]*)[\\t\\s]*(?P=reftaxonomyid)[\\t\\s]*(?P=refsource)[\\t\\s](?P<organism>.*)[\\t\\s](?P<refseq_length>\\d*)$\n",
      "['refseqid', 'reftaxonomyid', 'refsource', 'refseqnb', 'refseq_accession', 'organism', 'refseq_length']\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_ncbi_cov> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_rhinolophus_ferrumequinum> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Selected rule with most matches: aln_art_illumina-refseq-ncbi-std\n"
     ]
    }
   ],
   "source": [
    "aln.set_header_parsing_rules(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aln_art_illumina-refseq-ncbi-std\n",
      "^@SQ[\\t\\s]*(?P<refseqid>(?P<reftaxonomyid>\\d*):(?P<refsource>\\w*):(?P<refseqnb>\\d*))[\\t\\s]*(?P=refseqnb)[\\t\\s]*(?P<refseq_accession>[\\w\\d]*)[\\t\\s]*(?P=reftaxonomyid)[\\t\\s]*(?P=refsource)[\\t\\s](?P<organism>.*)[\\t\\s](?P<refseq_length>\\d*)$\n",
      "['refseqid', 'reftaxonomyid', 'refsource', 'refseqnb', 'refseq_accession', 'organism', 'refseq_length']\n"
     ]
    }
   ],
   "source": [
    "print(aln.re_header_rule_name)\n",
    "print(aln.re_header_pattern)\n",
    "print(aln.re_header_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reads are provided in various formats (plain text for original data, fastq + aln for simulated reads). The CNN_Virus model requires inputs in the form of three tensors:\n",
    "\n",
    "- `read_seq_batch`: a batch of 50-mer read sequences, in \"base-hot-encoded\" format (BHE)\n",
    "- `label_batch`: a batch of the species' labels, in one-hot-encoded format (187 classes) (OHE)\n",
    "- `pos_batch`: a batch of the read's relative positions in the reference sequence, in one-hot-encoded format (10 classes) (OHE)\n",
    "\n",
    "The classes below are custom pytorch `Dataset` used to load the reads from their file and transform it into BHE or OHE tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain text based data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets provided by CNN Virus team are in plain text format, one sequence per line, with format as follows:\n",
    "\n",
    "```ascii\n",
    "AAAAAGATTTTGAGAGAGGTCGACCTGTCCTCCTAAAACGTTTACAAAAG\t71\t0\n",
    "CATGTAACGCAGCTTAGTCCGATCGTGGCTATAATCCGTCTTTCGATTTG\t1\t7\n",
    "AACAACATCTTGTTGATGATAACCGTCAAAGTGTTTTGGGTCTGGAGGGA\t158\t6\n",
    "AGTACCTGGAGAGCGTTAAGAAACACAAACGGCTGGATGTAGTGCCGCGC\t6\t7\n",
    "CCACGTCGATGAAGCTCCGACGAGAGTCGGCGCTGAGCCCGCGCACCTCC\t71\t6\n",
    "AGCTCGTGGATCTCCCCTCCTTCTGCAGTTTCAACATCAGAAGCCCTGAA\t87\t1\n",
    "```\n",
    "\n",
    "The first column is the k-base (k-mer) read, followed byt the specie label and the relative position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using this type of data, the data pipeline consist of:\n",
    "\n",
    "- Creating a `TextFileDataset` instance to load the data\n",
    "\n",
    "- Using the created dataset to in a `Dataloader` used for training or inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextFileDataset(IterableDataset):\n",
    "    \"\"\"Load data from text file and yield (BHE sequence tensor, (label OHE tensor, position OHE tensor))\"\"\"\n",
    "\n",
    "    base2encoding = {\n",
    "        'A': [1,0,0,0,0], \n",
    "        'C': [0,1,0,0,0], \n",
    "        'G': [0,0,1,0,0], \n",
    "        'T': [0,0,0,1,0], \n",
    "        'N': [0,0,0,0,1],\n",
    "        '-': [0,0,0,0,1],\n",
    "        }\n",
    "    nb_labels = 187\n",
    "    nb_pos = 10\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        p2file:str|Path,  # path to the file to read\n",
    "    ):\n",
    "        self.p2file = safe_path(p2file)\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.p2file, 'r') as f:\n",
    "            for line in f:\n",
    "                # wi = torch.utils.data.get_worker_info()\n",
    "                # if wi:\n",
    "                #     print(f\"{wi.id} loading {line}\")\n",
    "                seq, lbl, pos = line.replace('\\n','').strip().split('\\t')\n",
    "                seq_bhe = torch.tensor(list(map(self._bhe_fn, seq)))\n",
    "                lbl_ohe = torch.zeros(self.nb_labels)\n",
    "                lbl_ohe[int(lbl)] = 1\n",
    "                pos_ohe = torch.zeros(self.nb_pos)\n",
    "                pos_ohe[int(pos)] = 1\n",
    "                yield seq_bhe, (lbl_ohe, pos_ohe)\n",
    "    \n",
    "    def _bhe_fn(self, base:str) -> list[int]:\n",
    "        \"\"\"Convert a base to a one hot encoding vector\"\"\"\n",
    "        return self.base2encoding[base]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `IterableDataset` reads the file line by line and yields a tupple `(seq_bhe, (lbl_ohe, pos_ohe))`:\n",
    "\n",
    "- `seq_bhe`: tensor of shape [k, 5] with the k-mer read in base-hot-encoded format\n",
    "- `lbl_ohe`: tensor of shape [187] with the specie label in one-hot-encoded format\n",
    "- `pos_ohe`: tensor of shape [10] with the relative position label in one-hot-encoded format\n",
    "\n",
    "The dataset is then used with a pytorch `DataLoader` to handle batching. When using the GPU, the `pin_memory=True` should be used to make transfer of data to the GPU faster.\n",
    "\n",
    "Let's use a small data file to illustrate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([8, 50, 5]) torch.Size([8, 187]) torch.Size([8, 10])\n",
      "torch.Size([4, 50, 5]) torch.Size([4, 187]) torch.Size([4, 10])\n",
      "torch.Size([4, 50, 5]) torch.Size([4, 187]) torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "p2file = Path('data_dev/CNN_Virus_data/50mer_ds_100_seq')\n",
    "assert p2file.is_file()\n",
    "\n",
    "ds = TextFileDataset(p2file)\n",
    "dl = DataLoader(ds, batch_size=8, num_workers=2, pin_memory=False)\n",
    "\n",
    "for seq_batch, (lbl_batch, pos_batch) in dl:\n",
    "    print(seq_batch.shape, lbl_batch.shape, pos_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The read sequence tensor is BHE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0]],\n",
       "\n",
       "        [[1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0],\n",
       "         [1, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_batch[:2, :3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([94, 18]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_batch[:2, :], lbl_batch.argmax(dim=1)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALN file based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All simulated reads are stored in a `.fastq` and `.aln` file. We use the `.aln` file because it includes all metadata information required to retrieve both the specie and the relative position of each read.\n",
    "\n",
    "`AlnFileDataset` allows to load reads and metadata stored in ART Illumina simulated ALN file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40,200 reads in this ALN file\n"
     ]
    }
   ],
   "source": [
    "p2aln = pfs.data / 'ncbi/simreads/cov/single_1seq_150bp/single_1seq_150bp.aln'\n",
    "aln = AlnFileReader(p2aln)\n",
    "\n",
    "for i, d in enumerate(aln):\n",
    "    pass\n",
    "print(f\"{i+1:,d} reads in this ALN file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AlnFileDataset(IterableDataset):\n",
    "    \"\"\"Load data and metadata from ALN file, yield BHE sequence, OHE label, OHE position tensors + metadata\n",
    "\n",
    "    The iterator yield tupple (read tensor,(label tensor, position tensor)):\n",
    "    \n",
    "    - kmer read tensor in base hot encoded format (shape [k, 5])\n",
    "    - label tensor in one hot encoded format (shape [187])\n",
    "    - position tensor in one hote encoded format (shape [10])\n",
    "\n",
    "    It also optionally returns a dictionary of the read metadata available in the ALN file.\n",
    "    \"\"\"\n",
    "\n",
    "    base2encoding = {\n",
    "        'A': [1,0,0,0,0], \n",
    "        'C': [0,1,0,0,0], \n",
    "        'G': [0,0,1,0,0], \n",
    "        'T': [0,0,0,1,0], \n",
    "        'N': [0,0,0,0,1],\n",
    "        '-': [0,0,0,0,1],\n",
    "        }\n",
    "    nb_labels = 187\n",
    "    nb_pos = 10\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        p2file:str|Path,            # path to the file to read\n",
    "        label:int = 118,            # label for this batch (assuming all reads are from the same species)\n",
    "        return_metadata:bool=False  # yield each read metadata as a dictionary when Trud\n",
    "    ):\n",
    "        self.p2file = safe_path(p2file)\n",
    "        self.aln = AlnFileReader(self.p2file)\n",
    "        self.label = label\n",
    "        self.return_metadata = return_metadata\n",
    "\n",
    "    def __iter__(self):\n",
    "        for d in self.aln:\n",
    "            metadata = self.aln.parse_definition_line_with_position(d['definition line'])\n",
    "            seq = d['read_seq_aligned']\n",
    "            seq_bhe = torch.tensor(list(map(self._bhe_fn, seq)))\n",
    "            lbl_ohe = torch.zeros(self.nb_labels)\n",
    "            lbl_ohe[int(self.label)] = 1\n",
    "            pos = metadata['read_pos']\n",
    "            pos_ohe = torch.zeros(self.nb_pos)\n",
    "            pos_ohe[int(pos-1)] = 1\n",
    "            if self.return_metadata:\n",
    "                yield seq_bhe, (lbl_ohe, pos_ohe), metadata\n",
    "            else:   \n",
    "                yield seq_bhe, (lbl_ohe, pos_ohe)\n",
    "    \n",
    "    def _bhe_fn(self, base:str) -> list[int]:\n",
    "        \"\"\"Convert a base to a one hot encoding vector\"\"\"\n",
    "        return self.base2encoding[base]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L465){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileDataset\n",
       "\n",
       ">      AlnFileDataset (p2file:str|pathlib.Path, label:int=118,\n",
       ">                      return_metadata:bool=False)\n",
       "\n",
       "*Load data and metadata from ALN file, yield BHE sequence, OHE label, OHE position tensors + metadata\n",
       "\n",
       "The iterator yield tupple (read tensor,(label tensor, position tensor)):\n",
       "\n",
       "- kmer read tensor in base hot encoded format (shape [k, 5])\n",
       "- label tensor in one hot encoded format (shape [187])\n",
       "- position tensor in one hote encoded format (shape [10])\n",
       "\n",
       "It also optionally returns a dictionary of the read metadata available in the ALN file.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| p2file | str \\| pathlib.Path |  | path to the file to read |\n",
       "| label | int | 118 | label for this batch (assuming all reads are from the same species) |\n",
       "| return_metadata | bool | False | yield each read metadata as a dictionary when Trud |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentorch/blob/main/metagentorch/cnn_virus/data.py#L465){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileDataset\n",
       "\n",
       ">      AlnFileDataset (p2file:str|pathlib.Path, label:int=118,\n",
       ">                      return_metadata:bool=False)\n",
       "\n",
       "*Load data and metadata from ALN file, yield BHE sequence, OHE label, OHE position tensors + metadata\n",
       "\n",
       "The iterator yield tupple (read tensor,(label tensor, position tensor)):\n",
       "\n",
       "- kmer read tensor in base hot encoded format (shape [k, 5])\n",
       "- label tensor in one hot encoded format (shape [187])\n",
       "- position tensor in one hote encoded format (shape [10])\n",
       "\n",
       "It also optionally returns a dictionary of the read metadata available in the ALN file.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| p2file | str \\| pathlib.Path |  | path to the file to read |\n",
       "| label | int | 118 | label for this batch (assuming all reads are from the same species) |\n",
       "| return_metadata | bool | False | yield each read metadata as a dictionary when Trud |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AlnFileDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset from a ALN file, then pass it to the class `Dataloader` to create batches of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "  Shapes of each data tensor:\n",
      "    seq: torch.Size([16, 150, 5]), label: torch.Size([16, 187]), pos: torch.Size([16, 10])\n",
      "Sample 2:\n",
      "  Shapes of each data tensor:\n",
      "    seq: torch.Size([16, 150, 5]), label: torch.Size([16, 187]), pos: torch.Size([16, 10])\n"
     ]
    }
   ],
   "source": [
    "ds = AlnFileDataset(p2aln, label=118)\n",
    "\n",
    "dl = DataLoader(ds, batch_size=16, shuffle=False)\n",
    "\n",
    "for i, (s, (lbl,pos)) in enumerate(dl):\n",
    "    print(f\"Sample {i+1:000d}:\")\n",
    "    print(f\"  Shapes of each data tensor:\")\n",
    "    print(f\"    seq: {s.shape}, label: {lbl.shape}, pos: {pos.shape}\")\n",
    "    if i+1 >= 2:break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When an `AlnFileDataset` instance with `return_metadata` set as True is passed to `DataLoader`, the dataloader will yield (read_tensor,(lbl_tensor, pos_tensor), metadata) where:\n",
    "\n",
    "- read_tensor is the batch of kmer read tensor in base hot encoded format (shape [bs, k, 5])\n",
    "- lbl_tensor is the batch of label tensor in one hot encoded format (shape [bs, 187])\n",
    "- pos_tensor is the batch of position tensor in one hote encoded format (shape [bs, 10])\n",
    "- metadata is a dictionary of form `key:list` like:\n",
    "```python\n",
    "{\n",
    "    'aln_start_pos': ['14370', '15144', '2971'], \n",
    "    'readid': ['2591237:ncbi:1-40200', '2591237:ncbi:1-40199', '2591237:ncbi:1-40198'], \n",
    "    'readnb': ['40200', '40199', '40198'], \n",
    "    'refseq_strand': ['+', '-', '-'], \n",
    "    'refseqid': ['2591237:ncbi:1', '2591237:ncbi:1', '2591237:ncbi:1'], \n",
    "    'refseqnb': ['1', '1', '1'], \n",
    "    'refsource': ['ncbi', 'ncbi', 'ncbi'], \n",
    "    'reftaxonomyid': ['2591237', '2591237', '2591237'], \n",
    "    'read_pos': tensor([ 5,  6,  1])\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "  Shapes of each data tensor:\n",
      "    seq: torch.Size([3, 150, 5]), label: torch.Size([3, 187]), pos: torch.Size([3, 10])\n",
      "  Metadata dictionary for the batch\n",
      "    {'aln_start_pos': ['14370', '15144', '2971'], 'readid': ['2591237:ncbi:1-40200', '2591237:ncbi:1-40199', '2591237:ncbi:1-40198'], 'readnb': ['40200', '40199', '40198'], 'refseq_strand': ['+', '-', '-'], 'refseqid': ['2591237:ncbi:1', '2591237:ncbi:1', '2591237:ncbi:1'], 'refseqnb': ['1', '1', '1'], 'refsource': ['ncbi', 'ncbi', 'ncbi'], 'reftaxonomyid': ['2591237', '2591237', '2591237'], 'read_pos': tensor([5, 6, 1])}\n",
      "Sample 2:\n",
      "  Shapes of each data tensor:\n",
      "    seq: torch.Size([3, 150, 5]), label: torch.Size([3, 187]), pos: torch.Size([3, 10])\n",
      "  Metadata dictionary for the batch\n",
      "    {'aln_start_pos': ['15485', '16221', '18953'], 'readid': ['2591237:ncbi:1-40197', '2591237:ncbi:1-40196', '2591237:ncbi:1-40195'], 'readnb': ['40197', '40196', '40195'], 'refseq_strand': ['-', '-', '-'], 'refseqid': ['2591237:ncbi:1', '2591237:ncbi:1', '2591237:ncbi:1'], 'refseqnb': ['1', '1', '1'], 'refsource': ['ncbi', 'ncbi', 'ncbi'], 'reftaxonomyid': ['2591237', '2591237', '2591237'], 'read_pos': tensor([6, 6, 7])}\n"
     ]
    }
   ],
   "source": [
    "ds = AlnFileDataset(p2aln, label=118, return_metadata=True)\n",
    "dl = DataLoader(ds, batch_size=3, shuffle=False)\n",
    "\n",
    "for i, (s, (lbl,pos), d) in enumerate(dl):\n",
    "    print(f\"Sample {i+1:000d}:\")\n",
    "    print(f\"  Shapes of each data tensor:\")\n",
    "    print(f\"    seq: {s.shape}, label: {lbl.shape}, pos: {pos.shape}\")\n",
    "    print(f\"  Metadata dictionary for the batch\")\n",
    "    print(f\"    {d}\")\n",
    "    if i+1 >= 2:break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle long reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Virus handles 50-mer reads only. Longer reads need to be split into 50-mer reads by sliding a window of size 50 along the read sequence. The set of multiple 50-mer reads is then used as input to the model. After prediction on the set of 50-mer reads, the final prediction is obtained by filtering 50-mer prediction that yield a hight enough probability and then voting for the most predicted species.\n",
    "\n",
    "A k-mer (150-mer) read will be split in k-49 (101) 50-mer reads which will yield k-49 (101) probability tensors. The final prediction will be the species with the highest number of votes, as long as their respective probabilities > 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def split_kmer_batch_into_50mers(\n",
    "    kmer: torch.Tensor        # tensor representing a batch of k-mer reads, BHE format, shape [b, k, 5]\n",
    "    ) -> torch.Tensor:\n",
    "    \"\"\"Convert a batch of k-mer reads into 50-mer reads, by shifting the k-mer one base at a time.\n",
    "\n",
    "    Shapes: for a batch of `b` k-mer reads, returns a batch of `b * (k - 49)` 50-mer reads\n",
    "\n",
    "    Technical Note: we use advanced indexing of the tensor to create the 50-mer and roll them, with no loop.\n",
    "    \"\"\"\n",
    "    b = kmer.shape[0]\n",
    "    k = kmer.shape[1]\n",
    "    n = k - 49\n",
    "    # Create an array of indices for rolling\n",
    "    idx_rows = np.arange(n)[:, None]    # shape (n, 1) broadcast n rows to create the split effect\n",
    "    idx_bases = np.arange(k)            # shape (k) creates the shifting effect\n",
    "    indices = idx_rows + idx_bases      # shape (n, k)\n",
    "    rolled_indices = indices  % k       # shape (n, k) Modulo to create the rolling effect\n",
    "\n",
    "    # Create a rolled tensor using broadcasting\n",
    "    rolled_tensor = kmer[:, rolled_indices, :].reshape(-1,k, 5)\n",
    "\n",
    "    return rolled_tensor[:, :50,:] # keep only the first 50 bases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the function, with a test tensor designed to make the validation easier.\n",
    "\n",
    "We create a tensor with a batch of b k-mer reads in the following format:\n",
    "```ascii\n",
    "read 1: 10001, 10002, 10003, .... 10149\n",
    "read 2: 20001, 20002, 20003, .... 20149\n",
    "read 3: 30001, 30002, 30003, .... 30149\n",
    "    ...\n",
    "```\n",
    "\n",
    "We then add an additional dimension to simulate the BHE encoding, by repeating the same value 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 150, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[10000, 10001, 10002, 10147, 10148, 10149],\n",
       "        [20000, 20001, 20002, 20147, 20148, 20149],\n",
       "        [30000, 30001, 30002, 30147, 30148, 30149],\n",
       "        [40000, 40001, 40002, 40147, 40148, 40149]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 4\n",
    "k = 150\n",
    "\n",
    "# Build a tensor of shape (b,k) with the pattern described above\n",
    "kmer = torch.stack([(i+1) * 10000 + torch.arange(k) for i in range(b)], dim=0)\n",
    "\n",
    "# simulate the BHE buy duplicating the 5 dimensions\n",
    "kmer = torch.stack([kmer]*5, dim=2)\n",
    "\n",
    "print(kmer.shape)\n",
    "# Slice the tensor to only show the first and last three bases, and only one of the 5 BHE dimensions\n",
    "idx_bases = np.array([0,1,2,147,148,149])\n",
    "kmer[:, idx_bases, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the function `split_reads` returns a tensor with shape [b, k-49, 50, 5] as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([404, 50, 5])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_tensor = split_kmer_batch_into_50mers(kmer)\n",
    "split_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets slice only the first and last 2 50-mer reads for each batch and a selection of bases to confirm that the batch is properly split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10000, 10001, 10025, 10026, 10048, 10049],\n",
      "        [10001, 10002, 10026, 10027, 10049, 10050],\n",
      "        [10099, 10100, 10124, 10125, 10147, 10148],\n",
      "        [10100, 10101, 10125, 10126, 10148, 10149],\n",
      "        [20000, 20001, 20025, 20026, 20048, 20049],\n",
      "        [20001, 20002, 20026, 20027, 20049, 20050],\n",
      "        [20099, 20100, 20124, 20125, 20147, 20148],\n",
      "        [20100, 20101, 20125, 20126, 20148, 20149],\n",
      "        [30000, 30001, 30025, 30026, 30048, 30049],\n",
      "        [30001, 30002, 30026, 30027, 30049, 30050],\n",
      "        [30099, 30100, 30124, 30125, 30147, 30148],\n",
      "        [30100, 30101, 30125, 30126, 30148, 30149]])\n"
     ]
    }
   ],
   "source": [
    "dim0_idxs = [0,1,99,100,101,102,200,201,202,203,301,302]\n",
    "dim1_idxs = [0,1,25,26,48,49]\n",
    "dim2_idxs = [0,1,2,3,4]\n",
    "print(split_tensor[np.ix_(dim0_idxs, dim1_idxs, dim2_idxs)][:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model CNN Virus requires post inderence processing of the predictions:\n",
    "- before inference, each k-mer was split into $k-49$ 50-mer, where were presented to the model for inference. Each k-mer led to $k-49$ predictions (probability tensors)\n",
    "- now, we need to combine these $n = k-49 $ probability tensor into a single prediction for the original k-mer. This is done by first filtering all 50-reads that gave a max probability lower than a specific threshold (0.9 by default) and then combining the prediction by a simple vote\n",
    "\n",
    "Now we can build the function `combine_predictions` step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets create a test probabilities tensor representing a batch of `bs` k-mers, each split into `k-49` 50-mers, with a probability tensor of shape `[bs, k-49, nb_class]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a batch of probabilities for 4 55-mer reads with 10 classes:\n",
      "probs torch.Size([4, 6, 10])\n"
     ]
    }
   ],
   "source": [
    "def create_test_probs(bs=4, k=150, nb_class=187):\n",
    "    print(f\"Creating a batch of probabilities for {bs} {k}-mer reads with {nb_class} classes:\")\n",
    "\n",
    "    probs = torch.from_numpy(np.random.rand(bs * n ,nb_class)).reshape(-1,n,nb_class)\n",
    "    print(f\"probs {probs.shape}\")\n",
    "    return probs\n",
    "\n",
    "bs, k, nb_class = 4, 55, 10\n",
    "n = k - 49\n",
    "probs = create_test_probs(bs, k, nb_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Extract the predictions from probabilities for each 50-mer read, then filter the predictions with a probability lower than a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds torch.Size([4, 6]):\n",
      " tensor([[9, 6, 2, 0, 5, 3],\n",
      "        [7, 6, 8, 7, 6, 1],\n",
      "        [5, 6, 4, 8, 5, 8],\n",
      "        [8, 2, 8, 8, 4, 9]])\n",
      "preds torch.Size([4, 6]):\n",
      " tensor([[   9,    6,    2,    0,    5,    3],\n",
      "        [   7,    6,    8, 9999,    6,    1],\n",
      "        [   5,    6,    4,    8, 9999,    8],\n",
      "        [   8,    2,    8,    8,    4,    9]])\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.8\n",
    "preds = probs.argmax(dim=2)\n",
    "print(f\"preds {preds.shape}:\\n\",preds)\n",
    "\n",
    "INVALID = 9999\n",
    "invalid_labels_filter = probs.max(dim=2).values <= threshold\n",
    "preds[invalid_labels_filter] = INVALID\n",
    "print(f\"preds {preds.shape}:\\n\",preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Extract the prediction unique values and their counts.\n",
    "\n",
    "> Code explanation note: \n",
    ">\n",
    "> We do not want to use a python loop. This is why we use `torch.unique(preds)` with `return_inverse`. The function returns a flat tensor with the unique values accross the entire `preds` and a tensor of the same shape as `preds` with the indices of the unique values in `preds`.\n",
    ">\n",
    "```python\n",
    "    x = torch.tensor([[10, 30, 20, 30],[20,20,20,50]], dtype=torch.long)\n",
    "\n",
    "    uniques, inverse = torch.unique(x, sorted=True, return_inverse=True)\n",
    "\n",
    "    > unique:  tensor([10, 20, 30, 50])\n",
    "      inverse: tensor([[0, 2, 1, 2],\n",
    "                       [1, 1, 1, 3]]))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_values torch.Size([11]):\n",
      " tensor([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9, 9999])\n",
      "inverse_indices torch.Size([4, 6]):\n",
      " tensor([[ 9,  6,  2,  0,  5,  3],\n",
      "        [ 7,  6,  8, 10,  6,  1],\n",
      "        [ 5,  6,  4,  8, 10,  8],\n",
      "        [ 8,  2,  8,  8,  4,  9]])\n"
     ]
    }
   ],
   "source": [
    "# Get unique values and their counts for the entire tensor\n",
    "unique_values, inverse_indices = torch.unique(preds, return_inverse=True)\n",
    "inverse_indices = inverse_indices.view(preds.shape)\n",
    "print(f\"unique_values {unique_values.shape}:\\n\",unique_values)\n",
    "print(f\"inverse_indices {inverse_indices.shape}:\\n\",inverse_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Compute the counts of unique values in each 50-mer read and store it in a tensor of shape (batch size, nb of unique values in the the shole tensor `preds`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_values torch.Size([11]):\n",
      " tensor([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9, 9999])\n",
      "counts torch.Size([4, 11]):\n",
      " tensor([[1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 2, 1, 1, 0, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 0, 2, 0, 1],\n",
      "        [0, 0, 1, 0, 1, 0, 0, 0, 3, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor to hold the counts (shape (bs, nb_unique_values_across_the_batch))\n",
    "counts = torch.zeros((preds.shape[0], unique_values.shape[0]), dtype=torch.int64)\n",
    "# Count occurrences of each unique value per row\n",
    "counts.scatter_add_(dim=1, index=inverse_indices, src=torch.ones_like(inverse_indices, dtype=torch.int64))\n",
    "print(f\"unique_values {unique_values.shape}:\\n\", unique_values)\n",
    "print(f\"counts {counts.shape}:\\n\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Get the index of the most frequent value for each 50-mer read, excluding the INVALID placeholder, and extract the corresponding values into a tensor of shape (`bs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most_voted_value torch.Size([4, 1]):\n",
      " tensor([[0],\n",
      "        [6],\n",
      "        [8],\n",
      "        [8]])\n"
     ]
    }
   ],
   "source": [
    "# get value most voted per 50-read (vertical tensor), excluding the placeholder INVALID\n",
    "most_voted_idxs = counts[:, :-1].argmax(dim=1)\n",
    "most_voted_value = unique_values[most_voted_idxs][:, None]\n",
    "print(f\"most_voted_value {most_voted_value.shape}:\\n\", most_voted_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put it all in the function `combine_predictions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def combine_predictions(\n",
    "    label_probs: torch.Tensor,   # Probabilities for the labels classes for each 50-mer\n",
    "    pos_probs: torch.Tensor,     # Probabolities for the position classes for each 50-mer\n",
    "    threshold: float = 0.9       # Threshold to consider a prediction as valid\n",
    "    ):\n",
    "    \"\"\"Combine a batch of 50-mer probabilities into one batch of final prediction for label and position\n",
    "\n",
    "    Note: the input must be of shape (batch_size, n, c) where n is k-49 and c is the nb of labels or positions\n",
    "    \"\"\"\n",
    "    INVALID = 9999\n",
    "    is_batch = True\n",
    "    \n",
    "    assert label_probs.dim() == pos_probs.dim(), \"Input do not have the same nb of dimensions\"\n",
    "\n",
    "    if label_probs.dim() != 3:\n",
    "        is_batch = False\n",
    "        print('Converting probability tensors to 3 dimensions')\n",
    "        label_probs = label_probs.unsqueeze(0)\n",
    "        pos_probs = pos_probs.unsqueeze(0)\n",
    "\n",
    "    # Extract the prediction for each 50-mer read\n",
    "    label_preds = label_probs.argmax(dim=2) # shape (bs, nb_50mers)\n",
    "    pos_preds = pos_probs.argmax(dim=2)\n",
    "    # print(label_preds.shape, pos_preds.shape, label_probs.shape, pos_probs.shape)\n",
    "\n",
    "    # Identify reads with too low prediction probability and replace their prediction by INVALID\n",
    "    invalid_labels_filter = label_probs.max(dim=2).values <= threshold\n",
    "    # print(invalid_labels_filter.shape)\n",
    "    label_preds[invalid_labels_filter] = INVALID\n",
    "    pos_preds[invalid_labels_filter] = INVALID\n",
    "\n",
    "    def most_common_value(preds, invalid_filter):\n",
    "        # print(f\"_preds {preds.shape}:\\n\",preds)\n",
    "        # Get unique values and their counts for the entire tensor\n",
    "        unique_values, inverse_indices = torch.unique(preds, return_inverse=True)\n",
    "        inverse_indices = inverse_indices.view(preds.shape)\n",
    "        # print(f\"unique_values {unique_values.shape}:\\n\",unique_values)\n",
    "        # print(f\"inverse_indices {inverse_indices.shape}:\\n\",inverse_indices)\n",
    "\n",
    "        # Create a tensor to hold the counts (shape (bs, nb_unique_values_across_the_batch))\n",
    "        counts = torch.zeros((preds.shape[0], unique_values.shape[0]), dtype=torch.int64)\n",
    "        # Count occurrences of each unique value per row\n",
    "        counts.scatter_add_(dim=1, index=inverse_indices, src=torch.ones_like(inverse_indices, dtype=torch.int64))\n",
    "        # print(f\"counts {counts.shape}:\\n\", counts)\n",
    "\n",
    "        # get value most voted per 50-read (vertical tensor), excluding the placeholder INVALID\n",
    "        most_voted_value = unique_values[counts[:, :-1].argmax(dim=1)][:, None]\n",
    "        # print(f\"most_voted_value {most_voted_value.shape}:\\n\", most_voted_value)\n",
    "        return most_voted_value\n",
    "\n",
    "    combined_labels = most_common_value(label_preds, invalid_labels_filter)\n",
    "    combined_pos = most_common_value(pos_preds, invalid_labels_filter)\n",
    "\n",
    "    # Concatenate combined_label and combined_position\n",
    "    combined_preds = torch.cat([combined_labels, combined_pos], dim=1)\n",
    "\n",
    "    return combined_preds if is_batch else combined_preds.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the function with the same test tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2],\n",
       "        [6, 6],\n",
       "        [6, 6],\n",
       "        [8, 8]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_predictions(probs, probs[:, :,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the function with test tensor correponding to real probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a batch of probabilities for 1 150-mer reads with 187 classes:\n",
      "probs torch.Size([1, 6, 187])\n",
      "Creating a batch of probabilities for 1 150-mer reads with 10 classes:\n",
      "probs torch.Size([1, 6, 10])\n",
      "\n",
      "combine for a batch of data\n",
      "tensor([[32,  4]])\n",
      "\n",
      "combine for a single sample (not a batch)\n",
      "Converting probability tensors to 3 dimensions\n",
      "tensor([32,  4])\n"
     ]
    }
   ],
   "source": [
    "bs, k = 1, 150\n",
    "label_probs = create_test_probs(bs, k, 187)\n",
    "pos_probs = create_test_probs(bs, k, 10)\n",
    "\n",
    "print('\\ncombine for a batch of data')\n",
    "p = combine_predictions(label_probs,pos_probs)\n",
    "print(p)\n",
    "print('\\ncombine for a single sample (not a batch)')\n",
    "p = combine_predictions(label_probs[0,:,:],pos_probs[0,:,:])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When any of the following classes and functions is called, it will raise an exception with an error message indicating how to handle the required code refactoring.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "---------------------------------------------------------------------------\n",
    "DeprecationWarning                        Traceback (most recent call last)\n",
    "Input In [484], in <cell line: 1>()\n",
    "----> 1 combine_prediction_batch(label_probs, pos_probs)\n",
    "\n",
    "Input In [481], in combine_prediction_batch(*args, **kwargs)\n",
    "      3 \"\"\"Deprecated\"\"\"\n",
    "      4 msg = \"\"\"\n",
    "      5 `combine_prediction_batch` is deprecated. \n",
    "      6 Use `combine_predictions` instead, with same capabilities and more.\"\"\"\n",
    "----> 7 raise DeprecationWarning(msg)\n",
    "\n",
    "DeprecationWarning: \n",
    "    `combine_prediction_batch` is deprecated. \n",
    "    Use `combine_predictions` instead, with same capabilities and more.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def combine_prediction_batch(*args, **kwargs):\n",
    "    \"\"\"Deprecated\"\"\"\n",
    "    msg = \"\"\"\n",
    "    `combine_prediction_batch` is deprecated. \n",
    "    Use `combine_predictions` instead, with same capabilities and more.\"\"\"\n",
    "    raise DeprecationWarning(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_fail(\n",
    "    combine_prediction_batch, \n",
    "    args=[probs, probs[:, :,:10]],\n",
    "    msg=\"`combine_prediction_batch` is deprecated.\", \n",
    "    contains=\"is deprecated.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "26b205197a934fdeabb71e65ac11acba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "514ad0bfcabf4df580a9a872af814af9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "55646397fc9349d3af9e98b1f2b26f5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70f6be4247664b708b662c34e7abe3ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7179c6cc207941648c348b1bf10cb87f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70f6be4247664b708b662c34e7abe3ee",
      "placeholder": "",
      "style": "IPY_MODEL_bdde467d943148ce9bb6355fd7582a5c",
      "value": "0.078 MB of 0.078 MB uploaded (0.020 MB deduped)\r"
     }
    },
    "7849f255e99b4853bdba7f8badf1054a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7179c6cc207941648c348b1bf10cb87f",
       "IPY_MODEL_e0819a1ddcc64c08a748a2fd88350f09"
      ],
      "layout": "IPY_MODEL_26b205197a934fdeabb71e65ac11acba"
     }
    },
    "bdde467d943148ce9bb6355fd7582a5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e0819a1ddcc64c08a748a2fd88350f09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55646397fc9349d3af9e98b1f2b26f5d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_514ad0bfcabf4df580a9a872af814af9",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
